{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ca57fe402e8130",
   "metadata": {},
   "source": [
    "# Test pipeline for processing PDFs and storing in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9857901cc3d54ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.095056Z",
     "start_time": "2025-12-08T16:21:20.178241Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: c:\\Users\\kronask\\OneDrive - TU Wien\\TU Wien\\3. Semester\\GenAI\\GenAI\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Dict, Any\n",
    "import sys\n",
    "\n",
    "# Get the absolute path of the current notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (which is the 'GenAI' folder)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Verify it worked\n",
    "print(f\"Added to path: {parent_dir}\")\n",
    "\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from embeddingModels.BaseEmbeddingModel import BaseEmbeddingModel\n",
    "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
    "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
    "from pdfProcessing.doclingTest import setup_docling_converter, extract_sections_from_doc, extract_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b858877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available? True\n",
      "GPU Name: NVIDIA GeForce RTX 2080 Ti\n",
      "CUDA Version: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Check if CUDA (NVIDIA GPU support) is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"Is GPU available? {gpu_available}\")\n",
    "\n",
    "# 2. If available, print the name of the GPU\n",
    "if gpu_available:\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"Running on CPU only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369673091010da59",
   "metadata": {},
   "source": [
    "## Set up folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8e9ac4987b88c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.267916Z",
     "start_time": "2025-12-08T16:21:28.099560Z"
    }
   },
   "outputs": [],
   "source": [
    "CURRENT_MODEL = \"bert\"  # Select either qwen or bert\n",
    "INPUT_FOLDER = \"../data/testPDFs\"\n",
    "OUTPUT_FOLDER = \"../data/testPDFOutput/pipelineTest\"\n",
    "CHROMA_DB_DIR = \"./chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa40ed88b671b0d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.389623Z",
     "start_time": "2025-12-08T16:21:28.271922Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "pdf_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.pdf\"))\n",
    "collection_names = {\"bert\": \"scientific_papers_bert\", \"qwen\": \"scientific_papers_qwen\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8855d4627c4ad",
   "metadata": {},
   "source": [
    "## Set up ChromaDB Client, Collection and Document Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2392b77680b74271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.914449Z",
     "start_time": "2025-12-08T16:21:28.395128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:13:23,703 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA detected. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "pipeline_test_collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_names[CURRENT_MODEL],\n",
    "    metadata={\"hnsw:space\": \"ip\"}\n",
    ")\n",
    "converter = setup_docling_converter()\n",
    "# I used docling from IBM, can also describe images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0c7ad3eab1e67",
   "metadata": {},
   "source": [
    "### Docling\n",
    "https://www.docling.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d4d4e08dc3a8a",
   "metadata": {},
   "source": [
    "## Convert PDFs and store in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8235021d4583806b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:52.184133Z",
     "start_time": "2025-12-08T16:21:28.918953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]2026-01-05 20:13:24,252 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:13:24,287 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:13:24,289 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 1064fff70b16649e2a9cc84da931292b\n",
      "2026-01-05 20:13:24,307 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-05 20:13:24,311 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-05 20:13:24,325 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-05 20:13:24,334 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-05 20:13:24,670 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,695 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,704 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,705 [RapidOCR] main.py:53: Using C:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,787 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,790 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,792 [RapidOCR] main.py:53: Using C:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,842 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,851 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-05 20:13:24,852 [RapidOCR] main.py:53: Using C:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-01-05 20:13:24,984 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2026-01-05 20:13:25,000 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-05 20:13:25,007 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-05 20:13:25,016 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-05 20:13:25,992 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-05 20:13:25,994 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-05 20:13:26,171 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-05 20:13:26,693 - INFO - Processing document Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf\n",
      "2026-01-05 20:13:28,022 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-05 20:13:38,374 - INFO - Finished converting document Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf in 14.12 sec.\n",
      "  7%|▋         | 1/14 [00:14<03:03, 14.13s/it]2026-01-05 20:13:38,381 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:13:38,390 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:13:38,392 - INFO - Processing document Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy\n",
      "   found ID: None\n",
      "   found 1 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:13:51,583 - INFO - Finished converting document Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf in 13.20 sec.\n",
      " 14%|█▍        | 2/14 [00:27<02:43, 13.59s/it]2026-01-05 20:13:51,596 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:13:51,604 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:13:51,605 - INFO - Processing document MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons\n",
      "   found ID: None\n",
      "   found 25 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:14:00,483 - INFO - Finished converting document MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf in 8.89 sec.\n",
      " 21%|██▏       | 3/14 [00:36<02:05, 11.45s/it]2026-01-05 20:14:00,493 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:14:00,497 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:14:00,497 - INFO - Processing document Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties\n",
      "   found ID: None\n",
      "   found 2 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:14:12,124 - INFO - Finished converting document Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf in 11.64 sec.\n",
      " 29%|██▊       | 4/14 [00:47<01:55, 11.53s/it]2026-01-05 20:14:12,140 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:14:12,147 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:14:12,149 - INFO - Processing document Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning\n",
      "   found ID: None\n",
      "   found 6 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:14:29,045 - INFO - Finished converting document Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf in 16.91 sec.\n",
      " 36%|███▌      | 5/14 [01:04<02:01, 13.47s/it]2026-01-05 20:14:29,059 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:14:29,070 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:14:29,072 - INFO - Processing document Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning\n",
      "   found ID: None\n",
      "   found 4 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:14:30,207 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-05 20:14:58,594 - INFO - Finished converting document Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf in 29.55 sec.\n",
      " 43%|████▎     | 6/14 [01:34<02:31, 18.94s/it]2026-01-05 20:14:58,605 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:14:58,611 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:14:58,612 - INFO - Processing document Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines\n",
      "   found ID: None\n",
      "   found 2 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:15:37,343 - INFO - Finished converting document Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf in 38.75 sec.\n",
      " 50%|█████     | 7/14 [02:13<02:57, 25.42s/it]2026-01-05 20:15:37,360 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:15:37,365 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:15:37,367 - INFO - Processing document Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control\n",
      "   found ID: None\n",
      "   found 2 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:16:10,008 - INFO - Finished converting document Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf in 32.64 sec.\n",
      " 57%|█████▋    | 8/14 [02:45<02:46, 27.72s/it]2026-01-05 20:16:10,029 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:16:10,038 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:16:10,040 - INFO - Processing document Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems\n",
      "   found ID: None\n",
      "   found 3 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:16:21,936 - INFO - Finished converting document Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf in 11.92 sec.\n",
      " 64%|██████▍   | 9/14 [02:57<01:53, 22.78s/it]2026-01-05 20:16:21,954 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:16:21,971 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:16:21,972 - INFO - Processing document Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography\n",
      "   found ID: None\n",
      "   found 45 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:16:32,090 - INFO - Finished converting document Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.pdf in 10.14 sec.\n",
      " 71%|███████▏  | 10/14 [03:07<01:15, 18.89s/it]2026-01-05 20:16:32,122 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:16:32,217 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:16:32,219 - INFO - Processing document Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials\n",
      "   found ID: None\n",
      "   found 245 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:16:54,957 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-05 20:18:50,346 - INFO - Finished converting document Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf in 138.25 sec.\n",
      " 79%|███████▊  | 11/14 [05:26<02:46, 55.42s/it]2026-01-05 20:18:50,364 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:18:50,369 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:18:50,370 - INFO - Processing document Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science\n",
      "   found ID: None\n",
      "   found 3 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:19:00,989 - INFO - Finished converting document Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf in 10.62 sec.\n",
      " 86%|████████▌ | 12/14 [05:36<01:23, 41.82s/it]2026-01-05 20:19:01,083 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:19:01,093 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:19:01,095 - INFO - Processing document Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science\n",
      "   found ID: None\n",
      "   found 3 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:19:23,759 - INFO - Finished converting document Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf in 22.67 sec.\n",
      " 93%|█████████▎| 13/14 [05:59<00:36, 36.03s/it]2026-01-05 20:19:23,771 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-05 20:19:23,785 - INFO - Going to convert document batch...\n",
      "2026-01-05 20:19:23,787 - INFO - Processing document Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system\n",
      "   found ID: None\n",
      "   found 1 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:19:38,070 - INFO - Finished converting document Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf in 14.30 sec.\n",
      "100%|██████████| 14/14 [06:13<00:00, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni\n",
      "   found ID: None\n",
      "   found 1 authors\n",
      "CPU times: total: 27min 46s\n",
      "Wall time: 6min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for pdf_path in tqdm(pdf_files):\n",
    "    file_stem = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    try:\n",
    "        result = converter.convert(pdf_path)\n",
    "\n",
    "        sections = extract_sections_from_doc(result.document)\n",
    "\n",
    "        metadata = extract_metadata(sections)\n",
    "\n",
    "        final_output = {\n",
    "            \"filename\": os.path.basename(pdf_path),\n",
    "            \"metadata\": metadata,\n",
    "            \"sections\": sections\n",
    "        }\n",
    "\n",
    "        out_path = os.path.join(OUTPUT_FOLDER, f\"{file_stem}_converted.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_output, f, indent=2)\n",
    "\n",
    "        print(f\"✅ Processed: {file_stem}\")\n",
    "        print(f\"   found ID: {metadata.get('arxiv_id')}\")\n",
    "        print(f\"   found {len(metadata.get('authors', []))} authors\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed {file_stem}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6c869cbd31b428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:52.647638Z",
     "start_time": "2025-12-08T16:21:52.517366Z"
    }
   },
   "outputs": [],
   "source": [
    "del converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee759cd9fe4a3c6c",
   "metadata": {},
   "source": [
    "## Embed and store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90afab37ab9077ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:52.780452Z",
     "start_time": "2025-12-08T16:21:52.652644Z"
    }
   },
   "outputs": [],
   "source": [
    "def ingest_papers_to_chroma(\n",
    "        json_folder: str,\n",
    "        collection: chromadb.Collection,\n",
    "        embedding_model: BaseEmbeddingModel\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads structured JSON papers and ingests them into ChromaDB.\n",
    "    \"\"\"\n",
    "\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files to ingest.\")\n",
    "\n",
    "    for json_file in tqdm(json_files, desc=\"Processing Papers\"):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # --- A. Determine Parent ID ---\n",
    "        # Prefer arXiv ID, fallback to filename if missing\n",
    "        parent_id = data['metadata'].get('arxiv_id')\n",
    "        if not parent_id:\n",
    "            parent_id = data['filename']\n",
    "            # TODO: get ID from sematic scholar\n",
    "\n",
    "        # Clean ID (Chroma requires IDs to be strings, usually safe chars)\n",
    "        parent_id = parent_id.replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "        # --- B. Prepare Batches for this Document ---\n",
    "        documents: List[str] = []\n",
    "        metadatas: List[Dict[str, Any]] = []\n",
    "        ids: List[str] = []\n",
    "\n",
    "        global_meta = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"filename\": data['filename'],\n",
    "            \"title\": data['metadata'].get('title', \"Unknown\"),\n",
    "            \"authors\": \", \".join(data['metadata'].get('authors', [])),\n",
    "            \"arxiv_id\": data['metadata'].get('arxiv_id', \"N/A\")\n",
    "        }\n",
    "\n",
    "        for section_header, content in tqdm(data['sections'].items(), desc=\"Processing Sections\"):\n",
    "            if not content.strip():\n",
    "                continue\n",
    "\n",
    "            # 1. Create Unique ID for this chunk\n",
    "            safe_header = section_header.replace(\" \", \"_\")[:50]\n",
    "            chunk_id = f\"{parent_id}#{safe_header}\"\n",
    "\n",
    "            # 2. Create Metadata for this chunk\n",
    "            chunk_meta = global_meta.copy()\n",
    "            chunk_meta[\"section\"] = section_header\n",
    "            chunk_meta[\"is_preamble\"] = (section_header == \"Preamble\")\n",
    "\n",
    "            # removing \\n from content\n",
    "            content = content.replace(\"\\n\", \" \")\n",
    "            documents.append(content)\n",
    "            metadatas.append(chunk_meta)\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "        # --- D. Generate Embeddings ---\n",
    "        if documents:\n",
    "            # Use your custom class to encode\n",
    "            embeddings_np = embedding_model.encode(documents)\n",
    "            # Convert numpy to python list for Chroma\n",
    "            embeddings_list = embeddings_np.tolist()\n",
    "\n",
    "            # --- E. Upsert to Chroma ---\n",
    "            # using upsert handles re-runs gracefully (updates existing IDs)\n",
    "            print('Generating embeddings')\n",
    "            collection.upsert(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "    print(\"Ingestion Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7b616a1ede268f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:55.366957Z",
     "start_time": "2025-12-08T16:21:52.786957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alibaba-NLP/gte-modernbert-base on cuda...\n"
     ]
    }
   ],
   "source": [
    "match CURRENT_MODEL:\n",
    "    case \"bert\":\n",
    "        embedder = ModernBertEmbedder(\n",
    "            model_name=\"Alibaba-NLP/gte-modernbert-base\",\n",
    "            normalize=True\n",
    "        )\n",
    "    case \"qwen\":\n",
    "        embedder = QwenEmbedder(\"Qwen/Qwen3-Embedding-8B\", use_fp16=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6a42e510f21e41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:04.637004Z",
     "start_time": "2025-12-08T16:21:55.666855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 JSON files to ingest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 21/21 [00:00<?, ?it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Papers:   7%|▋         | 1/14 [00:02<00:29,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "Processing Papers:  14%|█▍        | 2/14 [00:03<00:17,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "Processing Papers:  21%|██▏       | 3/14 [00:10<00:44,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "Processing Papers:  29%|██▊       | 4/14 [00:11<00:29,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "Processing Papers:  36%|███▌      | 5/14 [00:12<00:19,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 31/31 [00:00<?, ?it/s]\n",
      "Processing Papers:  43%|████▎     | 6/14 [00:15<00:19,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 33/33 [00:00<00:00, 61846.31it/s]\n",
      "Processing Papers:  50%|█████     | 7/14 [00:17<00:15,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 20/20 [00:00<?, ?it/s]\n",
      "Processing Papers:  57%|█████▋    | 8/14 [00:19<00:12,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 12/12 [00:00<00:00, 11599.83it/s]\n",
      "Processing Papers:  64%|██████▍   | 9/14 [00:22<00:12,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "Processing Papers:  71%|███████▏  | 10/14 [00:24<00:09,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sections: 100%|██████████| 53/53 [00:00<00:00, 27252.43it/s]\n",
      "Processing Papers:  71%|███████▏  | 10/14 [05:28<02:11, 32.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 29s\n",
      "Wall time: 5min 28s\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.11 GiB. GPU 0 has a total capacity of 11.00 GiB of which 8.96 GiB is free. Of the allocated memory 766.01 MiB is allocated by PyTorch, and 115.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mingest_papers_to_chroma(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    json_folder=OUTPUT_FOLDER,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    collection=pipeline_test_collection,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    embedding_model=embedder\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1397\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1395\u001b[39m st = clock2()\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m     out = \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1399\u001b[39m     captured_exception = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed eval>:1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mingest_papers_to_chroma\u001b[39m\u001b[34m(json_folder, collection, embedding_model)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# --- D. Generate Embeddings ---\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m documents:\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# Use your custom class to encode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     embeddings_np = \u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# Convert numpy to python list for Chroma\u001b[39;00m\n\u001b[32m     64\u001b[39m     embeddings_list = embeddings_np.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\OneDrive - TU Wien\\TU Wien\\3. Semester\\GenAI\\GenAI\\embeddingModels\\ModernBertEmbedder.py:110\u001b[39m, in \u001b[36mModernBertEmbedder.encode\u001b[39m\u001b[34m(self, texts, batch_size, show_progress)\u001b[39m\n\u001b[32m    102\u001b[39m batch = texts[i: i + batch_size]\n\u001b[32m    103\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m    104\u001b[39m     batch,\n\u001b[32m    105\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    106\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    107\u001b[39m     return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    108\u001b[39m ).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m._mean_pooling(outputs.last_hidden_state, inputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.normalize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\transformers\\models\\modernbert\\modeling_modernbert.py:859\u001b[39m, in \u001b[36mModernBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    857\u001b[39m         position_ids = torch.arange(seq_len, device=device).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m     attention_mask, sliding_window_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_attention_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, inputs_embeds=inputs_embeds)\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\transformers\\models\\modernbert\\modeling_modernbert.py:929\u001b[39m, in \u001b[36mModernBertModel._update_attention_mask\u001b[39m\u001b[34m(self, attention_mask, output_attentions)\u001b[39m\n\u001b[32m    922\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    923\u001b[39m         logger.warning_once(\n\u001b[32m    924\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutputting attentions is only supported with the eager attention implementation, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    925\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config._attn_implementation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Consider setting `attn_implementation=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    926\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Setting `output_attentions=False`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    927\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m global_attention_mask = \u001b[43m_prepare_4d_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Create position indices\u001b[39;00m\n\u001b[32m    932\u001b[39m rows = torch.arange(global_attention_mask.shape[\u001b[32m2\u001b[39m]).unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:432\u001b[39m, in \u001b[36m_prepare_4d_attention_mask\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_4d_attention_mask\u001b[39m(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    420\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[33;03m    Creates a non-causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    `(batch_size, key_value_length)`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m \u001b[33;03m            The target length or query length the created mask shall have.\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kronask\\.conda\\envs\\genai_env\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:194\u001b[39m, in \u001b[36mAttentionMaskConverter._expand_mask\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    191\u001b[39m bsz, src_len = mask.size()\n\u001b[32m    192\u001b[39m tgt_len = tgt_len \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m src_len\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m expanded_mask = \u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m inverted_mask = torch.tensor(\u001b[32m1.0\u001b[39m, dtype=dtype) - expanded_mask\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 48.11 GiB. GPU 0 has a total capacity of 11.00 GiB of which 8.96 GiB is free. Of the allocated memory 766.01 MiB is allocated by PyTorch, and 115.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ingest_papers_to_chroma(\n",
    "    json_folder=OUTPUT_FOLDER,\n",
    "    collection=pipeline_test_collection,\n",
    "    embedding_model=embedder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1545019170534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:05.052056Z",
     "start_time": "2025-12-08T16:22:04.911869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Count: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Collection Count: {pipeline_test_collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea30a078f6d3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:05.198448Z",
     "start_time": "2025-12-08T16:22:05.063061Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_chroma(\n",
    "        collection: chromadb.Collection,\n",
    "        query_text: str,\n",
    "        model: BaseEmbeddingModel,\n",
    "        n_results: int = 5,\n",
    "):\n",
    "    print(f\"--- 🔍 Querying for: '{query_text}' ---\")\n",
    "\n",
    "    try:\n",
    "        query_vector_np = model.encode([query_text])\n",
    "        query_vector_list = query_vector_np.tolist()\n",
    "\n",
    "        results = collection.query(\n",
    "            query_embeddings=query_vector_list,\n",
    "            n_results=n_results,\n",
    "            # Optional: Filter by metadata (e.g., only from specific paper)\n",
    "            # where={\"parent_id\": \"arXiv:1706.03762v7\"}\n",
    "        )\n",
    "\n",
    "        # 5. Display Results\n",
    "        if not results['ids'][0]:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n✅ Found {len(results['ids'][0])} relevant chunks:\\n\")\n",
    "\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            doc_id = results['ids'][0][i]\n",
    "            score = results['distances'][0][i]  # Lower is better (L2 distance)\n",
    "            content = results['documents'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "\n",
    "            print(f\"Result #{i + 1} (Distance: {score:.4f})\")\n",
    "            print(f\"📄 Paper: {metadata.get('title', 'Unknown')}\")\n",
    "            print(f\"📌 Section: {metadata.get('section', 'Unknown')}\")\n",
    "            print(f\"🔗 ID: {doc_id}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"📝 Content Snippet:\")\n",
    "            print(textwrap.fill(content[:300] + \"...\", width=80))  # Preview first 300 chars\n",
    "            print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667d6db2e0f383a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:05.378283Z",
     "start_time": "2025-12-08T16:22:05.203952Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 🔍 Querying for: 'transformer architecture details' ---\n",
      "No results found.\n"
     ]
    }
   ],
   "source": [
    "results = query_chroma(\n",
    "    collection=pipeline_test_collection,\n",
    "    query_text=\"transformer architecture details\",\n",
    "    model=embedder,\n",
    "    n_results=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
