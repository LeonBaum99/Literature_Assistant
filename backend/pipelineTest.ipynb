{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test pipeline for processing PDFs and storing in ChromaDB",
   "id": "63ca57fe402e8130"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T15:30:02.992969Z",
     "start_time": "2025-12-08T15:30:00.562379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from embeddingModels.BaseEmbeddingModel import BaseEmbeddingModel\n",
    "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
    "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
    "from pdfProcessing.doclingTest import setup_docling_converter, extract_sections_from_doc, extract_metadata"
   ],
   "id": "d9857901cc3d54ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up folders",
   "id": "369673091010da59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:31:41.309005Z",
     "start_time": "2025-12-08T15:31:41.191526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CURRENT_MODEL = \"bert\"  # Select either qwen or bert\n",
    "INPUT_FOLDER = \"../data/testPDFs\"\n",
    "OUTPUT_FOLDER = \"../data/testPDFOutput/pipelineTest\"\n",
    "CHROMA_DB_DIR = \"./chroma_db\""
   ],
   "id": "1e8e9ac4987b88c0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:31:42.712735Z",
     "start_time": "2025-12-08T15:31:42.590238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "pdf_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.pdf\"))\n",
    "collection_names = {\"bert\": \"scientific_papers_bert\", \"qwen\": \"scientific_papers_qwen\"}"
   ],
   "id": "fa40ed88b671b0d5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up ChromaDB Client, Collection and Document Converter",
   "id": "7dd8855d4627c4ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:31:47.193250Z",
     "start_time": "2025-12-08T15:31:44.514964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "pipeline_test_collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_names[CURRENT_MODEL],\n",
    "    metadata={\"hnsw:space\": \"ip\"}\n",
    ")\n",
    "converter = setup_docling_converter()\n",
    "# I used docling from IBM, can also describe images"
   ],
   "id": "2392b77680b74271",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA detected. Using GPU.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Docling\n",
    "https://www.docling.ai/"
   ],
   "id": "f7a0c7ad3eab1e67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert PDFs and store in json",
   "id": "631d4d4e08dc3a8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T14:58:36.110104Z",
     "start_time": "2025-12-08T14:58:13.737309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "for pdf_path in tqdm(pdf_files):\n",
    "    file_stem = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    try:\n",
    "        result = converter.convert(pdf_path)\n",
    "\n",
    "        sections = extract_sections_from_doc(result.document)\n",
    "\n",
    "        metadata = extract_metadata(sections)\n",
    "\n",
    "        final_output = {\n",
    "            \"filename\": os.path.basename(pdf_path),\n",
    "            \"metadata\": metadata,\n",
    "            \"sections\": sections\n",
    "        }\n",
    "\n",
    "        out_path = os.path.join(OUTPUT_FOLDER, f\"{file_stem}_converted.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_output, f, indent=2)\n",
    "\n",
    "        print(f\"‚úÖ Processed: {file_stem}\")\n",
    "        print(f\"   found ID: {metadata.get('arxiv_id')}\")\n",
    "        print(f\"   found {len(metadata.get('authors', []))} authors\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed {file_stem}: {e}\")\n"
   ],
   "id": "8235021d4583806b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]2025-12-08 15:58:13,861 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-08 15:58:13,916 - INFO - Going to convert document batch...\n",
      "2025-12-08 15:58:13,917 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 1064fff70b16649e2a9cc84da931292b\n",
      "2025-12-08 15:58:13,954 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 15:58:13,956 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-08 15:58:13,995 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 15:58:14,000 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-08 15:58:14,305 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,318 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,324 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,325 [RapidOCR] main.py:53: Using C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,372 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,374 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,374 [RapidOCR] main.py:53: Using C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,405 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,413 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 15:58:14,413 [RapidOCR] main.py:53: Using C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001B[0m\n",
      "2025-12-08 15:58:14,482 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-12-08 15:58:14,520 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 15:58:14,524 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-08 15:58:14,531 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-08 15:58:15,207 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 15:58:15,208 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-08 15:58:15,360 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-08 15:58:15,688 - INFO - Processing document Attention is all you need.pdf\n",
      "2025-12-08 15:58:24,727 - INFO - Finished converting document Attention is all you need.pdf in 10.86 sec.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:10<00:21, 10.87s/it]2025-12-08 15:58:24,730 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-08 15:58:24,731 - INFO - Going to convert document batch...\n",
      "2025-12-08 15:58:24,732 - INFO - Processing document BERT.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: Attention is all you need\n",
      "   found ID: arXiv:1706.03762v7\n",
      "   found 9 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 15:58:32,199 - INFO - Finished converting document BERT.pdf in 7.47 sec.\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:18<00:08,  8.87s/it]2025-12-08 15:58:32,205 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-08 15:58:32,206 - INFO - Going to convert document batch...\n",
      "2025-12-08 15:58:32,207 - INFO - Processing document sentence bert.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: BERT\n",
      "   found ID: arXiv:1810.04805v2\n",
      "   found 3 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 15:58:36,094 - INFO - Finished converting document sentence bert.pdf in 3.89 sec.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:22<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: sentence bert\n",
      "   found ID: arXiv:1908.10084v1\n",
      "   found 3 authors\n",
      "CPU times: total: 1min 9s\n",
      "Wall time: 22.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:31:52.641470Z",
     "start_time": "2025-12-08T15:31:52.523559Z"
    }
   },
   "cell_type": "code",
   "source": "del converter",
   "id": "ef6c869cbd31b428",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embed and store in ChromaDB",
   "id": "ee759cd9fe4a3c6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:40:52.839415Z",
     "start_time": "2025-12-08T15:40:52.714931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ingest_papers_to_chroma(\n",
    "        json_folder: str,\n",
    "        collection: chromadb.Collection,\n",
    "        embedding_model: BaseEmbeddingModel\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads structured JSON papers and ingests them into ChromaDB.\n",
    "    \"\"\"\n",
    "\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files to ingest.\")\n",
    "\n",
    "    for json_file in tqdm(json_files, desc=\"Processing Papers\"):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # --- A. Determine Parent ID ---\n",
    "        # Prefer arXiv ID, fallback to filename if missing\n",
    "        parent_id = data['metadata'].get('arxiv_id')\n",
    "        if not parent_id:\n",
    "            parent_id = data['filename']\n",
    "            # TODO: get ID from sematic scholar\n",
    "\n",
    "        # Clean ID (Chroma requires IDs to be strings, usually safe chars)\n",
    "        parent_id = parent_id.replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "        # --- B. Prepare Batches for this Document ---\n",
    "        documents: List[str] = []\n",
    "        metadatas: List[Dict[str, Any]] = []\n",
    "        ids: List[str] = []\n",
    "\n",
    "        global_meta = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"filename\": data['filename'],\n",
    "            \"title\": data['metadata'].get('title', \"Unknown\"),\n",
    "            \"authors\": \", \".join(data['metadata'].get('authors', [])),\n",
    "            \"arxiv_id\": data['metadata'].get('arxiv_id', \"N/A\")\n",
    "        }\n",
    "\n",
    "        for section_header, content in tqdm(data['sections'].items(), desc=\"Processing Sections\"):\n",
    "            if not content.strip():\n",
    "                continue\n",
    "\n",
    "            # 1. Create Unique ID for this chunk\n",
    "            safe_header = section_header.replace(\" \", \"_\")[:50]\n",
    "            chunk_id = f\"{parent_id}#{safe_header}\"\n",
    "\n",
    "            # 2. Create Metadata for this chunk\n",
    "            chunk_meta = global_meta.copy()\n",
    "            chunk_meta[\"section\"] = section_header\n",
    "            chunk_meta[\"is_preamble\"] = (section_header == \"Preamble\")\n",
    "\n",
    "            # removing \\n from content\n",
    "            content = content.replace(\"\\n\", \" \")\n",
    "            documents.append(content)\n",
    "            metadatas.append(chunk_meta)\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "        # --- D. Generate Embeddings ---\n",
    "        if documents:\n",
    "            # Use your custom class to encode\n",
    "            embeddings_np = embedding_model.encode(documents)\n",
    "            # Convert numpy to python list for Chroma\n",
    "            embeddings_list = embeddings_np.tolist()\n",
    "\n",
    "            # --- E. Upsert to Chroma ---\n",
    "            # using upsert handles re-runs gracefully (updates existing IDs)\n",
    "            print('Generating embeddings')\n",
    "            collection.upsert(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "    print(\"Ingestion Complete.\")"
   ],
   "id": "90afab37ab9077ea",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:33:11.721450Z",
     "start_time": "2025-12-08T15:32:51.109915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match CURRENT_MODEL:\n",
    "    case \"bert\":\n",
    "        embedder = ModernBertEmbedder(\n",
    "            model_name=\"Alibaba-NLP/gte-modernbert-base\",\n",
    "            normalize=True\n",
    "        )\n",
    "    case \"qwen\":\n",
    "        embedder = QwenEmbedder(\"Qwen/Qwen3-Embedding-8B\", use_fp16=True)\n"
   ],
   "id": "cf7b616a1ede268f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen3-Embedding-8B on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ff7e5a49a3e4e5e899d24ae6cc3e807"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:40:52.699428Z",
     "start_time": "2025-12-08T15:34:04.972613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "ingest_papers_to_chroma(\n",
    "    json_folder=OUTPUT_FOLDER,\n",
    "    collection=pipeline_test_collection,\n",
    "    embedding_model=embedder\n",
    ")"
   ],
   "id": "cc6a42e510f21e41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 JSON files to ingest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Processing Sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<?, ?it/s][A\n",
      "Processing Papers:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:35<03:11, 95.85s/it]\n",
      "Processing Sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00<?, ?it/s][A\n",
      "Processing Papers:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [04:51<02:34, 154.56s/it]\n",
      "Processing Sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 35985.45it/s]\n",
      "Processing Papers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [06:47<00:00, 135.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Complete.\n",
      "CPU times: total: 6min 31s\n",
      "Wall time: 6min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:41:05.125029Z",
     "start_time": "2025-12-08T15:41:05.002524Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Collection Count: {pipeline_test_collection.count()}\")",
   "id": "aed1545019170534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Count: 77\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:41:10.121532Z",
     "start_time": "2025-12-08T15:41:09.999905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_chroma(\n",
    "        collection: chromadb.Collection,\n",
    "        query_text: str,\n",
    "        model: BaseEmbeddingModel,\n",
    "        n_results: int = 5,\n",
    "):\n",
    "    print(f\"--- üîç Querying for: '{query_text}' ---\")\n",
    "\n",
    "    try:\n",
    "        query_vector_np = model.encode([query_text])\n",
    "        query_vector_list = query_vector_np.tolist()\n",
    "\n",
    "        results = collection.query(\n",
    "            query_embeddings=query_vector_list,\n",
    "            n_results=n_results,\n",
    "            # Optional: Filter by metadata (e.g., only from specific paper)\n",
    "            # where={\"parent_id\": \"arXiv:1706.03762v7\"}\n",
    "        )\n",
    "\n",
    "        # 5. Display Results\n",
    "        if not results['ids'][0]:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(results['ids'][0])} relevant chunks:\\n\")\n",
    "\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            doc_id = results['ids'][0][i]\n",
    "            score = results['distances'][0][i]  # Lower is better (L2 distance)\n",
    "            content = results['documents'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "\n",
    "            print(f\"Result #{i + 1} (Distance: {score:.4f})\")\n",
    "            print(f\"üìÑ Paper: {metadata.get('title', 'Unknown')}\")\n",
    "            print(f\"üìå Section: {metadata.get('section', 'Unknown')}\")\n",
    "            print(f\"üîó ID: {doc_id}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"üìù Content Snippet:\")\n",
    "            print(textwrap.fill(content[:300] + \"...\", width=80))  # Preview first 300 chars\n",
    "            print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)"
   ],
   "id": "7eea30a078f6d3e3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T15:49:02.184042Z",
     "start_time": "2025-12-08T15:49:01.832031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = query_chroma(\n",
    "    collection=pipeline_test_collection,\n",
    "    query_text=\"transformer architecture details\",\n",
    "    model=embedder,\n",
    "    n_results=3\n",
    ")"
   ],
   "id": "7667d6db2e0f383a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç Querying for: 'Sentence embeddings' ---\n",
      "\n",
      "‚úÖ Found 3 relevant chunks:\n",
      "\n",
      "Result #1 (Distance: 0.2980)\n",
      "üìÑ Paper: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
      "üìå Section: References\n",
      "üîó ID: arXiv_1908.10084v1#References\n",
      "----------------------------------------\n",
      "üìù Content Snippet:\n",
      "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor\n",
      "Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\n",
      "Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task\n",
      "2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretabi...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Result #2 (Distance: 0.3155)\n",
      "üìÑ Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "üìå Section: References\n",
      "üîó ID: arXiv_1810.04805v2#References\n",
      "----------------------------------------\n",
      "üìù Content Snippet:\n",
      "Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string\n",
      "embeddings for sequence labeling. In Proceedings of the 27th International\n",
      "Conference on Computational Linguistics , pages 1638-1649. Rami Al-Rfou, Dokook\n",
      "Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level la...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Result #3 (Distance: 0.3467)\n",
      "üìÑ Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "üìå Section: Preamble\n",
      "üîó ID: arXiv_1810.04805v2#Preamble\n",
      "----------------------------------------\n",
      "üìù Content Snippet:\n",
      "arXiv:1810.04805v2  [cs.CL]  24 May 2019...\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df8c19caec82c702"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
