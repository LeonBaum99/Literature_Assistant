{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "63ca57fe402e8130",
      "metadata": {},
      "source": [
        "# Test pipeline for processing PDFs and storing in ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "79757909",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusted sys.path to include project root: c:\\Users\\Noah\\Documents\\Workspace\\GenAI\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project root to the Python path to allow imports from sibling directories\n",
        "# This is necessary because the notebook is in a subfolder ('backend')\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "    print(f\"Adjusted sys.path to include project root: {project_root}\")\n",
        "\n",
        "# Ensure llmAG is importable for the RAG pipeline + LLM factory\n",
        "llm_root = os.path.join(project_root, 'llmAG')\n",
        "if llm_root not in sys.path:\n",
        "    sys.path.insert(0, llm_root)\n",
        "    print(f\"Adjusted sys.path to include LLM root: {llm_root}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d9857901cc3d54ca",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:28.095056Z",
          "start_time": "2025-12-08T16:21:20.178241Z"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import textwrap\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import chromadb\n",
        "from tqdm import tqdm\n",
        "\n",
        "from embeddingModels.BaseEmbeddingModel import BaseEmbeddingModel\n",
        "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
        "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
        "from pdfProcessing.doclingTest import setup_docling_converter, extract_sections_from_doc, extract_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369673091010da59",
      "metadata": {},
      "source": [
        "## Set up folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1e8e9ac4987b88c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:28.267916Z",
          "start_time": "2025-12-08T16:21:28.099560Z"
        }
      },
      "outputs": [],
      "source": [
        "CURRENT_MODEL = \"bert\"  # Select either qwen or bert\n",
        "INPUT_FOLDER = \"../data/testPDFs\"\n",
        "OUTPUT_FOLDER = \"../data/testPDFOutput/pipelineTest\"\n",
        "CHROMA_DB_DIR = \"./chroma_db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fa40ed88b671b0d5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:28.389623Z",
          "start_time": "2025-12-08T16:21:28.271922Z"
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "pdf_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.pdf\"))\n",
        "collection_names = {\"bert\": \"scientific_papers_bert\", \"qwen\": \"scientific_papers_qwen\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dd8855d4627c4ad",
      "metadata": {},
      "source": [
        "## Set up ChromaDB Client, Collection and Document Converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2392b77680b74271",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:28.914449Z",
          "start_time": "2025-12-08T16:21:28.395128Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-02 16:32:29,834 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
          ]
        }
      ],
      "source": [
        "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
        "pipeline_test_collection = chroma_client.get_or_create_collection(\n",
        "    name=collection_names[CURRENT_MODEL],\n",
        "    metadata={\"hnsw:space\": \"ip\"}\n",
        ")\n",
        "converter = setup_docling_converter()\n",
        "# I used docling from IBM, can also describe images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a0c7ad3eab1e67",
      "metadata": {},
      "source": [
        "### Docling\n",
        "https://www.docling.ai/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631d4d4e08dc3a8a",
      "metadata": {},
      "source": [
        "## Convert PDFs and store in json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8235021d4583806b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:52.184133Z",
          "start_time": "2025-12-08T16:21:28.918953Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]2026-01-02 16:32:30,265 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2026-01-02 16:32:32,033 - INFO - Going to convert document batch...\n",
            "2026-01-02 16:32:32,035 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 98cea8ab80245d656e57fdc38b70960e\n",
            "2026-01-02 16:32:32,056 - INFO - Loading plugin 'docling_defaults'\n",
            "2026-01-02 16:32:32,060 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2026-01-02 16:32:32,077 - INFO - Loading plugin 'docling_defaults'\n",
            "2026-01-02 16:32:32,085 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
            "2026-01-02 16:32:32,822 - INFO - Accelerator device: 'cpu'\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:32,852 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:32,870 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:32,871 [RapidOCR] main.py:53: Using C:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:32,986 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:32,995 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:32,996 [RapidOCR] main.py:53: Using C:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:33,038 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:33,059 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-02 16:32:33,059 [RapidOCR] main.py:53: Using C:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
            "2026-01-02 16:32:33,170 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
            "2026-01-02 16:32:33,186 - INFO - Loading plugin 'docling_defaults'\n",
            "2026-01-02 16:32:33,191 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
            "2026-01-02 16:32:33,200 - INFO - Accelerator device: 'cpu'\n",
            "2026-01-02 16:32:34,010 - INFO - Loading plugin 'docling_defaults'\n",
            "2026-01-02 16:32:34,012 - INFO - Registered table structure engines: ['docling_tableformer']\n",
            "2026-01-02 16:32:34,302 - INFO - Accelerator device: 'cpu'\n",
            "2026-01-02 16:32:35,092 - INFO - Processing document Attention is all you need.pdf\n",
            "2026-01-02 16:33:09,950 - INFO - Finished converting document Attention is all you need.pdf in 39.70 sec.\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:39<01:19, 39.71s/it]2026-01-02 16:33:09,968 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2026-01-02 16:33:09,971 - INFO - Going to convert document batch...\n",
            "2026-01-02 16:33:09,972 - INFO - Processing document BERT.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed: Attention is all you need\n",
            "   found ID: arXiv:1706.03762v7\n",
            "   found 9 authors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-02 16:33:41,992 - INFO - Finished converting document BERT.pdf in 32.05 sec.\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:11<00:35, 35.20s/it]2026-01-02 16:33:42,013 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2026-01-02 16:33:42,016 - INFO - Going to convert document batch...\n",
            "2026-01-02 16:33:42,016 - INFO - Processing document sentence bert.pdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed: BERT\n",
            "   found ID: arXiv:1810.04805v2\n",
            "   found 3 authors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-02 16:34:03,963 - INFO - Finished converting document sentence bert.pdf in 21.97 sec.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:33<00:00, 31.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed: sentence bert\n",
            "   found ID: arXiv:1908.10084v1\n",
            "   found 3 authors\n",
            "CPU times: total: 8min 24s\n",
            "Wall time: 1min 33s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for pdf_path in tqdm(pdf_files):\n",
        "    file_stem = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "    try:\n",
        "        result = converter.convert(pdf_path)\n",
        "\n",
        "        sections = extract_sections_from_doc(result.document)\n",
        "\n",
        "        metadata = extract_metadata(sections)\n",
        "\n",
        "        final_output = {\n",
        "            \"filename\": os.path.basename(pdf_path),\n",
        "            \"metadata\": metadata,\n",
        "            \"sections\": sections\n",
        "        }\n",
        "\n",
        "        out_path = os.path.join(OUTPUT_FOLDER, f\"{file_stem}_converted.json\")\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(final_output, f, indent=2)\n",
        "\n",
        "        print(f\"âœ… Processed: {file_stem}\")\n",
        "        print(f\"   found ID: {metadata.get('arxiv_id')}\")\n",
        "        print(f\"   found {len(metadata.get('authors', []))} authors\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed {file_stem}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef6c869cbd31b428",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:52.647638Z",
          "start_time": "2025-12-08T16:21:52.517366Z"
        }
      },
      "outputs": [],
      "source": [
        "del converter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee759cd9fe4a3c6c",
      "metadata": {},
      "source": [
        "## Embed and store in ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "90afab37ab9077ea",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:52.780452Z",
          "start_time": "2025-12-08T16:21:52.652644Z"
        }
      },
      "outputs": [],
      "source": [
        "def ingest_papers_to_chroma(\n",
        "        json_folder: str,\n",
        "        collection: chromadb.Collection,\n",
        "        embedding_model: BaseEmbeddingModel\n",
        "):\n",
        "    \"\"\"\n",
        "    Reads structured JSON papers and ingests them into ChromaDB.\n",
        "    \"\"\"\n",
        "\n",
        "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
        "    print(f\"Found {len(json_files)} JSON files to ingest.\")\n",
        "\n",
        "    for json_file in tqdm(json_files, desc=\"Processing Papers\"):\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- A. Determine Parent ID ---\n",
        "        # Prefer arXiv ID, fallback to filename if missing\n",
        "        parent_id = data['metadata'].get('arxiv_id')\n",
        "        if not parent_id:\n",
        "            parent_id = data['filename']\n",
        "            # TODO: get ID from sematic scholar\n",
        "\n",
        "        # Clean ID (Chroma requires IDs to be strings, usually safe chars)\n",
        "        parent_id = parent_id.replace(\" \", \"_\").replace(\":\", \"_\")\n",
        "\n",
        "        # --- B. Prepare Batches for this Document ---\n",
        "        documents: List[str] = []\n",
        "        metadatas: List[Dict[str, Any]] = []\n",
        "        ids: List[str] = []\n",
        "\n",
        "        global_meta = {\n",
        "            \"parent_id\": parent_id,\n",
        "            \"filename\": data['filename'],\n",
        "            \"title\": data['metadata'].get('title', \"Unknown\"),\n",
        "            \"authors\": \", \".join(data['metadata'].get('authors', [])),\n",
        "            \"arxiv_id\": data['metadata'].get('arxiv_id', \"N/A\")\n",
        "        }\n",
        "\n",
        "        for section_header, content in tqdm(data['sections'].items(), desc=\"Processing Sections\"):\n",
        "            if not content.strip():\n",
        "                continue\n",
        "\n",
        "            # 1. Create Unique ID for this chunk\n",
        "            safe_header = section_header.replace(\" \", \"_\")[:50]\n",
        "            chunk_id = f\"{parent_id}#{safe_header}\"\n",
        "\n",
        "            # 2. Create Metadata for this chunk\n",
        "            chunk_meta = global_meta.copy()\n",
        "            chunk_meta[\"section\"] = section_header\n",
        "            chunk_meta[\"is_preamble\"] = (section_header == \"Preamble\")\n",
        "\n",
        "            # removing \\n from content\n",
        "            content = content.replace(\"\\n\", \" \")\n",
        "            documents.append(content)\n",
        "            metadatas.append(chunk_meta)\n",
        "            ids.append(chunk_id)\n",
        "\n",
        "        # --- D. Generate Embeddings ---\n",
        "        if documents:\n",
        "            # Use your custom class to encode\n",
        "            embeddings_np = embedding_model.encode(documents)\n",
        "            # Convert numpy to python list for Chroma\n",
        "            embeddings_list = embeddings_np.tolist()\n",
        "\n",
        "            # --- E. Upsert to Chroma ---\n",
        "            # using upsert handles re-runs gracefully (updates existing IDs)\n",
        "            print('Generating embeddings')\n",
        "            collection.upsert(\n",
        "                ids=ids,\n",
        "                embeddings=embeddings_list,\n",
        "                documents=documents,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "\n",
        "    print(\"Ingestion Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf7b616a1ede268f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:21:55.366957Z",
          "start_time": "2025-12-08T16:21:52.786957Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Alibaba-NLP/gte-modernbert-base on cpu...\n"
          ]
        }
      ],
      "source": [
        "match CURRENT_MODEL:\n",
        "    case \"bert\":\n",
        "        embedder = ModernBertEmbedder(\n",
        "            model_name=\"Alibaba-NLP/gte-modernbert-base\",\n",
        "            normalize=True\n",
        "        )\n",
        "    case \"qwen\":\n",
        "        embedder = QwenEmbedder(\"Qwen/Qwen3-Embedding-8B\", use_fp16=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cc6a42e510f21e41",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:22:04.637004Z",
          "start_time": "2025-12-08T16:21:55.666855Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 JSON files to ingest.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Sections: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<?, ?it/s]\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Processing Papers:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:45<01:31, 45.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Sections: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<?, ?it/s]\n",
            "Processing Papers:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:29<01:20, 80.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Sections: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<?, ?it/s]\n",
            "Processing Papers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:00<00:00, 60.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings\n",
            "Ingestion Complete.\n",
            "CPU times: total: 11min 35s\n",
            "Wall time: 3min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "ingest_papers_to_chroma(\n",
        "    json_folder=OUTPUT_FOLDER,\n",
        "    collection=pipeline_test_collection,\n",
        "    embedding_model=embedder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "aed1545019170534",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:22:05.052056Z",
          "start_time": "2025-12-08T16:22:04.911869Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection Count: 77\n"
          ]
        }
      ],
      "source": [
        "print(f\"Collection Count: {pipeline_test_collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7eea30a078f6d3e3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:22:05.198448Z",
          "start_time": "2025-12-08T16:22:05.063061Z"
        }
      },
      "outputs": [],
      "source": [
        "def query_chroma(\n",
        "        collection: chromadb.Collection,\n",
        "        query_text: str,\n",
        "        model: BaseEmbeddingModel,\n",
        "        n_results: int = 5,\n",
        "):\n",
        "    print(f\"--- ðŸ” Querying for: '{query_text}' ---\")\n",
        "\n",
        "    try:\n",
        "        query_vector_np = model.encode([query_text])\n",
        "        query_vector_list = query_vector_np.tolist()\n",
        "\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_vector_list,\n",
        "            n_results=n_results,\n",
        "            # Optional: Filter by metadata (e.g., only from specific paper)\n",
        "            # where={\"parent_id\": \"arXiv:1706.03762v7\"}\n",
        "        )\n",
        "\n",
        "        # 5. Display Results\n",
        "        if not results['ids'][0]:\n",
        "            print(\"No results found.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nâœ… Found {len(results['ids'][0])} relevant chunks:\\n\")\n",
        "\n",
        "        for i in range(len(results['ids'][0])):\n",
        "            doc_id = results['ids'][0][i]\n",
        "            score = results['distances'][0][i]  # Lower is better (L2 distance)\n",
        "            content = results['documents'][0][i]\n",
        "            metadata = results['metadatas'][0][i]\n",
        "\n",
        "            print(f\"Result #{i + 1} (Distance: {score:.4f})\")\n",
        "            print(f\"ðŸ“„ Paper: {metadata.get('title', 'Unknown')}\")\n",
        "            print(f\"ðŸ“Œ Section: {metadata.get('section', 'Unknown')}\")\n",
        "            print(f\"ðŸ”— ID: {doc_id}\")\n",
        "            print(\"-\" * 40)\n",
        "            print(\"ðŸ“ Content Snippet:\")\n",
        "            print(textwrap.fill(content[:300] + \"...\", width=80))  # Preview first 300 chars\n",
        "            print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7667d6db2e0f383a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-08T16:22:05.378283Z",
          "start_time": "2025-12-08T16:22:05.203952Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ðŸ” Querying for: 'transformer architecture details' ---\n",
            "\n",
            "âœ… Found 3 relevant chunks:\n",
            "\n",
            "Result #1 (Distance: 0.5367)\n",
            "ðŸ“„ Paper: Attention Is All You Need\n",
            "ðŸ“Œ Section: 5 Training\n",
            "ðŸ”— ID: arXiv_1706.03762v7#5_Training\n",
            "----------------------------------------\n",
            "ðŸ“ Content Snippet:\n",
            "This section describes the training regime for our models....\n",
            "\n",
            "============================================================\n",
            "\n",
            "Result #2 (Distance: 0.5779)\n",
            "ðŸ“„ Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "ðŸ“Œ Section: 4 Experiments\n",
            "ðŸ”— ID: arXiv_1810.04805v2#4_Experiments\n",
            "----------------------------------------\n",
            "ðŸ“ Content Snippet:\n",
            "In this section, we present BERT fine-tuning results on 11 NLP tasks....\n",
            "\n",
            "============================================================\n",
            "\n",
            "Result #3 (Distance: 0.6134)\n",
            "ðŸ“„ Paper: Attention Is All You Need\n",
            "ðŸ“Œ Section: Scaled Dot-Product Attention\n",
            "ðŸ”— ID: arXiv_1706.03762v7#Scaled_Dot-Product_Attention\n",
            "----------------------------------------\n",
            "ðŸ“ Content Snippet:\n",
            "MatMul SoftMax Mask (opt.) Scale MatMul K Figure 2: (left) Scaled Dot-Product\n",
            "Attention. (right) Multi-Head Attention consists of several attention layers\n",
            "running in parallel....\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = query_chroma(\n",
        "    collection=pipeline_test_collection,\n",
        "    query_text=\"transformer architecture details\",\n",
        "    model=embedder,\n",
        "    n_results=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb3ccabcc3bd416db3ccc7f89669720f",
      "metadata": {},
      "source": [
        "## RAG: LLM answers from Chroma context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "25a18924e0c94af78aa87991da2ea025",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure llmAG/rag is importable even if earlier cells were skipped\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "llm_root = os.path.join(project_root, \"llmAG\")\n",
        "if llm_root not in sys.path:\n",
        "    sys.path.insert(0, llm_root)\n",
        "\n",
        "\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from rag import BaseRetriever, RagPipeline\n",
        "\n",
        "class ChromaRetriever(BaseRetriever):\n",
        "    def __init__(self, collection: chromadb.Collection, model: BaseEmbeddingModel):\n",
        "        self._collection = collection\n",
        "        self._model = model\n",
        "\n",
        "    def get_relevant_documents(self, query: str, k: int = 4) -> List[Document]:\n",
        "        query_vector_np = self._model.encode([query])\n",
        "        results = self._collection.query(\n",
        "            query_embeddings=query_vector_np.tolist(),\n",
        "            n_results=k,\n",
        "        )\n",
        "\n",
        "        ids = results.get(\"ids\", [[]])[0] if results else []\n",
        "        documents = results.get(\"documents\", [[]])[0] if results else []\n",
        "        metadatas = results.get(\"metadatas\", [[]])[0] if results else []\n",
        "        distances = results.get(\"distances\", [[]])[0] if results else []\n",
        "\n",
        "        docs: List[Document] = []\n",
        "        for doc_id, text, meta, score in zip(ids, documents, metadatas, distances):\n",
        "            meta_dict = dict(meta) if meta else {}\n",
        "            meta_dict[\"score\"] = score\n",
        "            meta_dict[\"id\"] = doc_id\n",
        "            docs.append(Document(page_content=text, metadata=meta_dict))\n",
        "        return docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c2c807ee9acd4aa3b62df7d2d34d01ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = ChromaRetriever(pipeline_test_collection, embedder)\n",
        "pipeline = RagPipeline(retriever, model='mistral-nemo', temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5c645aa278ab4d82856b12b72c8869eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-02 19:16:02,520 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Question ===\n",
            "How does the Transformer in \"Attention Is All You Need\" replace recurrence and convolution, and what role does self-attention play in modeling dependencies?\n",
            "\n",
            "Answer:\n",
            " The Transformer in \"Attention Is All You Need\" replaces recurrence (like LSTMs or GRUs) and convolution by using self-attention mechanisms to model dependencies between input elements. Self-attention allows the model to selectively \"attend\" to different positions of a single sequence, enabling it to capture dependencies regardless of their distance. This is unlike recurrent models that process inputs sequentially, or convolutional models where the number of operations grows with the distance between positions. In the Transformer, the number of operations required to relate signals from two arbitrary input positions remains constant, making it easier to learn long-range dependencies. However, this comes at the cost of reduced effective resolution due to averaging attention-weighted positions, which is counteracted by using Multi-Head Attention.\n",
            "\n",
            "Sources:\n",
            "1. Attention Is All You Need | 2 Background | score=0.2870\n",
            "2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | 2.2 Unsupervised Fine-tuning Approaches | score=0.2988\n",
            "3. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | A.5 Illustrations of Fine-tuning on Different Tasks | score=0.3090\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m questions = [\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHow does the Transformer in \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mAttention Is All You Need\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m replace recurrence and convolution, and what role does self-attention play in modeling dependencies?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFor BERTLARGE, how many layers, what hidden size, and how many attention heads are used?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn BERT, what is the special classification token placed at the start of every input sequence?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m ]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     response = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_sources\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Question ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(q)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\llmAG\\rag\\pipeline.py:5\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(self, question, k, include_sources)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"RAG orchestration layer for the sandbox pipeline.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[33;03mThis module wires together three responsibilities:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m1) retrieval of relevant context (via a retriever interface),\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m2) prompt assembly with bounded context size,\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m3) LLM invocation using the existing `build_llm` factory.\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[33;03mThe interface is intentionally small so a future Chroma retriever can\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03mbe swapped in without changing the rest of the pipeline.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Iterable, List, Optional\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\ollama\\_client.py:174\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m      \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpx\\_client.py:868\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m     method=method,\n\u001b[32m    857\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m     extensions=extensions,\n\u001b[32m    867\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noah\\Documents\\Workspace\\GenAI\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "'''questions = [\n",
        "    \"What does RAG combine, and why is it useful?\",\n",
        "    \"What does top-k retrieval mean in this context?\",\n",
        "    \"What chunk size and overlap ranges are suggested?\",\n",
        "    \"Why does chunk overlap matter for retrieval?\",\n",
        "    \"What should the assistant do when the answer is not in the context?\",\n",
        "]'''\n",
        "\n",
        "questions = [\n",
        "    \"How does the Transformer in \\\"Attention Is All You Need\\\" replace recurrence and convolution, and what role does self-attention play in modeling dependencies?\",\n",
        "    \"For BERTLARGE, how many layers, what hidden size, and how many attention heads are used?\",\n",
        "    \"In BERT, what is the special classification token placed at the start of every input sequence?\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    response = pipeline.run(q, k=3, include_sources=True)\n",
        "    print(\"\\n=== Question ===\")\n",
        "    print(q)\n",
        "    print(\"\\nAnswer:\\n\", response.answer)\n",
        "    if response.sources:\n",
        "        print(\"\\nSources:\")\n",
        "        for idx, doc in enumerate(response.sources, start=1):\n",
        "            title = doc.metadata.get('title', 'Unknown') if doc.metadata else 'Unknown'\n",
        "            section = doc.metadata.get('section', 'Unknown') if doc.metadata else 'Unknown'\n",
        "            score = doc.metadata.get('score') if doc.metadata else None\n",
        "            score_str = f\"{score:.4f}\" if isinstance(score, (int, float)) else 'n/a'\n",
        "            print(f\"{idx}. {title} | {section} | score={score_str}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
