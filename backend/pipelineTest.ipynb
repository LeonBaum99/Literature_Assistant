{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test pipeline for processing PDFs and storing in ChromaDB",
   "id": "63ca57fe402e8130"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.095056Z",
     "start_time": "2025-12-08T16:21:20.178241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from embeddingModels.BaseEmbeddingModel import BaseEmbeddingModel\n",
    "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
    "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
    "from pdfProcessing.doclingTest import setup_docling_converter, extract_sections_from_doc, extract_metadata"
   ],
   "id": "d9857901cc3d54ca",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up folders",
   "id": "369673091010da59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.267916Z",
     "start_time": "2025-12-08T16:21:28.099560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CURRENT_MODEL = \"bert\"  # Select either qwen or bert\n",
    "INPUT_FOLDER = \"../data/testPDFs\"\n",
    "OUTPUT_FOLDER = \"../data/testPDFOutput/pipelineTest\"\n",
    "CHROMA_DB_DIR = \"./chroma_db\""
   ],
   "id": "1e8e9ac4987b88c0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.389623Z",
     "start_time": "2025-12-08T16:21:28.271922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "pdf_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.pdf\"))\n",
    "collection_names = {\"bert\": \"scientific_papers_bert\", \"qwen\": \"scientific_papers_qwen\"}"
   ],
   "id": "fa40ed88b671b0d5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up ChromaDB Client, Collection and Document Converter",
   "id": "7dd8855d4627c4ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:28.914449Z",
     "start_time": "2025-12-08T16:21:28.395128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)\n",
    "pipeline_test_collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_names[CURRENT_MODEL],\n",
    "    metadata={\"hnsw:space\": \"ip\"}\n",
    ")\n",
    "converter = setup_docling_converter()\n",
    "# I used docling from IBM, can also describe images"
   ],
   "id": "2392b77680b74271",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 17:21:28,537 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA detected. Using GPU.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Docling\n",
    "https://www.docling.ai/"
   ],
   "id": "f7a0c7ad3eab1e67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert PDFs and store in json",
   "id": "631d4d4e08dc3a8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:52.184133Z",
     "start_time": "2025-12-08T16:21:28.918953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "for pdf_path in tqdm(pdf_files):\n",
    "    file_stem = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    try:\n",
    "        result = converter.convert(pdf_path)\n",
    "\n",
    "        sections = extract_sections_from_doc(result.document)\n",
    "\n",
    "        metadata = extract_metadata(sections)\n",
    "\n",
    "        final_output = {\n",
    "            \"filename\": os.path.basename(pdf_path),\n",
    "            \"metadata\": metadata,\n",
    "            \"sections\": sections\n",
    "        }\n",
    "\n",
    "        out_path = os.path.join(OUTPUT_FOLDER, f\"{file_stem}_converted.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_output, f, indent=2)\n",
    "\n",
    "        print(f\"‚úÖ Processed: {file_stem}\")\n",
    "        print(f\"   found ID: {metadata.get('arxiv_id')}\")\n",
    "        print(f\"   found {len(metadata.get('authors', []))} authors\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed {file_stem}: {e}\")\n"
   ],
   "id": "8235021d4583806b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]2025-12-08 17:21:29,048 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-08 17:21:29,161 - INFO - Going to convert document batch...\n",
      "2025-12-08 17:21:29,162 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 1064fff70b16649e2a9cc84da931292b\n",
      "2025-12-08 17:21:29,199 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 17:21:29,202 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-08 17:21:29,235 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 17:21:29,240 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-08 17:21:29,527 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,540 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,550 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,551 [RapidOCR] main.py:53: Using C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,632 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,635 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,635 [RapidOCR] main.py:53: Using C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,661 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,674 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001B[0m\n",
      "\u001B[32m[INFO] 2025-12-08 17:21:29,674 [RapidOCR] main.py:53: Using C:\\Users\\tnkru\\anaconda3\\envs\\GenAI\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001B[0m\n",
      "2025-12-08 17:21:29,747 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2025-12-08 17:21:29,779 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 17:21:29,784 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-08 17:21:29,791 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-08 17:21:30,582 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-08 17:21:30,584 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-08 17:21:30,741 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-08 17:21:31,171 - INFO - Processing document Attention is all you need.pdf\n",
      "2025-12-08 17:21:40,800 - INFO - Finished converting document Attention is all you need.pdf in 11.75 sec.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:11<00:23, 11.76s/it]2025-12-08 17:21:40,802 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-08 17:21:40,804 - INFO - Going to convert document batch...\n",
      "2025-12-08 17:21:40,805 - INFO - Processing document BERT.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: Attention is all you need\n",
      "   found ID: arXiv:1706.03762v7\n",
      "   found 9 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 17:21:48,067 - INFO - Finished converting document BERT.pdf in 7.27 sec.\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:19<00:09,  9.12s/it]2025-12-08 17:21:48,074 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-08 17:21:48,075 - INFO - Going to convert document batch...\n",
      "2025-12-08 17:21:48,076 - INFO - Processing document sentence bert.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: BERT\n",
      "   found ID: arXiv:1810.04805v2\n",
      "   found 3 authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 17:21:52,168 - INFO - Finished converting document sentence bert.pdf in 4.09 sec.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:23<00:00,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: sentence bert\n",
      "   found ID: arXiv:1908.10084v1\n",
      "   found 3 authors\n",
      "CPU times: total: 1min 8s\n",
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:52.647638Z",
     "start_time": "2025-12-08T16:21:52.517366Z"
    }
   },
   "cell_type": "code",
   "source": "del converter",
   "id": "ef6c869cbd31b428",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embed and store in ChromaDB",
   "id": "ee759cd9fe4a3c6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:52.780452Z",
     "start_time": "2025-12-08T16:21:52.652644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ingest_papers_to_chroma(\n",
    "        json_folder: str,\n",
    "        collection: chromadb.Collection,\n",
    "        embedding_model: BaseEmbeddingModel\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads structured JSON papers and ingests them into ChromaDB.\n",
    "    \"\"\"\n",
    "\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files to ingest.\")\n",
    "\n",
    "    for json_file in tqdm(json_files, desc=\"Processing Papers\"):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # --- A. Determine Parent ID ---\n",
    "        # Prefer arXiv ID, fallback to filename if missing\n",
    "        parent_id = data['metadata'].get('arxiv_id')\n",
    "        if not parent_id:\n",
    "            parent_id = data['filename']\n",
    "            # TODO: get ID from sematic scholar\n",
    "\n",
    "        # Clean ID (Chroma requires IDs to be strings, usually safe chars)\n",
    "        parent_id = parent_id.replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "        # --- B. Prepare Batches for this Document ---\n",
    "        documents: List[str] = []\n",
    "        metadatas: List[Dict[str, Any]] = []\n",
    "        ids: List[str] = []\n",
    "\n",
    "        global_meta = {\n",
    "            \"parent_id\": parent_id,\n",
    "            \"filename\": data['filename'],\n",
    "            \"title\": data['metadata'].get('title', \"Unknown\"),\n",
    "            \"authors\": \", \".join(data['metadata'].get('authors', [])),\n",
    "            \"arxiv_id\": data['metadata'].get('arxiv_id', \"N/A\")\n",
    "        }\n",
    "\n",
    "        for section_header, content in tqdm(data['sections'].items(), desc=\"Processing Sections\"):\n",
    "            if not content.strip():\n",
    "                continue\n",
    "\n",
    "            # 1. Create Unique ID for this chunk\n",
    "            safe_header = section_header.replace(\" \", \"_\")[:50]\n",
    "            chunk_id = f\"{parent_id}#{safe_header}\"\n",
    "\n",
    "            # 2. Create Metadata for this chunk\n",
    "            chunk_meta = global_meta.copy()\n",
    "            chunk_meta[\"section\"] = section_header\n",
    "            chunk_meta[\"is_preamble\"] = (section_header == \"Preamble\")\n",
    "\n",
    "            # removing \\n from content\n",
    "            content = content.replace(\"\\n\", \" \")\n",
    "            documents.append(content)\n",
    "            metadatas.append(chunk_meta)\n",
    "            ids.append(chunk_id)\n",
    "\n",
    "        # --- D. Generate Embeddings ---\n",
    "        if documents:\n",
    "            # Use your custom class to encode\n",
    "            embeddings_np = embedding_model.encode(documents)\n",
    "            # Convert numpy to python list for Chroma\n",
    "            embeddings_list = embeddings_np.tolist()\n",
    "\n",
    "            # --- E. Upsert to Chroma ---\n",
    "            # using upsert handles re-runs gracefully (updates existing IDs)\n",
    "            print('Generating embeddings')\n",
    "            collection.upsert(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "    print(\"Ingestion Complete.\")"
   ],
   "id": "90afab37ab9077ea",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:21:55.366957Z",
     "start_time": "2025-12-08T16:21:52.786957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match CURRENT_MODEL:\n",
    "    case \"bert\":\n",
    "        embedder = ModernBertEmbedder(\n",
    "            model_name=\"Alibaba-NLP/gte-modernbert-base\",\n",
    "            normalize=True\n",
    "        )\n",
    "    case \"qwen\":\n",
    "        embedder = QwenEmbedder(\"Qwen/Qwen3-Embedding-8B\", use_fp16=True)\n"
   ],
   "id": "cf7b616a1ede268f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alibaba-NLP/gte-modernbert-base on cuda...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:04.637004Z",
     "start_time": "2025-12-08T16:21:55.666855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "ingest_papers_to_chroma(\n",
    "    json_folder=OUTPUT_FOLDER,\n",
    "    collection=pipeline_test_collection,\n",
    "    embedding_model=embedder\n",
    ")"
   ],
   "id": "cc6a42e510f21e41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 JSON files to ingest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Processing Sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<?, ?it/s][A\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Papers:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:02<00:05,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00<?, ?it/s][A\n",
      "Processing Papers:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:07<00:03,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<?, ?it/s][A\n",
      "Processing Papers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n",
      "Ingestion Complete.\n",
      "CPU times: total: 12.6 s\n",
      "Wall time: 8.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:05.052056Z",
     "start_time": "2025-12-08T16:22:04.911869Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Collection Count: {pipeline_test_collection.count()}\")",
   "id": "aed1545019170534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Count: 77\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:05.198448Z",
     "start_time": "2025-12-08T16:22:05.063061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_chroma(\n",
    "        collection: chromadb.Collection,\n",
    "        query_text: str,\n",
    "        model: BaseEmbeddingModel,\n",
    "        n_results: int = 5,\n",
    "):\n",
    "    print(f\"--- üîç Querying for: '{query_text}' ---\")\n",
    "\n",
    "    try:\n",
    "        query_vector_np = model.encode([query_text])\n",
    "        query_vector_list = query_vector_np.tolist()\n",
    "\n",
    "        results = collection.query(\n",
    "            query_embeddings=query_vector_list,\n",
    "            n_results=n_results,\n",
    "            # Optional: Filter by metadata (e.g., only from specific paper)\n",
    "            # where={\"parent_id\": \"arXiv:1706.03762v7\"}\n",
    "        )\n",
    "\n",
    "        # 5. Display Results\n",
    "        if not results['ids'][0]:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(results['ids'][0])} relevant chunks:\\n\")\n",
    "\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            doc_id = results['ids'][0][i]\n",
    "            score = results['distances'][0][i]  # Lower is better (L2 distance)\n",
    "            content = results['documents'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "\n",
    "            print(f\"Result #{i + 1} (Distance: {score:.4f})\")\n",
    "            print(f\"üìÑ Paper: {metadata.get('title', 'Unknown')}\")\n",
    "            print(f\"üìå Section: {metadata.get('section', 'Unknown')}\")\n",
    "            print(f\"üîó ID: {doc_id}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"üìù Content Snippet:\")\n",
    "            print(textwrap.fill(content[:300] + \"...\", width=80))  # Preview first 300 chars\n",
    "            print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)"
   ],
   "id": "7eea30a078f6d3e3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T16:22:05.378283Z",
     "start_time": "2025-12-08T16:22:05.203952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = query_chroma(\n",
    "    collection=pipeline_test_collection,\n",
    "    query_text=\"transformer architecture details\",\n",
    "    model=embedder,\n",
    "    n_results=3\n",
    ")"
   ],
   "id": "7667d6db2e0f383a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç Querying for: 'transformer architecture details' ---\n",
      "\n",
      "‚úÖ Found 3 relevant chunks:\n",
      "\n",
      "Result #1 (Distance: 0.5367)\n",
      "üìÑ Paper: Attention Is All You Need\n",
      "üìå Section: 5 Training\n",
      "üîó ID: arXiv_1706.03762v7#5_Training\n",
      "----------------------------------------\n",
      "üìù Content Snippet:\n",
      "This section describes the training regime for our models....\n",
      "\n",
      "============================================================\n",
      "\n",
      "Result #2 (Distance: 0.5779)\n",
      "üìÑ Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "üìå Section: 4 Experiments\n",
      "üîó ID: arXiv_1810.04805v2#4_Experiments\n",
      "----------------------------------------\n",
      "üìù Content Snippet:\n",
      "In this section, we present BERT fine-tuning results on 11 NLP tasks....\n",
      "\n",
      "============================================================\n",
      "\n",
      "Result #3 (Distance: 0.6134)\n",
      "üìÑ Paper: Attention Is All You Need\n",
      "üìå Section: Scaled Dot-Product Attention\n",
      "üîó ID: arXiv_1706.03762v7#Scaled_Dot-Product_Attention\n",
      "----------------------------------------\n",
      "üìù Content Snippet:\n",
      "MatMul SoftMax Mask (opt.) Scale MatMul K Figure 2: (left) Scaled Dot-Product\n",
      "Attention. (right) Multi-Head Attention consists of several attention layers\n",
      "running in parallel....\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
