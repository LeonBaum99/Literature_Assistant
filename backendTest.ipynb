{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-02T11:02:52.715610300Z",
     "start_time": "2026-01-02T11:02:52.647561500Z"
    }
   },
   "source": "import requests",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "15ba941ffea107c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T11:15:37.949930500Z",
     "start_time": "2026-01-02T11:14:51.000315300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8000/ingest\"\n",
    "file_path = \"data/testPDFs/Attention is all you need.pdf\"\n",
    "\n",
    "# We open the file in binary mode\n",
    "with open(file_path, \"rb\") as f:\n",
    "    files = {\"file\": f}\n",
    "    # We send the model choice as form data\n",
    "    data = {\"model_name\": \"bert\"}\n",
    "\n",
    "    print(f\"üì§ Uploading {file_path}...\")\n",
    "    response = requests.post(url, files=files, data=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Ingestion Success!\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")"
   ],
   "id": "3b9a42d3bb1ef0de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading data/testPDFs/Attention is all you need.pdf...\n",
      "‚úÖ Ingestion Success!\n",
      "{'filename': 'Attention is all you need.pdf', 'message': 'Ingestion successful', 'chunks_added': 28, 'parent_id': 'arXiv:1706.03762v7'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T11:15:51.571024700Z",
     "start_time": "2026-01-02T11:15:49.942522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8000/query\"\n",
    "payload = {\n",
    "    \"query_text\": \"What are the architecture details of the transformer?\",\n",
    "    \"n_results\": 2,\n",
    "    \"model_name\": \"bert\"\n",
    "}\n",
    "\n",
    "print(f\"üîç Searching for: '{payload['query_text']}'\")\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    print(f\"‚úÖ Found {len(results)} matches:\\n\")\n",
    "    for res in results:\n",
    "        print(f\"üìÑ Doc ID: {res['doc_id']}\")\n",
    "        print(f\"üìä Score:  {res['score']:.4f}\")\n",
    "        print(f\"üìù Content: {res['content'][:100]}...\") # Preview first 100 chars\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")"
   ],
   "id": "e8204d15f36903ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'What are the architecture details of the transformer?'\n",
      "‚úÖ Found 2 matches:\n",
      "\n",
      "üìÑ Doc ID: arXiv:1706.03762v7#6.2_Model_Variations\n",
      "üìä Score:  0.4787\n",
      "üìù Content: To evaluate the importance of different components of the Transformer, we varied our base model in d...\n",
      "------------------------------\n",
      "üìÑ Doc ID: arXiv:1706.03762v7#5.4_Regularization\n",
      "üìä Score:  0.4823\n",
      "üìù Content: We employ three types of regularization during training: 7 Table 2: The Transformer achieves better ...\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T11:16:11.619384200Z",
     "start_time": "2026-01-02T11:16:11.593950700Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "41eb921ef43673e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 'arXiv:1706.03762v7#6.2_Model_Variations',\n",
       "  'score': 0.4787372350692749,\n",
       "  'content': 'To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.',\n",
       "  'metadata': {'section': '6.2 Model Variations',\n",
       "   'parent_id': 'arXiv:1706.03762v7',\n",
       "   'filename': 'Attention is all you need.pdf',\n",
       "   'authors': 'Ashish Vaswani ‚àó Google Brain avaswani@google.com, Noam Shazeer ‚àó Google Brain noam@google.com, Llion Jones ‚àó Google Research llion@google.com, Niki Parmar ‚àó Google Research nikip@google.com, Aidan N. Gomez ‚àó ‚Ä† University of Toronto aidan@cs.toronto.edu, Jakob Uszkoreit ‚àó Google Research usz@google.com, ≈Åukasz Kaiser ‚àó Google Brain lukaszkaiser@google.com, Illia Polosukhin ‚àó ‚Ä°, illia.polosukhin@gmail.com',\n",
       "   'title': 'Attention Is All You Need'}},\n",
       " {'doc_id': 'arXiv:1706.03762v7#5.4_Regularization',\n",
       "  'score': 0.4822516441345215,\n",
       "  'content': 'We employ three types of regularization during training: 7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0 . 1 . Label Smoothing During training, we employed label smoothing of value œµ ls = 0 . 1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.',\n",
       "  'metadata': {'parent_id': 'arXiv:1706.03762v7',\n",
       "   'title': 'Attention Is All You Need',\n",
       "   'filename': 'Attention is all you need.pdf',\n",
       "   'section': '5.4 Regularization',\n",
       "   'authors': 'Ashish Vaswani ‚àó Google Brain avaswani@google.com, Noam Shazeer ‚àó Google Brain noam@google.com, Llion Jones ‚àó Google Research llion@google.com, Niki Parmar ‚àó Google Research nikip@google.com, Aidan N. Gomez ‚àó ‚Ä† University of Toronto aidan@cs.toronto.edu, Jakob Uszkoreit ‚àó Google Research usz@google.com, ≈Åukasz Kaiser ‚àó Google Brain lukaszkaiser@google.com, Illia Polosukhin ‚àó ‚Ä°, illia.polosukhin@gmail.com'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4303e869272b7f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
