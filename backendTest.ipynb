{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-31T13:59:04.860541Z",
     "start_time": "2025-12-31T13:59:04.793157500Z"
    }
   },
   "source": "import requests",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:02:57.786216800Z",
     "start_time": "2025-12-31T14:02:42.020136600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8000/ingest\"\n",
    "file_path = \"./data/testPDFs/BERT.pdf\"\n",
    "\n",
    "# We open the file in binary mode\n",
    "with open(file_path, \"rb\") as f:\n",
    "    files = {\"file\": f}\n",
    "    # We send the model choice as form data\n",
    "    data = {\"model_name\": \"bert\"}\n",
    "\n",
    "    print(f\"üì§ Uploading {file_path}...\")\n",
    "    response = requests.post(url, files=files, data=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Ingestion Success!\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")"
   ],
   "id": "3b9a42d3bb1ef0de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading ./data/testPDFs/BERT.pdf...\n",
      "‚úÖ Ingestion Success!\n",
      "{'filename': 'BERT.pdf', 'message': 'Ingestion successful', 'chunks_added': 31, 'parent_id': 'arXiv:1810.04805v2'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:07:20.907408700Z",
     "start_time": "2025-12-31T14:07:18.744599100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8000/query\"\n",
    "payload = {\n",
    "    \"query_text\": \"What are the architecture details of the transformer?\",\n",
    "    \"n_results\": 2,\n",
    "    \"model_name\": \"bert\"\n",
    "}\n",
    "\n",
    "print(f\"üîç Searching for: '{payload['query_text']}'\")\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    print(f\"‚úÖ Found {len(results)} matches:\\n\")\n",
    "    for res in results:\n",
    "        print(f\"üìÑ Doc ID: {res['doc_id']}\")\n",
    "        print(f\"üìä Score:  {res['score']:.4f}\")\n",
    "        print(f\"üìù Content: {res['content'][:100]}...\") # Preview first 100 chars\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")"
   ],
   "id": "e8204d15f36903ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'What are the architecture details of the transformer?'\n",
      "‚úÖ Found 2 matches:\n",
      "\n",
      "üìÑ Doc ID: arXiv:1810.04805v2#4_Experiments\n",
      "üìä Score:  0.4823\n",
      "üìù Content: In this section, we present BERT fine-tuning results on 11 NLP tasks....\n",
      "------------------------------\n",
      "üìÑ Doc ID: arXiv:1810.04805v2#2.2_Unsupervised_Fine-tuning_Approaches\n",
      "üìä Score:  0.4918\n",
      "üìù Content: As with the feature-based approaches, the first works in this direction only pre-trained word embedd...\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:07:50.358121600Z",
     "start_time": "2025-12-31T14:07:50.332228500Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "41eb921ef43673e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 'arXiv:1810.04805v2#4_Experiments',\n",
       "  'score': 0.48231053352355957,\n",
       "  'content': 'In this section, we present BERT fine-tuning results on 11 NLP tasks.',\n",
       "  'metadata': {'authors': 'Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova, Google AI Language, { jacobdevlin,mingweichang,kentonl,kristout } @google.com',\n",
       "   'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "   'section': '4 Experiments',\n",
       "   'parent_id': 'arXiv:1810.04805v2',\n",
       "   'filename': 'BERT.pdf'}},\n",
       " {'doc_id': 'arXiv:1810.04805v2#2.2_Unsupervised_Fine-tuning_Approaches',\n",
       "  'score': 0.49176812171936035,\n",
       "  'content': \"As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008). More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers). BERT BERT E [CLS] E 1 E [SEP] ... E N E 1 ' ... E M ' C T 1 T [SEP] ... T N T 1 ' ... T M ' [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Question Paragraph Start/End Span BERT E [CLS] E 1 E [SEP] ... E N E 1 ' ... E M ' C T 1 T [SEP] ... T N T 1 ' ... T M ' [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Masked Sentence A Masked Sentence B Pre-training Fine-Tuning NSP Mask LM Mask LM Unlabeled Sentence A and B Pair SQuAD Question Answer Pair NER MNLI ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\",\n",
       "  'metadata': {'section': '2.2 Unsupervised Fine-tuning Approaches',\n",
       "   'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "   'parent_id': 'arXiv:1810.04805v2',\n",
       "   'authors': 'Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova, Google AI Language, { jacobdevlin,mingweichang,kentonl,kristout } @google.com',\n",
       "   'filename': 'BERT.pdf'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4303e869272b7f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
