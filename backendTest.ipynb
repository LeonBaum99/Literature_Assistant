{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-02T11:02:52.715610300Z",
     "start_time": "2026-01-02T11:02:52.647561500Z"
    }
   },
   "source": "import requests",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:03:44.024121700Z",
     "start_time": "2026-01-02T13:02:56.859808800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8000/ingest\"\n",
    "file_path = \"data/testPDFs/BERT.pdf\"\n",
    "\n",
    "# We open the file in binary mode\n",
    "with open(file_path, \"rb\") as f:\n",
    "    files = {\"file\": f}\n",
    "    # We send the model choice as form data\n",
    "    data = {\"model_name\": \"bert\"}\n",
    "\n",
    "    print(f\"üì§ Uploading {file_path}...\")\n",
    "    response = requests.post(url, files=files, data=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Ingestion Success!\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")"
   ],
   "id": "3b9a42d3bb1ef0de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading data/testPDFs/BERT.pdf...\n",
      "‚úÖ Ingestion Success!\n",
      "{'filename': 'BERT.pdf', 'message': 'Ingestion successful', 'chunks_added': 31, 'parent_id': 'arXiv:1810.04805v2'}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:03:51.655815700Z",
     "start_time": "2026-01-02T13:03:49.890125400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8000/query\"\n",
    "payload = {\n",
    "    \"query_text\": \"What are the architecture details of the transformer?\",\n",
    "    \"n_results\": 2,\n",
    "    \"model_name\": \"bert\"\n",
    "}\n",
    "\n",
    "print(f\"üîç Searching for: '{payload['query_text']}'\")\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    print(f\"‚úÖ Found {len(results)} matches:\\n\")\n",
    "    for res in results:\n",
    "        print(f\"üìÑ Doc ID: {res['doc_id']}\")\n",
    "        print(f\"üìä Score:  {res['score']:.4f}\")\n",
    "        print(f\"üìù Content: {res['content'][:100]}...\") # Preview first 100 chars\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(f\"‚ùå Error {response.status_code}: {response.text}\")"
   ],
   "id": "e8204d15f36903ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'What are the architecture details of the transformer?'\n",
      "‚úÖ Found 2 matches:\n",
      "\n",
      "üìÑ Doc ID: arXiv:1810.04805v2#4_Experiments\n",
      "üìä Score:  0.4823\n",
      "üìù Content: In this section, we present BERT fine-tuning results on 11 NLP tasks....\n",
      "------------------------------\n",
      "üìÑ Doc ID: arXiv:1810.04805v2#2.2_Unsupervised_Fine-tuning_Approaches\n",
      "üìä Score:  0.4918\n",
      "üìù Content: As with the feature-based approaches, the first works in this direction only pre-trained word embedd...\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:03:57.990317400Z",
     "start_time": "2026-01-02T13:03:57.588324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting specific documents\n",
    "url = \"http://localhost:8000/delete\"\n",
    "payload = {\n",
    "    \"model_name\": \"bert\",\n",
    "    \"doc_ids\": [\"arXiv_1706.03762v7#Introduction\", \"arXiv_1706.03762v7#Conclusion\"]\n",
    "}\n",
    "requests.post(url, json=payload)"
   ],
   "id": "4303e869272b7f7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:04:10.819257800Z",
     "start_time": "2026-01-02T13:04:10.373275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Resetting the entire collection for a model\n",
    "url = \"http://localhost:8000/reset\"\n",
    "payload = {\"model_name\": \"bert\"}\n",
    "requests.post(url, json=payload)"
   ],
   "id": "ae7780165ccc755a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:04:02.475690Z",
     "start_time": "2026-01-02T13:04:02.447184200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List items in the collection\n",
    "url = \"http://localhost:8000/list-ids\"\n",
    "\n",
    "params = {\n",
    "    \"model_name\": \"bert\",\n",
    "    \"limit\": 5\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "print(response.json())"
   ],
   "id": "7abeeefdf43a7fae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bert', 'ids': ['arXiv:1810.04805v2#Preamble', 'arXiv:1810.04805v2#BERT:_Pre-training_of_Deep_Bidirectional_Transform', 'arXiv:1810.04805v2#Abstract', 'arXiv:1810.04805v2#1_Introduction', 'arXiv:1810.04805v2#2_Related_Work'], 'total_in_batch': 5}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:04:14.853180700Z",
     "start_time": "2026-01-02T13:04:14.811167500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Debug embeddings for a specific text\n",
    "url = \"http://localhost:8000/debug/embed\"\n",
    "payload = {\n",
    "    \"text\": \"The transformer architecture allows for parallelization.\",\n",
    "    \"model_name\": \"bert\"\n",
    "}\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json()[\"dimension\"])"
   ],
   "id": "e9cb0c6c43239e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T13:04:23.341093400Z",
     "start_time": "2026-01-02T13:04:16.139348300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse a PDF and get JSON\n",
    "url = \"http://localhost:8000/debug/parse-pdf\"\n",
    "files = {'file': open('./data/testPDFs/BERT.pdf', 'rb')}\n",
    "\n",
    "response = requests.post(url, files=files)\n",
    "data = response.json()\n",
    "\n",
    "print(\"Title detected:\", data['metadata_extracted']['title'])\n",
    "print(\"Sections found:\", list(data['sections'].keys()))"
   ],
   "id": "4dfdfe7a61d26fb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title detected: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "Sections found: ['Preamble', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Abstract', '1 Introduction', '2 Related Work', '2.1 Unsupervised Feature-based Approaches', '2.2 Unsupervised Fine-tuning Approaches', '2.3 Transfer Learning from Supervised Data', '3 BERT', '3.1 Pre-training BERT', '3.2 Fine-tuning BERT', '4 Experiments', '4.1 GLUE', '4.2 SQuAD v1.1', '4.3 SQuAD v2.0', '4.4 SWAG', '5 Ablation Studies', '5.1 Effect of Pre-training Tasks', '5.2 Effect of Model Size', '5.3 Feature-based Approach with BERT', '6 Conclusion', 'References', \"Appendix for 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\", 'A.1 Illustration of the Pre-training Tasks', 'A.2 Pre-training Procedure', 'A.3 Fine-tuning Procedure', 'A.4 Comparison of BERT, ELMo ,and OpenAI GPT', 'A.5 Illustrations of Fine-tuning on Different Tasks', 'B.1 Detailed Descriptions for the GLUE Benchmark Experiments.', 'C.1 Effect of Number of Training Steps', 'C.2 Ablation for Different Masking Procedures']\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b009b9bb2372a2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
