{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1b9735",
   "metadata": {},
   "source": [
    "# End-to-End RAG Pipeline - Evaluation\n",
    "\n",
    "Full RAG pipeline: PDF ingestion → Improved chunking → Embedding → ChromaDB → Retrieval → LLM answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9810b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\kronask\\OneDrive - TU Wien\\TU Wien\\3. Semester\\GenAI\\GenAI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Change to parent directory for config.yaml access\n",
    "parent_dir = Path.cwd().parent\n",
    "os.chdir(parent_dir)\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from pdfProcessing.docling_PDF_processor import DoclingPDFProcessor\n",
    "from pdfProcessing.chunking import create_chunks_from_sections\n",
    "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
    "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
    "from backend.services.embedder import EmbeddingService\n",
    "from backend.services.vector_db import VectorDBService\n",
    "from backend.services.rag_answer_service import ChromaRagRetriever\n",
    "from llmAG.rag.pipeline import RagPipeline\n",
    "from llmAG.llm import build_llm\n",
    "from zotero_integration.metadata_loader import ZoteroMetadataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bb01b",
   "metadata": {},
   "source": [
    "## 1. Initialize Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296ffd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Zotero metadata loader...\n",
      "Loaded 24 items from zotero_export_20260112_191851.json\n",
      "✓ Zotero metadata loaded\n",
      "Initializing PDF processor...\n",
      "Initializing Docling Converter...\n",
      "CUDA detected. Using GPU for PDF Processing.\n",
      "Initializing embedding service...\n",
      "Loading Model Key: bert...\n",
      "Loading Alibaba-NLP/gte-modernbert-base on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:06:17,240 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB...\n",
      "Initializing LLM (Ollama mistral-nemo)...\n",
      "✓ LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EMBEDDER_TYPE = \"bert\"  # \"bert\" or \"qwen\"\n",
    "CHROMA_PATH = \"./backend/chroma_db\"  # Use same DB as backend\n",
    "MAX_CHUNK_SIZE = 2500\n",
    "OVERLAP_SIZE = 200\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "# Database Management\n",
    "CLEAR_DB_ON_RUN = False  # Set to True to clear DB and re-ingest all PDFs\n",
    "\n",
    "# Set Ollama URL for local execution (not Docker)\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "# Initialize Zotero metadata loader\n",
    "print(\"Initializing Zotero metadata loader...\")\n",
    "try:\n",
    "    zotero_loader = ZoteroMetadataLoader()\n",
    "    print(f\"✓ Zotero metadata loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Zotero metadata not available: {e}\")\n",
    "    print(\"  Will fall back to Docling extraction\")\n",
    "    zotero_loader = None\n",
    "\n",
    "# Initialize PDF processor\n",
    "print(\"Initializing PDF processor...\")\n",
    "processor = DoclingPDFProcessor()\n",
    "\n",
    "# Initialize embedding service\n",
    "print(\"Initializing embedding service...\")\n",
    "embed_service = EmbeddingService()\n",
    "# Load the model to have direct access to embedder for manual operations\n",
    "embedder = embed_service.load_model(EMBEDDER_TYPE)\n",
    "\n",
    "# Initialize ChromaDB service\n",
    "print(\"Initializing ChromaDB...\")\n",
    "db_service = VectorDBService(\n",
    "    db_path=CHROMA_PATH,\n",
    "    collection_names={\n",
    "        \"bert\": \"scientific_papers_bert\",\n",
    "        \"qwen\": \"scientific_papers_qwen\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "print(\"Initializing LLM (Ollama mistral-nemo)...\")\n",
    "try:\n",
    "    llm = build_llm(model=\"mistral-nemo\", temperature=0.1)\n",
    "    print(\"✓ LLM initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ LLM initialization failed: {e}\")\n",
    "    print(\"  Make sure Ollama app is running (check system tray)\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e33fbc",
   "metadata": {},
   "source": [
    "## 2. Database Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c987461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATABASE STATUS\n",
      "================================================================================\n",
      "Current database status (model: bert)\n",
      "  Chunks in database: 538\n",
      "  ✓ Database ready for evaluation\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check current database state\n",
    "print(f\"{'='*80}\")\n",
    "print(\"DATABASE STATUS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "try:\n",
    "    collection = db_service.get_collection(EMBEDDER_TYPE)\n",
    "    chunk_count = collection.count()\n",
    "    \n",
    "    print(f\"Current database status (model: {EMBEDDER_TYPE})\")\n",
    "    print(f\"  Chunks in database: {chunk_count}\")\n",
    "    \n",
    "    if chunk_count == 0:\n",
    "        print(f\"  ⚠ Database is empty - run ingestion first\")\n",
    "    else:\n",
    "        print(f\"  ✓ Database ready for evaluation\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808221d2",
   "metadata": {},
   "source": [
    "## 3. Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d26c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 16 questions from eval_dataset.json\n"
     ]
    }
   ],
   "source": [
    "def load_eval_dataset(filename=\"eval_dataset.json\"):\n",
    "    file_path = Path.cwd() / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"⚠ Warning: {filename} not found in {Path.cwd()}\")\n",
    "        return []\n",
    "        \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(data)} questions from {filename}\")\n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "eval_dataset = load_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c065b",
   "metadata": {},
   "source": [
    "## 4. Initialize RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369d3045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG components\n",
    "retriever = ChromaRagRetriever(\n",
    "    embed_service=embed_service,\n",
    "    db_service=db_service,\n",
    "    model_name=EMBEDDER_TYPE\n",
    ")\n",
    "\n",
    "# Initialize RAG pipeline (builds LLM internally)\n",
    "rag_pipeline = RagPipeline(\n",
    "    retriever=retriever,\n",
    "    model=\"mistral-nemo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "print(\"✓ RAG pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5ead7",
   "metadata": {},
   "source": [
    "## 5. RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19110433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class RAGEvaluator:\n",
    "#     def __init__(self, pipeline):\n",
    "#         self.pipeline = pipeline\n",
    "#         self.results = []\n",
    "\n",
    "#     def evaluate(self, dataset, top_k=5):\n",
    "#         print(f\"Starting evaluation of {len(dataset)} questions...\")\n",
    "#         self.results = []\n",
    "        \n",
    "#         for item in tqdm(dataset):\n",
    "#             question = item['question']\n",
    "#             target_tag = item.get('target_tag')\n",
    "#             tier = item.get('tier')\n",
    "            \n",
    "#             start_time = time.time()\n",
    "#             try:\n",
    "#                 # Run RAG Pipeline\n",
    "#                 response = self.pipeline.run(question, k=top_k, include_sources=True)\n",
    "#                 elapsed = time.time() - start_time\n",
    "                \n",
    "#                 # 1. Retrieval Evaluation (Source Matching)\n",
    "#                 # Check if ANY of the retrieved docs contain the target tag in their title\n",
    "#                 retrieved_titles = [src.metadata.get('title', '').lower() for src in response.sources]\n",
    "                \n",
    "#                 hit = False\n",
    "#                 if target_tag:\n",
    "#                     tag_map = {\n",
    "#                         \"FAST\": [\"fast\", \"autonomous high-resolution scanning\"],\n",
    "#                         \"liquid lenses\": [\"liquid lenses\", \"zhang\"],\n",
    "#                         \"autofocus\": [\"autofocus\", \"zhang\", \"rebuffi\"],\n",
    "#                         \"ptychography\": [\"ptychography\", \"schloz\"],\n",
    "#                         \"alignment\": [\"alignment\", \"morris\", \"beamlines\"],\n",
    "#                         \"optics\": [\"adaptive optics\", \"nousiainen\", \"mareev\"]\n",
    "#                     }\n",
    "                    \n",
    "#                     search_terms = tag_map.get(target_tag, [target_tag.lower()])\n",
    "                    \n",
    "#                     # Check for hit\n",
    "#                     for title in retrieved_titles:\n",
    "#                         if any(term in title for term in search_terms):\n",
    "#                             hit = True\n",
    "#                             break\n",
    "#                 else:\n",
    "#                     hit = None # No target tag defined (Synthesis questions)\n",
    "\n",
    "#                 # Store Result\n",
    "#                 self.results.append({\n",
    "#                     \"Tier\": tier,\n",
    "#                     \"Question\": question,\n",
    "#                     \"Target_Tag\": target_tag,\n",
    "#                     \"Hit\": hit,\n",
    "#                     \"Answer\": response.answer,\n",
    "#                     \"Sources\": \" | \".join([t[:50] + \"...\" for t in retrieved_titles]),\n",
    "#                     \"Latency\": round(elapsed, 2)\n",
    "#                 })\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error on question: {question[:30]}... {e}\")\n",
    "#                 self.results.append({\n",
    "#                     \"Tier\": tier,\n",
    "#                     \"Question\": question,\n",
    "#                     \"Target_Tag\": target_tag,\n",
    "#                     \"Hit\": False,\n",
    "#                     \"Answer\": f\"ERROR: {str(e)}\",\n",
    "#                     \"Sources\": \"\",\n",
    "#                     \"Latency\": 0\n",
    "#                 })\n",
    "\n",
    "#         return pd.DataFrame(self.results)\n",
    "\n",
    "# # Initialize and Run\n",
    "# evaluator = RAGEvaluator(rag_pipeline)\n",
    "# df_results = evaluator.evaluate(eval_dataset, top_k=5)\n",
    "\n",
    "# # Display Summary\n",
    "# print(\"\\n=== Evaluation Summary ===\")\n",
    "# if 'Hit' in df_results.columns:\n",
    "#     # Filter out synthesis questions (Hit=None) for accuracy calc\n",
    "#     measurable = df_results.dropna(subset=['Hit'])\n",
    "#     print(f\"Retrieval Hit Rate (Targeted Questions): {measurable['Hit'].mean():.2%}\")\n",
    "\n",
    "# print(f\"Average Latency: {df_results['Latency'].mean():.2f}s\")\n",
    "# df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bf0be",
   "metadata": {},
   "source": [
    "## 6. Save and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbca5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_filename = \"rag_evaluation_results.csv\"\n",
    "# df_results.to_csv(output_filename, index=False)\n",
    "# print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "# # Inspect specifically the \"Missed\" items to debug retrieval\n",
    "# print(\"\\n=== Missed Retrieval Questions ===\")\n",
    "# missed = df_results[(df_results['Hit'] == False) & (df_results['Target_Tag'].notna())]\n",
    "# if not missed.empty:\n",
    "#     for _, row in missed.iterrows():\n",
    "#         print(f\"Q: {row['Question']}\")\n",
    "#         print(f\"Target: {row['Target_Tag']}\")\n",
    "#         print(f\"Got Sources: {row['Sources']}\\n\")\n",
    "# else:\n",
    "#     print(\"Great! No retrieval misses on targeted questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa9749",
   "metadata": {},
   "source": [
    "## 7. Detailed Evaluation by Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14184a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Breakdown by tier and target tag\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"EVALUATION BREAKDOWN BY TIER\")\n",
    "# print(f\"{'='*80}\\n\")\n",
    "\n",
    "# for tier in sorted(df_results['Tier'].unique()):\n",
    "#     tier_data = df_results[df_results['Tier'] == tier]\n",
    "#     print(f\"\\nTier {tier}:\")\n",
    "#     print(f\"  Total Questions: {len(tier_data)}\")\n",
    "    \n",
    "#     with_tags = tier_data[tier_data['Target_Tag'].notna()]\n",
    "#     if len(with_tags) > 0:\n",
    "#         hit_rate = with_tags['Hit'].mean()\n",
    "#         print(f\"  Retrieval Hit Rate: {hit_rate:.2%} ({int(with_tags['Hit'].sum())}/{len(with_tags)})\")\n",
    "    \n",
    "#     print(f\"  Avg Latency: {tier_data['Latency'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7a60b",
   "metadata": {},
   "source": [
    "## 8. Question-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f23a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show all questions with their results\n",
    "# display_cols = ['Tier', 'Target_Tag', 'Question', 'Hit', 'Latency']\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"ALL EVALUATION RESULTS\")\n",
    "# print(f\"{'='*80}\\n\")\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "# print(df_results[display_cols].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad374f6",
   "metadata": {},
   "source": [
    "##  Enhanced Evaluation (Chunk + Multi-Paper Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a08e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENHANCED EVALUATION (Chunk-level + Multi-paper)\n",
      "================================================================================\n",
      "\n",
      "Starting enhanced evaluation of 16 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]2026-01-18 19:07:48,352 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  6%|▋         | 1/16 [00:13<03:15, 13.06s/it]2026-01-18 19:08:03,289 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 12%|█▎        | 2/16 [00:36<04:26, 19.03s/it]2026-01-18 19:08:27,475 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 19%|█▉        | 3/16 [01:00<04:37, 21.38s/it]2026-01-18 19:08:49,825 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 4/16 [01:13<03:36, 18.07s/it]2026-01-18 19:09:01,860 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 31%|███▏      | 5/16 [01:35<03:33, 19.38s/it]2026-01-18 19:09:24,268 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 38%|███▊      | 6/16 [02:10<04:06, 24.69s/it]2026-01-18 19:09:59,724 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 44%|████▍     | 7/16 [03:10<05:27, 36.34s/it]2026-01-18 19:11:00,077 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 8/16 [04:04<05:36, 42.03s/it]2026-01-18 19:11:53,701 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 56%|█████▋    | 9/16 [04:32<04:23, 37.68s/it]2026-01-18 19:12:22,312 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 62%|██████▎   | 10/16 [04:45<02:59, 29.95s/it]2026-01-18 19:12:34,492 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▉   | 11/16 [05:01<02:09, 25.82s/it]2026-01-18 19:12:51,418 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 75%|███████▌  | 12/16 [06:06<02:29, 37.47s/it]2026-01-18 19:13:54,907 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 81%|████████▏ | 13/16 [06:32<01:42, 34.13s/it]2026-01-18 19:14:22,635 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 88%|████████▊ | 14/16 [07:09<01:09, 34.90s/it]2026-01-18 19:14:58,220 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 94%|█████████▍| 15/16 [07:29<00:30, 30.43s/it]2026-01-18 19:15:17,858 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 16/16 [07:56<00:00, 29.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION SUMMARY ===\n",
      "\n",
      "Tier 1-2 (Single Paper) - Exact Chunk Hit Rate: 100.00% (12/12)\n",
      "  → Avg rank of correct chunk: 1.0\n",
      "\n",
      "Tier 3 (Synthesis) - Multi-Paper Hit Rate: 100.00% (4/4)\n",
      "  → Avg papers retrieved: 2.5\n",
      "\n",
      "Average Latency (all): 29.79s\n",
      "\n",
      "================================================================================\n",
      "DETAILED RESULTS\n",
      "================================================================================\n",
      " Tier                                                        Question    Target_Tag  Exact_Chunk_Match  Chunk_Rank  Num_Papers Multi_Paper_Match                              Papers  Latency\n",
      "    1 What physical quantity is the controller changing (the actua... liquid lenses               True           1           2              None       Rebuffi et al. | Zhang et al.    13.06\n",
      "    1 Which classic search methods are used as baselines in the DR...     autofocus               True           1           2              None       Rebuffi et al. | Zhang et al.    23.21\n",
      "    1 What is the main objective of 'adaptive scanning' compared t...  ptychography               True           1           1              None                       Schloz et al.    24.18\n",
      "    1 What does the metric QSSIM represent in the ptychography eva...  ptychography               True           1           2              None       Morris et al. | Schloz et al.    13.00\n",
      "    1 What are the discrete actions available to the agent (action...     alignment               True           1           3              None      Morris et al. | Rebuffi et al.    21.71\n",
      "    1 What are the two main limitations of a well-tuned integrator...        optics               True           1           2              None Nousiainen et al. | Kuprikov et al.    35.00\n",
      "    2 List the reward hyperparameters (e.g., alpha, beta, mu, delt...     autofocus               True           1           2              None       Rebuffi et al. | Zhang et al.    60.33\n",
      "    2 What are the two action-set designs in the DRL autofocus pap...     autofocus               True           1           2              None       Rebuffi et al. | Zhang et al.    54.22\n",
      "    2 What is the reported speed improvement versus a named baseli...     autofocus               True           1           2              None       Rebuffi et al. | Zhang et al.    28.10\n",
      "    2 What are the ROP reconstruction settings (batch size, step s...  ptychography               True           1           2              None       Morris et al. | Schloz et al.    12.65\n",
      "    2 Summarize the encoder/feature-extractor architecture used to...  ptychography               True           1           2              None       Morris et al. | Schloz et al.    16.46\n",
      "    2 From the main hyperparameter table in the AO RL paper: what ...        optics               True           1           1              None                   Nousiainen et al.    64.10\n",
      "    3 How does FAST define 'scanning efficiency,' and in what way ...          FAST               True           1           3              True        Morris et al. | Zhang et al.    26.44\n",
      "    3 Why might discretizing the action space improve stability or...     autofocus               True           1           2              True      Zhang et al. | Kuprikov et al.    36.68\n",
      "    3 The ptychography paper uses a particular discount factor set...  ptychography               True           2           2              True       Morris et al. | Schloz et al.    20.06\n",
      "    3 Where does the exploration–exploitation trade-off appear in ...          FAST               True           1           3              True Morris et al. | Volk and Abolhasani    27.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced evaluator: tracks exact chunks (Tier 1-2) AND multi-paper retrieval (Tier 3)\n",
    "\n",
    "class EnhancedRAGEvaluator:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.results = []\n",
    "\n",
    "    def evaluate(self, dataset, top_k=5):\n",
    "        print(f\"Starting enhanced evaluation of {len(dataset)} questions...\")\n",
    "        self.results = []\n",
    "        \n",
    "        for item in tqdm(dataset):\n",
    "            question = item['question']\n",
    "            target_tag = item.get('target_tag')\n",
    "            tier = item.get('tier')\n",
    "            expected_chunk_id = item.get('expected_chunk_id')  # Ground truth\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Run RAG Pipeline\n",
    "                response = self.pipeline.run(question, k=top_k, include_sources=True)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Extract metadata from retrieved chunks\n",
    "                retrieved_titles = [src.metadata.get('title', '').lower() for src in response.sources]\n",
    "                retrieved_filenames = [src.metadata.get('filename', '') for src in response.sources]\n",
    "                retrieved_parents = [src.metadata.get('parent_id', '') for src in response.sources]\n",
    "                \n",
    "                # Get unique papers from retrieved chunks\n",
    "                unique_papers = list(set(retrieved_filenames))\n",
    "                num_unique_papers = len(unique_papers)\n",
    "                \n",
    "                # 1. Check for EXACT chunk match (for Tier 1-2)\n",
    "                exact_chunk_match = False\n",
    "                chunk_found_at_rank = None\n",
    "                if expected_chunk_id:\n",
    "                    # Try multiple ways to match the expected chunk ID\n",
    "                    for rank, src in enumerate(response.sources, 1):\n",
    "                        parent_id = src.metadata.get('parent_id', '')\n",
    "                        # Match by parent_id or if expected_chunk_id appears in the parent_id\n",
    "                        if parent_id == expected_chunk_id or expected_chunk_id.split('#')[0] in parent_id:\n",
    "                            exact_chunk_match = True\n",
    "                            chunk_found_at_rank = rank\n",
    "                            break\n",
    "                \n",
    "                # 2. Multi-paper match for Tier 3 (synthesis questions)\n",
    "                multi_paper_match = num_unique_papers >= 2\n",
    "                \n",
    "                # Store Result\n",
    "                self.results.append({\n",
    "                    \"Tier\": tier,\n",
    "                    \"Question\": question[:60] + \"...\" if len(question) > 60 else question,\n",
    "                    \"Target_Tag\": target_tag,\n",
    "                    \"Exact_Chunk_Match\": exact_chunk_match if expected_chunk_id else None,\n",
    "                    \"Chunk_Rank\": chunk_found_at_rank if exact_chunk_match else None,\n",
    "                    \"Num_Papers\": num_unique_papers,\n",
    "                    \"Multi_Paper_Match\": multi_paper_match if tier == 3 else None,\n",
    "                    \"Papers\": \" | \".join([p.split(' - ')[0][:30] for p in unique_papers[:2]]),\n",
    "                    \"Latency\": round(elapsed, 2)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on question: {question[:30]}... {e}\")\n",
    "                self.results.append({\n",
    "                    \"Tier\": tier,\n",
    "                    \"Question\": question[:60] + \"...\" if len(question) > 60 else question,\n",
    "                    \"Target_Tag\": target_tag,\n",
    "                    \"Exact_Chunk_Match\": False,\n",
    "                    \"Chunk_Rank\": None,\n",
    "                    \"Num_Papers\": 0,\n",
    "                    \"Multi_Paper_Match\": False,\n",
    "                    \"Papers\": f\"ERROR: {str(e)[:20]}\",\n",
    "                    \"Latency\": 0\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Run enhanced evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED EVALUATION (Chunk-level + Multi-paper)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "evaluator_enhanced = EnhancedRAGEvaluator(rag_pipeline)\n",
    "df_results_enhanced = evaluator_enhanced.evaluate(eval_dataset, top_k=5)\n",
    "\n",
    "# Display Summary\n",
    "print(\"\\n=== EVALUATION SUMMARY ===\\n\")\n",
    "\n",
    "# Tier 1-2: Exact chunk matching\n",
    "tier_12 = df_results_enhanced[df_results_enhanced['Tier'].isin([1, 2])]\n",
    "if len(tier_12) > 0:\n",
    "    chunk_match_rate = tier_12['Exact_Chunk_Match'].sum() / tier_12['Exact_Chunk_Match'].notna().sum()\n",
    "    print(f\"Tier 1-2 (Single Paper) - Exact Chunk Hit Rate: {chunk_match_rate:.2%} ({int(tier_12['Exact_Chunk_Match'].sum())}/{int(tier_12['Exact_Chunk_Match'].notna().sum())})\")\n",
    "    \n",
    "    # Show average rank when chunk is found\n",
    "    found_ranks = tier_12[tier_12['Exact_Chunk_Match'] == True]['Chunk_Rank']\n",
    "    if len(found_ranks) > 0:\n",
    "        print(f\"  → Avg rank of correct chunk: {found_ranks.mean():.1f}\")\n",
    "\n",
    "# Tier 3: Multi-paper matching\n",
    "tier_3 = df_results_enhanced[df_results_enhanced['Tier'] == 3]\n",
    "if len(tier_3) > 0:\n",
    "    multi_match_rate = tier_3['Multi_Paper_Match'].sum() / len(tier_3)\n",
    "    print(f\"\\nTier 3 (Synthesis) - Multi-Paper Hit Rate: {multi_match_rate:.2%} ({int(tier_3['Multi_Paper_Match'].sum())}/{len(tier_3)})\")\n",
    "    \n",
    "    avg_papers = tier_3['Num_Papers'].mean()\n",
    "    print(f\"  → Avg papers retrieved: {avg_papers:.1f}\")\n",
    "\n",
    "print(f\"\\nAverage Latency (all): {df_results_enhanced['Latency'].mean():.2f}s\")\n",
    "\n",
    "# Show detailed results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(df_results_enhanced.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4bc44",
   "metadata": {},
   "source": [
    "## Enhanced Evaluation with Ground Truth Chunk IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's debug what fields are actually available in the retrieved chunks\n",
    "print(\"Debugging chunk structure from pipeline response...\\n\")\n",
    "\n",
    "# Test with one question\n",
    "test_question = eval_dataset[0]['question']\n",
    "print(f\"Test question: {test_question}\\n\")\n",
    "\n",
    "response = rag_pipeline.run(test_question, k=3, include_sources=True)\n",
    "\n",
    "print(f\"Number of sources: {len(response.sources)}\\n\")\n",
    "\n",
    "for i, src in enumerate(response.sources):\n",
    "    print(f\"Source {i+1}:\")\n",
    "    print(f\"  Type: {type(src)}\")\n",
    "    print(f\"  Available attributes: {dir(src) if hasattr(src, '__dict__') else 'N/A'}\")\n",
    "    print(f\"  Metadata keys: {src.metadata.keys() if hasattr(src, 'metadata') else 'No metadata'}\")\n",
    "    print(f\"  Full metadata: {src.metadata}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
