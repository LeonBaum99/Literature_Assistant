{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1b9735",
   "metadata": {},
   "source": [
    "# End-to-End RAG Pipeline - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9810b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kronask\\.conda\\envs\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\kronask\\Documents\\TU\\3. Semester\\GenAI\\GenAI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Change to parent directory for config.yaml access\n",
    "parent_dir = Path.cwd().parent\n",
    "os.chdir(parent_dir)\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from pdfProcessing.docling_PDF_processor import DoclingPDFProcessor\n",
    "from pdfProcessing.chunking import create_chunks_from_sections\n",
    "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
    "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
    "from backend.services.embedder import EmbeddingService\n",
    "from backend.services.vector_db import VectorDBService\n",
    "from backend.services.rag_answer_service import ChromaRagRetriever\n",
    "from llmAG.rag.pipeline import RagPipeline\n",
    "from llmAG.llm import build_llm\n",
    "from zotero_integration.metadata_loader import ZoteroMetadataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bb01b",
   "metadata": {},
   "source": [
    "## 1. Initialize Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296ffd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Zotero metadata loader...\n",
      "Loaded 24 items from zotero_export_20260112_191851.json\n",
      "✓ Zotero metadata loaded\n",
      "Initializing PDF processor...\n",
      "Initializing Docling Converter...\n",
      "CUDA not found. Using CPU for PDF Processing.\n",
      "Initializing embedding service...\n",
      "Loading Model Key: bert...\n",
      "Loading Alibaba-NLP/gte-modernbert-base on cpu...\n",
      "Initializing ChromaDB...\n",
      "Initializing LLM (Ollama mistral-nemo)...\n",
      "✓ LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EMBEDDER_TYPE = \"bert\"  # \"bert\" or \"qwen\"\n",
    "CHROMA_PATH = \"./backend/chroma_db\"  # Use same DB as backend\n",
    "MAX_CHUNK_SIZE = 2500\n",
    "OVERLAP_SIZE = 200\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "# Database Management\n",
    "CLEAR_DB_ON_RUN = False  # Set to True to clear DB and re-ingest all PDFs\n",
    "\n",
    "# Set Ollama URL for local execution (not Docker)\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "# Initialize Zotero metadata loader\n",
    "print(\"Initializing Zotero metadata loader...\")\n",
    "try:\n",
    "    zotero_loader = ZoteroMetadataLoader()\n",
    "    print(f\"✓ Zotero metadata loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Zotero metadata not available: {e}\")\n",
    "    print(\"  Will fall back to Docling extraction\")\n",
    "    zotero_loader = None\n",
    "\n",
    "# Initialize PDF processor\n",
    "print(\"Initializing PDF processor...\")\n",
    "processor = DoclingPDFProcessor()\n",
    "\n",
    "# Initialize embedding service\n",
    "print(\"Initializing embedding service...\")\n",
    "embed_service = EmbeddingService()\n",
    "# Load the model to have direct access to embedder for manual operations\n",
    "embedder = embed_service.load_model(EMBEDDER_TYPE)\n",
    "\n",
    "# Initialize ChromaDB service\n",
    "print(\"Initializing ChromaDB...\")\n",
    "db_service = VectorDBService(\n",
    "    db_path=CHROMA_PATH,\n",
    "    collection_names={\n",
    "        \"bert\": \"scientific_papers_bert\",\n",
    "        \"qwen\": \"scientific_papers_qwen\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "print(\"Initializing LLM (Ollama mistral-nemo)...\")\n",
    "try:\n",
    "    llm = build_llm(model=\"mistral-nemo\", temperature=0.1)\n",
    "    print(\"✓ LLM initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ LLM initialization failed: {e}\")\n",
    "    print(\"  Make sure Ollama app is running (check system tray)\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e33fbc",
   "metadata": {},
   "source": [
    "## 2. Database Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c987461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATABASE STATUS\n",
      "================================================================================\n",
      "Current database status (model: bert)\n",
      "  Chunks in database: 304\n",
      "  ✓ Database ready for evaluation\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check current database state\n",
    "print(f\"{'='*80}\")\n",
    "print(\"DATABASE STATUS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "try:\n",
    "    collection = db_service.get_collection(EMBEDDER_TYPE)\n",
    "    chunk_count = collection.count()\n",
    "    \n",
    "    print(f\"Current database status (model: {EMBEDDER_TYPE})\")\n",
    "    print(f\"  Chunks in database: {chunk_count}\")\n",
    "    \n",
    "    if chunk_count == 0:\n",
    "        print(f\"  ⚠ Database is empty - run ingestion first\")\n",
    "    else:\n",
    "        print(f\"  ✓ Database ready for evaluation\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808221d2",
   "metadata": {},
   "source": [
    "## 3. Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d26c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\kronask\\Documents\\TU\\3. Semester\\GenAI\\GenAI\n",
      "✓ Loaded 16 questions from c:\\Users\\kronask\\Documents\\TU\\3. Semester\\GenAI\\GenAI\\eval_dataset.json\n"
     ]
    }
   ],
   "source": [
    "def load_eval_dataset(filename=\"eval_dataset.json\"):\n",
    "    potential_dirs = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path(__file__).parent.parent if '__file__' in dir() else None,\n",
    "        Path(\"C:/Users/kronask/Documents/TU/3. Semester/GenAI/GenAI\"),\n",
    "    ]\n",
    "    \n",
    "    for directory in potential_dirs:\n",
    "        if directory is None:\n",
    "            continue\n",
    "        file_path = directory / filename\n",
    "        if file_path.exists():\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"✓ Loaded {len(data)} questions from {file_path}\")\n",
    "            return data\n",
    "    \n",
    "    print(f\"⚠ Warning: {filename} not found\")\n",
    "    return []\n",
    "\n",
    "# Load the data\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "eval_dataset = load_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c065b",
   "metadata": {},
   "source": [
    "## 4. Initialize RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369d3045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG components\n",
    "retriever = ChromaRagRetriever(\n",
    "    embed_service=embed_service,\n",
    "    db_service=db_service,\n",
    "    model_name=EMBEDDER_TYPE\n",
    ")\n",
    "\n",
    "# Initialize RAG pipeline (builds LLM internally)\n",
    "rag_pipeline = RagPipeline(\n",
    "    retriever=retriever,\n",
    "    model=\"mistral-nemo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "print(\"✓ RAG pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5ead7",
   "metadata": {},
   "source": [
    "## 5. RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad374f6",
   "metadata": {},
   "source": [
    "##  Enhanced Evaluation (Chunk + Multi-Paper Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a08e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_dataset loaded: 16 items\n",
      "\n",
      "================================================================================\n",
      "ENHANCED EVALUATION (Chunk-level + Multi-paper + Answer Quality)\n",
      "================================================================================\n",
      "\n",
      "Loading semantic similarity model...\n",
      "✓ Model loaded\n",
      "Starting enhanced evaluation of 16 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 16/16 [08:23<00:00, 31.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Tier 1-2 (Single Paper) - Exact Chunk Hit Rate: 100.00% (12/12)\n",
      "  → Avg rank of correct chunk: 1.0\n",
      "\n",
      "Tier 3 (Synthesis) - Multi-Paper Hit Rate: 100.00% (4/4)\n",
      "  → Avg papers retrieved: 2.2\n",
      "  → Avg paper recall: 100.00%\n",
      "  → Avg paper precision: 45.82%\n",
      "\n",
      "Answer Quality (semantic similarity to expected):\n",
      "  → Avg answer similarity: 0.468 (16 questions with ground truth)\n",
      "  → High quality (>0.7): 0/16\n",
      "\n",
      "Average Latency (all): 31.39s\n",
      "\n",
      "================================================================================\n",
      "DETAILED RESULTS\n",
      "================================================================================\n",
      " Tier                                                        Question    Target_Tag  Exact_Chunk_Match  Chunk_Rank Semantic_Chunk_Hit Best_Chunk_Similarity  Num_Papers Multi_Paper_Match  Paper_Recall  Paper_Precision  Answer_Similarity                              Papers  Latency\n",
      "    1 What physical quantity is the controller changing (the actua... liquid lenses               True           1               None                  None           2              None           1.0            0.500              0.226       Zhang et al. | Rebuffi et al.    16.60\n",
      "    1 Which classic search methods are used as baselines in the DR...     autofocus               True           1               None                  None           2              None           1.0            0.500              0.437       Zhang et al. | Rebuffi et al.    19.39\n",
      "    1 What is the main objective of 'adaptive scanning' compared t...  ptychography               True           1               None                  None           1              None           1.0            1.000              0.636                       Schloz et al.    31.42\n",
      "    1 What does the metric QSSIM represent in the ptychography eva...  ptychography               True           1               None                  None           2              None           1.0            0.500              0.373       Morris et al. | Schloz et al.    10.48\n",
      "    1 What are the discrete actions available to the agent (action...     alignment               True           1               None                  None           3              None           1.0            0.333              0.220    Rebuffi et al. | Kuprikov et al.    18.03\n",
      "    1 What are the two main limitations of a well-tuned integrator...        optics               True           1               None                  None           2              None           1.0            0.500              0.378 Kuprikov et al. | Nousiainen et al.    26.97\n",
      "    2 List the reward hyperparameters (e.g., alpha, beta, mu, delt...     autofocus               True           1               None                  None           2              None           1.0            0.500              0.552       Zhang et al. | Rebuffi et al.    51.59\n",
      "    2 What are the two action-set designs in the DRL autofocus pap...     autofocus               True           1               None                  None           2              None           1.0            0.500              0.347       Zhang et al. | Rebuffi et al.    53.17\n",
      "    2 What is the reported speed improvement versus a named baseli...     autofocus               True           1               None                  None           2              None           1.0            0.500              0.485       Zhang et al. | Rebuffi et al.    26.31\n",
      "    2 What are the ROP reconstruction settings (batch size, step s...  ptychography               True           1               None                  None           2              None           1.0            0.500              0.642       Kandel et al. | Schloz et al.    34.39\n",
      "    2 Summarize the encoder/feature-extractor architecture used to...  ptychography               True           1               None                  None           3              None           1.0            0.333              0.283       Morris et al. | Kandel et al.    14.29\n",
      "    2 From the main hyperparameter table in the AO RL paper: what ...        optics               True           1               None                  None           1              None           1.0            1.000              0.652                   Nousiainen et al.    62.83\n",
      "    3 How does FAST define 'scanning efficiency,' and in what way ...          FAST               True           1               None                  None           2              True           1.0            0.500              0.616       Kandel et al. | Morris et al.    34.11\n",
      "    3 Why might discretizing the action space improve stability or...     autofocus               True           1               None                  None           2              True           1.0            0.500              0.457      Zhang et al. | Kuprikov et al.    33.39\n",
      "    3 The ptychography paper uses a particular discount factor set...  ptychography               True           2               None                  None           2              True           1.0            0.500              0.553       Schloz et al. | Morris et al.    15.91\n",
      "    3 Where does the exploration–exploitation trade-off appear in ...          FAST               True           1               None                  None           3              True           1.0            0.333              0.634 Kandel et al. | Volk and Abolhasani    53.33\n",
      "\n",
      "✓ Results saved to rag_evaluation_results_enhanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced evaluator: tracks exact chunks (Tier 1-2) AND multi-paper retrieval (Tier 3)\n",
    "# Plus answer quality and semantic similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class EnhancedRAGEvaluator:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.results = []\n",
    "        # Load semantic similarity model for answer quality evaluation\n",
    "        print(\"Loading semantic similarity model...\")\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ Model loaded\")\n",
    "\n",
    "    def evaluate(self, dataset, top_k=5):\n",
    "        print(f\"Starting enhanced evaluation of {len(dataset)} questions...\")\n",
    "        self.results = []\n",
    "        \n",
    "        for item in tqdm(dataset):\n",
    "            question = item['question']\n",
    "            target_tag = item.get('target_tag')\n",
    "            tier = item.get('tier')\n",
    "            expected_chunk_id = item.get('expected_chunk_id')  # Ground truth\n",
    "            expected_answer = item.get('expected_answer')  # Ground truth answer\n",
    "            expected_papers = item.get('expected_papers', [])  # Ground truth papers for Tier 3\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Run RAG Pipeline\n",
    "                response = self.pipeline.run(question, k=top_k, include_sources=True)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Extract metadata from retrieved chunks\n",
    "                retrieved_titles = [src.metadata.get('title', '').lower() for src in response.sources]\n",
    "                retrieved_filenames = [src.metadata.get('filename', '') for src in response.sources]\n",
    "                retrieved_parents = [src.metadata.get('parent_id', '') for src in response.sources]\n",
    "                \n",
    "                # Get unique papers from retrieved chunks\n",
    "                unique_papers = list(set(retrieved_filenames))\n",
    "                num_unique_papers = len(unique_papers)\n",
    "                \n",
    "                # 1. Check for EXACT chunk match (for Tier 1-2)\n",
    "                exact_chunk_match = False\n",
    "                chunk_found_at_rank = None\n",
    "                if expected_chunk_id:\n",
    "                    # Try multiple ways to match the expected chunk ID\n",
    "                    for rank, src in enumerate(response.sources, 1):\n",
    "                        parent_id = src.metadata.get('parent_id', '')\n",
    "                        # Match by parent_id or if expected_chunk_id appears in the parent_id\n",
    "                        if parent_id == expected_chunk_id or expected_chunk_id.split('#')[0] in parent_id:\n",
    "                            exact_chunk_match = True\n",
    "                            chunk_found_at_rank = rank\n",
    "                            break\n",
    "                \n",
    "                # 2. Semantic similarity to expected chunk (near-miss detection)\n",
    "                semantic_chunk_hit = None\n",
    "                best_chunk_similarity = None\n",
    "                if expected_chunk_id and not exact_chunk_match:\n",
    "                    # Get the expected chunk from database for comparison\n",
    "                    try:\n",
    "                        collection = self.pipeline.retriever.db_service.get_collection(\n",
    "                            self.pipeline.retriever.model_name\n",
    "                        )\n",
    "                        expected_docs = collection.get(ids=[expected_chunk_id])\n",
    "                        if expected_docs and expected_docs['documents']:\n",
    "                            expected_text = expected_docs['documents'][0]\n",
    "                            expected_embedding = self.semantic_model.encode([expected_text])\n",
    "                            \n",
    "                            # Compare with retrieved chunks\n",
    "                            retrieved_texts = [src.page_content for src in response.sources]\n",
    "                            retrieved_embeddings = self.semantic_model.encode(retrieved_texts)\n",
    "                            \n",
    "                            similarities = cosine_similarity(expected_embedding, retrieved_embeddings)[0]\n",
    "                            best_chunk_similarity = float(similarities.max())\n",
    "                            \n",
    "                            # Consider it a semantic hit if similarity > 0.7\n",
    "                            if best_chunk_similarity > 0.7:\n",
    "                                semantic_chunk_hit = True\n",
    "                            else:\n",
    "                                semantic_chunk_hit = False\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not compute semantic chunk similarity: {e}\")\n",
    "                \n",
    "                # 3. Multi-paper recall and precision for Tier 3 (synthesis questions)\n",
    "                multi_paper_match = num_unique_papers >= 2\n",
    "                paper_recall = None\n",
    "                paper_precision = None\n",
    "                \n",
    "                if expected_papers and len(expected_papers) > 0:\n",
    "                    # Normalize filenames for comparison\n",
    "                    retrieved_normalized = {f.lower() for f in retrieved_filenames if f}\n",
    "                    expected_normalized = {p.lower() for p in expected_papers}\n",
    "                    \n",
    "                    # Find intersection\n",
    "                    correct_papers = retrieved_normalized & expected_normalized\n",
    "                    \n",
    "                    if len(expected_normalized) > 0:\n",
    "                        paper_recall = len(correct_papers) / len(expected_normalized)\n",
    "                    \n",
    "                    if len(retrieved_normalized) > 0:\n",
    "                        paper_precision = len(correct_papers) / len(retrieved_normalized)\n",
    "                \n",
    "                # 4. Answer quality evaluation using semantic similarity\n",
    "                answer_similarity = None\n",
    "                if expected_answer:\n",
    "                    answer_embedding = self.semantic_model.encode([response.answer])\n",
    "                    expected_embedding = self.semantic_model.encode([expected_answer])\n",
    "                    answer_similarity = float(cosine_similarity(answer_embedding, expected_embedding)[0][0])\n",
    "                \n",
    "                # Store Result\n",
    "                self.results.append({\n",
    "                    \"Tier\": tier,\n",
    "                    \"Question\": question[:60] + \"...\" if len(question) > 60 else question,\n",
    "                    \"Target_Tag\": target_tag,\n",
    "                    \"Exact_Chunk_Match\": exact_chunk_match if expected_chunk_id else None,\n",
    "                    \"Chunk_Rank\": chunk_found_at_rank if exact_chunk_match else None,\n",
    "                    \"Semantic_Chunk_Hit\": semantic_chunk_hit,\n",
    "                    \"Best_Chunk_Similarity\": round(best_chunk_similarity, 3) if best_chunk_similarity else None,\n",
    "                    \"Num_Papers\": num_unique_papers,\n",
    "                    \"Multi_Paper_Match\": multi_paper_match if tier == 3 else None,\n",
    "                    \"Paper_Recall\": round(paper_recall, 3) if paper_recall is not None else None,\n",
    "                    \"Paper_Precision\": round(paper_precision, 3) if paper_precision is not None else None,\n",
    "                    \"Answer_Similarity\": round(answer_similarity, 3) if answer_similarity else None,\n",
    "                    \"Papers\": \" | \".join([p.split(' - ')[0][:30] for p in unique_papers[:2]]),\n",
    "                    \"Latency\": round(elapsed, 2)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on question: {question[:30]}... {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.results.append({\n",
    "                    \"Tier\": tier,\n",
    "                    \"Question\": question[:60] + \"...\" if len(question) > 60 else question,\n",
    "                    \"Target_Tag\": target_tag,\n",
    "                    \"Exact_Chunk_Match\": False,\n",
    "                    \"Chunk_Rank\": None,\n",
    "                    \"Semantic_Chunk_Hit\": None,\n",
    "                    \"Best_Chunk_Similarity\": None,\n",
    "                    \"Num_Papers\": 0,\n",
    "                    \"Multi_Paper_Match\": False,\n",
    "                    \"Paper_Recall\": None,\n",
    "                    \"Paper_Precision\": None,\n",
    "                    \"Answer_Similarity\": None,\n",
    "                    \"Papers\": f\"ERROR: {str(e)[:20]}\",\n",
    "                    \"Latency\": 0\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "print(f\"eval_dataset loaded: {len(eval_dataset)} items\")\n",
    "if len(eval_dataset) == 0:\n",
    "    print(\"⚠ Evaluation dataset is empty. Make sure eval_dataset.json exists and is in the current working directory.\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Looking for: {Path.cwd() / 'eval_dataset.json'}\")\n",
    "else:\n",
    "    # Run enhanced evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENHANCED EVALUATION (Chunk-level + Multi-paper + Answer Quality)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    evaluator_enhanced = EnhancedRAGEvaluator(rag_pipeline)\n",
    "    df_results_enhanced = evaluator_enhanced.evaluate(eval_dataset, top_k=5)\n",
    "\n",
    "    # Display Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    if len(df_results_enhanced) == 0:\n",
    "        print(\"No evaluation results generated.\")\n",
    "    else:\n",
    "        # Tier 1-2: Exact chunk matching\n",
    "        tier_12 = df_results_enhanced[df_results_enhanced['Tier'].isin([1, 2])]\n",
    "        if len(tier_12) > 0:\n",
    "            chunk_match_rate = tier_12['Exact_Chunk_Match'].sum() / tier_12['Exact_Chunk_Match'].notna().sum()\n",
    "            print(f\"Tier 1-2 (Single Paper) - Exact Chunk Hit Rate: {chunk_match_rate:.2%} ({int(tier_12['Exact_Chunk_Match'].sum())}/{int(tier_12['Exact_Chunk_Match'].notna().sum())})\")\n",
    "            \n",
    "            # Show average rank when chunk is found\n",
    "            found_ranks = tier_12[tier_12['Exact_Chunk_Match'] == True]['Chunk_Rank']\n",
    "            if len(found_ranks) > 0:\n",
    "                print(f\"  → Avg rank of correct chunk: {found_ranks.mean():.1f}\")\n",
    "            \n",
    "            # Semantic chunk hits (near misses)\n",
    "            semantic_hits = tier_12[tier_12['Semantic_Chunk_Hit'] == True]\n",
    "            if len(semantic_hits) > 0:\n",
    "                print(f\"  → Semantic near-miss hits: {len(semantic_hits)} (similarity > 0.7)\")\n",
    "            \n",
    "            # Average chunk similarity for misses\n",
    "            misses_with_sim = tier_12[(tier_12['Exact_Chunk_Match'] == False) & (tier_12['Best_Chunk_Similarity'].notna())]\n",
    "            if len(misses_with_sim) > 0:\n",
    "                print(f\"  → Avg similarity for misses: {misses_with_sim['Best_Chunk_Similarity'].mean():.3f}\")\n",
    "\n",
    "        # Tier 3: Multi-paper matching\n",
    "        tier_3 = df_results_enhanced[df_results_enhanced['Tier'] == 3]\n",
    "        if len(tier_3) > 0:\n",
    "            multi_match_rate = tier_3['Multi_Paper_Match'].sum() / len(tier_3)\n",
    "            print(f\"\\nTier 3 (Synthesis) - Multi-Paper Hit Rate: {multi_match_rate:.2%} ({int(tier_3['Multi_Paper_Match'].sum())}/{len(tier_3)})\")\n",
    "            \n",
    "            avg_papers = tier_3['Num_Papers'].mean()\n",
    "            print(f\"  → Avg papers retrieved: {avg_papers:.1f}\")\n",
    "            \n",
    "            # Paper recall and precision\n",
    "            tier_3_with_expected = tier_3[tier_3['Paper_Recall'].notna()]\n",
    "            if len(tier_3_with_expected) > 0:\n",
    "                avg_recall = tier_3_with_expected['Paper_Recall'].mean()\n",
    "                avg_precision = tier_3_with_expected['Paper_Precision'].mean()\n",
    "                print(f\"  → Avg paper recall: {avg_recall:.2%}\")\n",
    "                print(f\"  → Avg paper precision: {avg_precision:.2%}\")\n",
    "\n",
    "        # Answer Quality (all tiers)\n",
    "        with_answer_eval = df_results_enhanced[df_results_enhanced['Answer_Similarity'].notna()]\n",
    "        if len(with_answer_eval) > 0:\n",
    "            avg_answer_sim = with_answer_eval['Answer_Similarity'].mean()\n",
    "            print(f\"\\nAnswer Quality (semantic similarity to expected):\")\n",
    "            print(f\"  → Avg answer similarity: {avg_answer_sim:.3f} ({len(with_answer_eval)} questions with ground truth)\")\n",
    "            print(f\"  → High quality (>0.7): {(with_answer_eval['Answer_Similarity'] > 0.7).sum()}/{len(with_answer_eval)}\")\n",
    "\n",
    "        print(f\"\\nAverage Latency (all): {df_results_enhanced['Latency'].mean():.2f}s\")\n",
    "\n",
    "        # Show detailed results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DETAILED RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 40)\n",
    "\n",
    "        print(df_results_enhanced.to_string(index=False))\n",
    "\n",
    "        # Save results\n",
    "        output_filename = \"rag_evaluation_results_enhanced.csv\"\n",
    "        df_results_enhanced.to_csv(output_filename, index=False)\n",
    "        print(f\"\\n✓ Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4bc44",
   "metadata": {},
   "source": [
    "## Enhanced Evaluation with Ground Truth Chunk IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f95e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging chunk structure from pipeline response...\n",
      "\n",
      "Test question: What physical quantity is the controller changing (the actuator variable) in the liquid-lens autofocus setup?\n",
      "\n",
      "Number of sources: 3\n",
      "\n",
      "Source 1:\n",
      "  Type: <class 'langchain_core.documents.base.Document'>\n",
      "  Available attributes: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'id', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n",
      "  Metadata keys: dict_keys(['authors', 'parent_id', 'filename', 'title', 'section', 'score', 'id'])\n",
      "  Full metadata: {'authors': 'Jing Zhang, Yong-feng Fu, Hao Shen, Quan Liu, Li-ning Sun, Li-guo Chen', 'parent_id': 'Zhang_et_al.___2024___Precision_autofocus_in_optical_microscopy_with_liquid_lenses_controlled_by_deep_reinforcement_learni.pdf', 'filename': 'Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf', 'title': 'Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning', 'section': 'Introduction', 'score': 0.2860294580459595, 'id': 'Zhang_et_al.___2024___Precision_autofocus_in_optical_microscopy_with_liquid_lenses_controlled_by_deep_reinforcement_learni.pdf#Introduction_part5'}\n",
      "\n",
      "Source 2:\n",
      "  Type: <class 'langchain_core.documents.base.Document'>\n",
      "  Available attributes: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'id', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n",
      "  Metadata keys: dict_keys(['parent_id', 'filename', 'section', 'title', 'authors', 'score', 'id'])\n",
      "  Full metadata: {'parent_id': 'Zhang_et_al.___2024___Precision_autofocus_in_optical_microscopy_with_liquid_lenses_controlled_by_deep_reinforcement_learni.pdf', 'filename': 'Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf', 'section': 'Introduction', 'title': 'Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning', 'authors': 'Jing Zhang, Yong-feng Fu, Hao Shen, Quan Liu, Li-ning Sun, Li-guo Chen', 'score': 0.31366121768951416, 'id': 'Zhang_et_al.___2024___Precision_autofocus_in_optical_microscopy_with_liquid_lenses_controlled_by_deep_reinforcement_learni.pdf#Introduction_part1'}\n",
      "\n",
      "Source 3:\n",
      "  Type: <class 'langchain_core.documents.base.Document'>\n",
      "  Available attributes: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'id', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n",
      "  Metadata keys: dict_keys(['section', 'authors', 'title', 'filename', 'parent_id', 'score', 'id'])\n",
      "  Full metadata: {'section': 'Effect of actions on autofocus performance', 'authors': 'Jing Zhang, Yong-feng Fu, Hao Shen, Quan Liu, Li-ning Sun, Li-guo Chen', 'title': 'Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning', 'filename': 'Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf', 'parent_id': 'Zhang_et_al.___2024___Precision_autofocus_in_optical_microscopy_with_liquid_lenses_controlled_by_deep_reinforcement_learni.pdf', 'score': 0.32887399196624756, 'id': 'Zhang_et_al.___2024___Precision_autofocus_in_optical_microscopy_with_liquid_lenses_controlled_by_deep_reinforcement_learni.pdf#Effect_of_actions_on_autofocus_part0'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's debug what fields are actually available in the retrieved chunks\n",
    "print(\"Debugging chunk structure from pipeline response...\\n\")\n",
    "\n",
    "# Test with one question\n",
    "test_question = eval_dataset[0]['question']\n",
    "print(f\"Test question: {test_question}\\n\")\n",
    "\n",
    "response = rag_pipeline.run(test_question, k=3, include_sources=True)\n",
    "\n",
    "print(f\"Number of sources: {len(response.sources)}\\n\")\n",
    "\n",
    "for i, src in enumerate(response.sources):\n",
    "    print(f\"Source {i+1}:\")\n",
    "    print(f\"  Type: {type(src)}\")\n",
    "    print(f\"  Available attributes: {dir(src) if hasattr(src, '__dict__') else 'N/A'}\")\n",
    "    print(f\"  Metadata keys: {src.metadata.keys() if hasattr(src, 'metadata') else 'No metadata'}\")\n",
    "    print(f\"  Full metadata: {src.metadata}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
