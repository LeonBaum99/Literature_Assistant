{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1b9735",
   "metadata": {},
   "source": [
    "# End-to-End RAG Pipeline - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9810b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'genai_env (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n genai_env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Change to parent directory for config.yaml access\n",
    "parent_dir = Path.cwd().parent\n",
    "os.chdir(parent_dir)\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from pdfProcessing.docling_PDF_processor import DoclingPDFProcessor\n",
    "from pdfProcessing.chunking import create_chunks_from_sections\n",
    "from embeddingModels.ModernBertEmbedder import ModernBertEmbedder\n",
    "from embeddingModels.QwenEmbedder import QwenEmbedder\n",
    "from backend.services.embedder import EmbeddingService\n",
    "from backend.services.vector_db import VectorDBService\n",
    "from backend.services.rag_answer_service import ChromaRagRetriever\n",
    "from llmAG.rag.pipeline import RagPipeline\n",
    "from llmAG.llm import build_llm\n",
    "from zotero_integration.metadata_loader import ZoteroMetadataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bb01b",
   "metadata": {},
   "source": [
    "## 1. Initialize Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ffd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDER_TYPE = \"bert\"  # \"bert\" or \"qwen\"\n",
    "CHROMA_PATH = \"./backend/chroma_db\"  # Use same DB as backend\n",
    "MAX_CHUNK_SIZE = 2500\n",
    "OVERLAP_SIZE = 200\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "# Database Management\n",
    "CLEAR_DB_ON_RUN = False  # Set to True to clear DB and re-ingest all PDFs\n",
    "\n",
    "# Set Ollama URL for local execution (not Docker)\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "# Initialize Zotero metadata loader\n",
    "print(\"Initializing Zotero metadata loader...\")\n",
    "try:\n",
    "    zotero_loader = ZoteroMetadataLoader()\n",
    "    print(f\"✓ Zotero metadata loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Zotero metadata not available: {e}\")\n",
    "    print(\"  Will fall back to Docling extraction\")\n",
    "    zotero_loader = None\n",
    "\n",
    "# Initialize PDF processor\n",
    "print(\"Initializing PDF processor...\")\n",
    "processor = DoclingPDFProcessor()\n",
    "\n",
    "# Initialize embedding service\n",
    "print(\"Initializing embedding service...\")\n",
    "embed_service = EmbeddingService()\n",
    "# Load the model to have direct access to embedder for manual operations\n",
    "embedder = embed_service.load_model(EMBEDDER_TYPE)\n",
    "\n",
    "# Initialize ChromaDB service\n",
    "print(\"Initializing ChromaDB...\")\n",
    "db_service = VectorDBService(\n",
    "    db_path=CHROMA_PATH,\n",
    "    collection_names={\n",
    "        \"bert\": \"scientific_papers_bert\",\n",
    "        \"qwen\": \"scientific_papers_qwen\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "print(\"Initializing LLM (Ollama mistral-nemo)...\")\n",
    "try:\n",
    "    llm = build_llm(model=\"mistral-nemo\", temperature=0.1)\n",
    "    print(\"✓ LLM initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ LLM initialization failed: {e}\")\n",
    "    print(\"  Make sure Ollama app is running (check system tray)\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e33fbc",
   "metadata": {},
   "source": [
    "## 2. Database Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c987461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current database state\n",
    "print(f\"{'='*80}\")\n",
    "print(\"DATABASE STATUS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "try:\n",
    "    collection = db_service.get_collection(EMBEDDER_TYPE)\n",
    "    chunk_count = collection.count()\n",
    "    \n",
    "    print(f\"Current database status (model: {EMBEDDER_TYPE})\")\n",
    "    print(f\"  Chunks in database: {chunk_count}\")\n",
    "    \n",
    "    if chunk_count == 0:\n",
    "        print(f\"  ⚠ Database is empty - run ingestion first\")\n",
    "    else:\n",
    "        print(f\"  ✓ Database ready for evaluation\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808221d2",
   "metadata": {},
   "source": [
    "## 3. Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d26c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_dataset(filename=\"eval_dataset.json\"):\n",
    "    # Try multiple paths - more flexible approach\n",
    "    potential_dirs = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path(__file__).parent.parent if '__file__' in dir() else None,\n",
    "        Path(\"C:/Users/kronask/Documents/TU/3. Semester/GenAI/GenAI\"),\n",
    "    ]\n",
    "    \n",
    "    for directory in potential_dirs:\n",
    "        if directory is None:\n",
    "            continue\n",
    "        file_path = directory / filename\n",
    "        if file_path.exists():\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"✓ Loaded {len(data)} questions from {file_path}\")\n",
    "            return data\n",
    "    \n",
    "    print(f\"⚠ Warning: {filename} not found\")\n",
    "    return []\n",
    "\n",
    "# Load the data\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "eval_dataset = load_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c065b",
   "metadata": {},
   "source": [
    "## 4. Initialize RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG components\n",
    "retriever = ChromaRagRetriever(\n",
    "    embed_service=embed_service,\n",
    "    db_service=db_service,\n",
    "    model_name=EMBEDDER_TYPE\n",
    ")\n",
    "\n",
    "# Initialize RAG pipeline (builds LLM internally)\n",
    "rag_pipeline = RagPipeline(\n",
    "    retriever=retriever,\n",
    "    model=\"mistral-nemo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "print(\"✓ RAG pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5ead7",
   "metadata": {},
   "source": [
    "## 5. RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19110433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class RAGEvaluator:\n",
    "#     def __init__(self, pipeline):\n",
    "#         self.pipeline = pipeline\n",
    "#         self.results = []\n",
    "\n",
    "#     def evaluate(self, dataset, top_k=5):\n",
    "#         print(f\"Starting evaluation of {len(dataset)} questions...\")\n",
    "#         self.results = []\n",
    "        \n",
    "#         for item in tqdm(dataset):\n",
    "#             question = item['question']\n",
    "#             target_tag = item.get('target_tag')\n",
    "#             tier = item.get('tier')\n",
    "            \n",
    "#             start_time = time.time()\n",
    "#             try:\n",
    "#                 # Run RAG Pipeline\n",
    "#                 response = self.pipeline.run(question, k=top_k, include_sources=True)\n",
    "#                 elapsed = time.time() - start_time\n",
    "                \n",
    "#                 # 1. Retrieval Evaluation (Source Matching)\n",
    "#                 # Check if ANY of the retrieved docs contain the target tag in their title\n",
    "#                 retrieved_titles = [src.metadata.get('title', '').lower() for src in response.sources]\n",
    "                \n",
    "#                 hit = False\n",
    "#                 if target_tag:\n",
    "#                     tag_map = {\n",
    "#                         \"FAST\": [\"fast\", \"autonomous high-resolution scanning\"],\n",
    "#                         \"liquid lenses\": [\"liquid lenses\", \"zhang\"],\n",
    "#                         \"autofocus\": [\"autofocus\", \"zhang\", \"rebuffi\"],\n",
    "#                         \"ptychography\": [\"ptychography\", \"schloz\"],\n",
    "#                         \"alignment\": [\"alignment\", \"morris\", \"beamlines\"],\n",
    "#                         \"optics\": [\"adaptive optics\", \"nousiainen\", \"mareev\"]\n",
    "#                     }\n",
    "                    \n",
    "#                     search_terms = tag_map.get(target_tag, [target_tag.lower()])\n",
    "                    \n",
    "#                     # Check for hit\n",
    "#                     for title in retrieved_titles:\n",
    "#                         if any(term in title for term in search_terms):\n",
    "#                             hit = True\n",
    "#                             break\n",
    "#                 else:\n",
    "#                     hit = None # No target tag defined (Synthesis questions)\n",
    "\n",
    "#                 # Store Result\n",
    "#                 self.results.append({\n",
    "#                     \"Tier\": tier,\n",
    "#                     \"Question\": question,\n",
    "#                     \"Target_Tag\": target_tag,\n",
    "#                     \"Hit\": hit,\n",
    "#                     \"Answer\": response.answer,\n",
    "#                     \"Sources\": \" | \".join([t[:50] + \"...\" for t in retrieved_titles]),\n",
    "#                     \"Latency\": round(elapsed, 2)\n",
    "#                 })\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error on question: {question[:30]}... {e}\")\n",
    "#                 self.results.append({\n",
    "#                     \"Tier\": tier,\n",
    "#                     \"Question\": question,\n",
    "#                     \"Target_Tag\": target_tag,\n",
    "#                     \"Hit\": False,\n",
    "#                     \"Answer\": f\"ERROR: {str(e)}\",\n",
    "#                     \"Sources\": \"\",\n",
    "#                     \"Latency\": 0\n",
    "#                 })\n",
    "\n",
    "#         return pd.DataFrame(self.results)\n",
    "\n",
    "# # Initialize and Run\n",
    "# evaluator = RAGEvaluator(rag_pipeline)\n",
    "# df_results = evaluator.evaluate(eval_dataset, top_k=5)\n",
    "\n",
    "# # Display Summary\n",
    "# print(\"\\n=== Evaluation Summary ===\")\n",
    "# if 'Hit' in df_results.columns:\n",
    "#     # Filter out synthesis questions (Hit=None) for accuracy calc\n",
    "#     measurable = df_results.dropna(subset=['Hit'])\n",
    "#     print(f\"Retrieval Hit Rate (Targeted Questions): {measurable['Hit'].mean():.2%}\")\n",
    "\n",
    "# print(f\"Average Latency: {df_results['Latency'].mean():.2f}s\")\n",
    "# df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bf0be",
   "metadata": {},
   "source": [
    "## 6. Save and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbca5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_filename = \"rag_evaluation_results.csv\"\n",
    "# df_results.to_csv(output_filename, index=False)\n",
    "# print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "# # Inspect specifically the \"Missed\" items to debug retrieval\n",
    "# print(\"\\n=== Missed Retrieval Questions ===\")\n",
    "# missed = df_results[(df_results['Hit'] == False) & (df_results['Target_Tag'].notna())]\n",
    "# if not missed.empty:\n",
    "#     for _, row in missed.iterrows():\n",
    "#         print(f\"Q: {row['Question']}\")\n",
    "#         print(f\"Target: {row['Target_Tag']}\")\n",
    "#         print(f\"Got Sources: {row['Sources']}\\n\")\n",
    "# else:\n",
    "#     print(\"Great! No retrieval misses on targeted questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa9749",
   "metadata": {},
   "source": [
    "## 7. Detailed Evaluation by Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14184a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Breakdown by tier and target tag\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"EVALUATION BREAKDOWN BY TIER\")\n",
    "# print(f\"{'='*80}\\n\")\n",
    "\n",
    "# for tier in sorted(df_results['Tier'].unique()):\n",
    "#     tier_data = df_results[df_results['Tier'] == tier]\n",
    "#     print(f\"\\nTier {tier}:\")\n",
    "#     print(f\"  Total Questions: {len(tier_data)}\")\n",
    "    \n",
    "#     with_tags = tier_data[tier_data['Target_Tag'].notna()]\n",
    "#     if len(with_tags) > 0:\n",
    "#         hit_rate = with_tags['Hit'].mean()\n",
    "#         print(f\"  Retrieval Hit Rate: {hit_rate:.2%} ({int(with_tags['Hit'].sum())}/{len(with_tags)})\")\n",
    "    \n",
    "#     print(f\"  Avg Latency: {tier_data['Latency'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7a60b",
   "metadata": {},
   "source": [
    "## 8. Question-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show all questions with their results\n",
    "# display_cols = ['Tier', 'Target_Tag', 'Question', 'Hit', 'Latency']\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"ALL EVALUATION RESULTS\")\n",
    "# print(f\"{'='*80}\\n\")\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "# print(df_results[display_cols].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad374f6",
   "metadata": {},
   "source": [
    "##  Enhanced Evaluation (Chunk + Multi-Paper Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a08e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluator: tracks exact chunks (Tier 1-2) AND multi-paper retrieval (Tier 3)\n",
    "# Plus answer quality and semantic similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class EnhancedRAGEvaluator:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.results = []\n",
    "        # Load semantic similarity model for answer quality evaluation\n",
    "        print(\"Loading semantic similarity model...\")\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ Model loaded\")\n",
    "\n",
    "    def evaluate(self, dataset, top_k=5):\n",
    "        print(f\"Starting enhanced evaluation of {len(dataset)} questions...\")\n",
    "        self.results = []\n",
    "        \n",
    "        for item in tqdm(dataset):\n",
    "            question = item['question']\n",
    "            target_tag = item.get('target_tag')\n",
    "            tier = item.get('tier')\n",
    "            expected_chunk_id = item.get('expected_chunk_id')  # Ground truth\n",
    "            expected_answer = item.get('expected_answer')  # Ground truth answer\n",
    "            expected_papers = item.get('expected_papers', [])  # Ground truth papers for Tier 3\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Run RAG Pipeline\n",
    "                response = self.pipeline.run(question, k=top_k, include_sources=True)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Extract metadata from retrieved chunks\n",
    "                retrieved_titles = [src.metadata.get('title', '').lower() for src in response.sources]\n",
    "                retrieved_filenames = [src.metadata.get('filename', '') for src in response.sources]\n",
    "                retrieved_parents = [src.metadata.get('parent_id', '') for src in response.sources]\n",
    "                \n",
    "                # Get unique papers from retrieved chunks\n",
    "                unique_papers = list(set(retrieved_filenames))\n",
    "                num_unique_papers = len(unique_papers)\n",
    "                \n",
    "                # 1. Check for EXACT chunk match (for Tier 1-2)\n",
    "                exact_chunk_match = False\n",
    "                chunk_found_at_rank = None\n",
    "                if expected_chunk_id:\n",
    "                    # Try multiple ways to match the expected chunk ID\n",
    "                    for rank, src in enumerate(response.sources, 1):\n",
    "                        parent_id = src.metadata.get('parent_id', '')\n",
    "                        # Match by parent_id or if expected_chunk_id appears in the parent_id\n",
    "                        if parent_id == expected_chunk_id or expected_chunk_id.split('#')[0] in parent_id:\n",
    "                            exact_chunk_match = True\n",
    "                            chunk_found_at_rank = rank\n",
    "                            break\n",
    "                \n",
    "                # 2. Semantic similarity to expected chunk (near-miss detection)\n",
    "                semantic_chunk_hit = None\n",
    "                best_chunk_similarity = None\n",
    "                if expected_chunk_id and not exact_chunk_match:\n",
    "                    # Get the expected chunk from database for comparison\n",
    "                    try:\n",
    "                        collection = self.pipeline.retriever.db_service.get_collection(\n",
    "                            self.pipeline.retriever.model_name\n",
    "                        )\n",
    "                        expected_docs = collection.get(ids=[expected_chunk_id])\n",
    "                        if expected_docs and expected_docs['documents']:\n",
    "                            expected_text = expected_docs['documents'][0]\n",
    "                            expected_embedding = self.semantic_model.encode([expected_text])\n",
    "                            \n",
    "                            # Compare with retrieved chunks\n",
    "                            retrieved_texts = [src.page_content for src in response.sources]\n",
    "                            retrieved_embeddings = self.semantic_model.encode(retrieved_texts)\n",
    "                            \n",
    "                            similarities = cosine_similarity(expected_embedding, retrieved_embeddings)[0]\n",
    "                            best_chunk_similarity = float(similarities.max())\n",
    "                            \n",
    "                            # Consider it a semantic hit if similarity > 0.7\n",
    "                            if best_chunk_similarity > 0.7:\n",
    "                                semantic_chunk_hit = True\n",
    "                            else:\n",
    "                                semantic_chunk_hit = False\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not compute semantic chunk similarity: {e}\")\n",
    "                \n",
    "                # 3. Multi-paper recall and precision for Tier 3 (synthesis questions)\n",
    "                multi_paper_match = num_unique_papers >= 2\n",
    "                paper_recall = None\n",
    "                paper_precision = None\n",
    "                \n",
    "                if expected_papers and len(expected_papers) > 0:\n",
    "                    # Normalize filenames for comparison\n",
    "                    retrieved_normalized = {f.lower() for f in retrieved_filenames if f}\n",
    "                    expected_normalized = {p.lower() for p in expected_papers}\n",
    "                    \n",
    "                    # Find intersection\n",
    "                    correct_papers = retrieved_normalized & expected_normalized\n",
    "                    \n",
    "                    if len(expected_normalized) > 0:\n",
    "                        paper_recall = len(correct_papers) / len(expected_normalized)\n",
    "                    \n",
    "                    if len(retrieved_normalized) > 0:\n",
    "                        paper_precision = len(correct_papers) / len(retrieved_normalized)\n",
    "                \n",
    "                # 4. Answer quality evaluation using semantic similarity\n",
    "                answer_similarity = None\n",
    "                if expected_answer:\n",
    "                    answer_embedding = self.semantic_model.encode([response.answer])\n",
    "                    expected_embedding = self.semantic_model.encode([expected_answer])\n",
    "                    answer_similarity = float(cosine_similarity(answer_embedding, expected_embedding)[0][0])\n",
    "                \n",
    "                # Store Result\n",
    "                self.results.append({\n",
    "                    \"Tier\": tier,\n",
    "                    \"Question\": question[:60] + \"...\" if len(question) > 60 else question,\n",
    "                    \"Target_Tag\": target_tag,\n",
    "                    \"Exact_Chunk_Match\": exact_chunk_match if expected_chunk_id else None,\n",
    "                    \"Chunk_Rank\": chunk_found_at_rank if exact_chunk_match else None,\n",
    "                    \"Semantic_Chunk_Hit\": semantic_chunk_hit,\n",
    "                    \"Best_Chunk_Similarity\": round(best_chunk_similarity, 3) if best_chunk_similarity else None,\n",
    "                    \"Num_Papers\": num_unique_papers,\n",
    "                    \"Multi_Paper_Match\": multi_paper_match if tier == 3 else None,\n",
    "                    \"Paper_Recall\": round(paper_recall, 3) if paper_recall is not None else None,\n",
    "                    \"Paper_Precision\": round(paper_precision, 3) if paper_precision is not None else None,\n",
    "                    \"Answer_Similarity\": round(answer_similarity, 3) if answer_similarity else None,\n",
    "                    \"Papers\": \" | \".join([p.split(' - ')[0][:30] for p in unique_papers[:2]]),\n",
    "                    \"Latency\": round(elapsed, 2)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on question: {question[:30]}... {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.results.append({\n",
    "                    \"Tier\": tier,\n",
    "                    \"Question\": question[:60] + \"...\" if len(question) > 60 else question,\n",
    "                    \"Target_Tag\": target_tag,\n",
    "                    \"Exact_Chunk_Match\": False,\n",
    "                    \"Chunk_Rank\": None,\n",
    "                    \"Semantic_Chunk_Hit\": None,\n",
    "                    \"Best_Chunk_Similarity\": None,\n",
    "                    \"Num_Papers\": 0,\n",
    "                    \"Multi_Paper_Match\": False,\n",
    "                    \"Paper_Recall\": None,\n",
    "                    \"Paper_Precision\": None,\n",
    "                    \"Answer_Similarity\": None,\n",
    "                    \"Papers\": f\"ERROR: {str(e)[:20]}\",\n",
    "                    \"Latency\": 0\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Debug: Check where eval_dataset is\n",
    "print(f\"eval_dataset loaded: {len(eval_dataset)} items\")\n",
    "if len(eval_dataset) == 0:\n",
    "    print(\"⚠ Evaluation dataset is empty. Make sure eval_dataset.json exists and is in the current working directory.\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Looking for: {Path.cwd() / 'eval_dataset.json'}\")\n",
    "else:\n",
    "    # Run enhanced evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENHANCED EVALUATION (Chunk-level + Multi-paper + Answer Quality)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    evaluator_enhanced = EnhancedRAGEvaluator(rag_pipeline)\n",
    "    df_results_enhanced = evaluator_enhanced.evaluate(eval_dataset, top_k=5)\n",
    "\n",
    "    # Display Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    if len(df_results_enhanced) == 0:\n",
    "        print(\"No evaluation results generated.\")\n",
    "    else:\n",
    "        # Tier 1-2: Exact chunk matching\n",
    "        tier_12 = df_results_enhanced[df_results_enhanced['Tier'].isin([1, 2])]\n",
    "        if len(tier_12) > 0:\n",
    "            chunk_match_rate = tier_12['Exact_Chunk_Match'].sum() / tier_12['Exact_Chunk_Match'].notna().sum()\n",
    "            print(f\"Tier 1-2 (Single Paper) - Exact Chunk Hit Rate: {chunk_match_rate:.2%} ({int(tier_12['Exact_Chunk_Match'].sum())}/{int(tier_12['Exact_Chunk_Match'].notna().sum())})\")\n",
    "            \n",
    "            # Show average rank when chunk is found\n",
    "            found_ranks = tier_12[tier_12['Exact_Chunk_Match'] == True]['Chunk_Rank']\n",
    "            if len(found_ranks) > 0:\n",
    "                print(f\"  → Avg rank of correct chunk: {found_ranks.mean():.1f}\")\n",
    "            \n",
    "            # Semantic chunk hits (near misses)\n",
    "            semantic_hits = tier_12[tier_12['Semantic_Chunk_Hit'] == True]\n",
    "            if len(semantic_hits) > 0:\n",
    "                print(f\"  → Semantic near-miss hits: {len(semantic_hits)} (similarity > 0.7)\")\n",
    "            \n",
    "            # Average chunk similarity for misses\n",
    "            misses_with_sim = tier_12[(tier_12['Exact_Chunk_Match'] == False) & (tier_12['Best_Chunk_Similarity'].notna())]\n",
    "            if len(misses_with_sim) > 0:\n",
    "                print(f\"  → Avg similarity for misses: {misses_with_sim['Best_Chunk_Similarity'].mean():.3f}\")\n",
    "\n",
    "        # Tier 3: Multi-paper matching\n",
    "        tier_3 = df_results_enhanced[df_results_enhanced['Tier'] == 3]\n",
    "        if len(tier_3) > 0:\n",
    "            multi_match_rate = tier_3['Multi_Paper_Match'].sum() / len(tier_3)\n",
    "            print(f\"\\nTier 3 (Synthesis) - Multi-Paper Hit Rate: {multi_match_rate:.2%} ({int(tier_3['Multi_Paper_Match'].sum())}/{len(tier_3)})\")\n",
    "            \n",
    "            avg_papers = tier_3['Num_Papers'].mean()\n",
    "            print(f\"  → Avg papers retrieved: {avg_papers:.1f}\")\n",
    "            \n",
    "            # Paper recall and precision\n",
    "            tier_3_with_expected = tier_3[tier_3['Paper_Recall'].notna()]\n",
    "            if len(tier_3_with_expected) > 0:\n",
    "                avg_recall = tier_3_with_expected['Paper_Recall'].mean()\n",
    "                avg_precision = tier_3_with_expected['Paper_Precision'].mean()\n",
    "                print(f\"  → Avg paper recall: {avg_recall:.2%}\")\n",
    "                print(f\"  → Avg paper precision: {avg_precision:.2%}\")\n",
    "\n",
    "        # Answer Quality (all tiers)\n",
    "        with_answer_eval = df_results_enhanced[df_results_enhanced['Answer_Similarity'].notna()]\n",
    "        if len(with_answer_eval) > 0:\n",
    "            avg_answer_sim = with_answer_eval['Answer_Similarity'].mean()\n",
    "            print(f\"\\nAnswer Quality (semantic similarity to expected):\")\n",
    "            print(f\"  → Avg answer similarity: {avg_answer_sim:.3f} ({len(with_answer_eval)} questions with ground truth)\")\n",
    "            print(f\"  → High quality (>0.7): {(with_answer_eval['Answer_Similarity'] > 0.7).sum()}/{len(with_answer_eval)}\")\n",
    "\n",
    "        print(f\"\\nAverage Latency (all): {df_results_enhanced['Latency'].mean():.2f}s\")\n",
    "\n",
    "        # Show detailed results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DETAILED RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 40)\n",
    "\n",
    "        print(df_results_enhanced.to_string(index=False))\n",
    "\n",
    "        # Save results\n",
    "        output_filename = \"rag_evaluation_results_enhanced.csv\"\n",
    "        df_results_enhanced.to_csv(output_filename, index=False)\n",
    "        print(f\"\\n✓ Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4bc44",
   "metadata": {},
   "source": [
    "## Enhanced Evaluation with Ground Truth Chunk IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's debug what fields are actually available in the retrieved chunks\n",
    "print(\"Debugging chunk structure from pipeline response...\\n\")\n",
    "\n",
    "# Test with one question\n",
    "test_question = eval_dataset[0]['question']\n",
    "print(f\"Test question: {test_question}\\n\")\n",
    "\n",
    "response = rag_pipeline.run(test_question, k=3, include_sources=True)\n",
    "\n",
    "print(f\"Number of sources: {len(response.sources)}\\n\")\n",
    "\n",
    "for i, src in enumerate(response.sources):\n",
    "    print(f\"Source {i+1}:\")\n",
    "    print(f\"  Type: {type(src)}\")\n",
    "    print(f\"  Available attributes: {dir(src) if hasattr(src, '__dict__') else 'N/A'}\")\n",
    "    print(f\"  Metadata keys: {src.metadata.keys() if hasattr(src, 'metadata') else 'No metadata'}\")\n",
    "    print(f\"  Full metadata: {src.metadata}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
