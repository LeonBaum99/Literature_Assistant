{
    "Abstract": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Modern scanning microscopes can image materials with up to sub-atomic spatial and sub-picosecond time resolutions, but these capabilities come with large volumes of data, which can be difficult to store and analyze[cite: 3185]. We report the Fast Autonomous Scanning Toolkit (FAST) that addresses this challenge by combining a neural network, route optimization, and efficient hardware controls to enable a self-driving experiment that actively identifies and measures a sparse but representative data subset in lieu of the full dataset[cite: 3186]. FAST requires no prior information about the sample, is computationally efficient, and uses generic hardware controls with minimal experiment-specific wrapping[cite: 3187]. We test FAST in simulations and a dark-field X-ray microscopy experiment of a WSe2 film[cite: 3188]. Our studies show that a FAST scan of <25% is sufficient to accurately image and analyze the sample[cite: 3189]. FAST is easy to adapt for any scanning microscope; its broad adoption will empower general multi-level studies of materials evolution with respect to time, temperature, or other parameters[cite: 3190, 3191]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Increasing complexity of modern laser systems, mostly originated from the nonlinear dynamics of radiation, makes control of their operation more and more challenging, calling for development of new approaches in laser engineering. Machine learning methods, providing proven tools for identification, control, and data analytics of various complex systems, have been recently applied to mode-locked fiber lasers with the special focus on three key areas: self-starting, system optimization and characterization. However, the development of the machine learning algorithms for a particular laser system, while being an interesting research problem, is a demanding task requiring arduous efforts and tuning a large number of hyper-parameters in the laboratory arrangements. It is not obvious that this learning can be smoothly transferred to systems that differ from the specific laser used for the algorithm development by design or by varying environmental parameters. Here we demonstrate that a deep reinforcement learning (DRL) approach, based on trials and errors and sequential decisions, can be successfully used for control of the generation of dissipative solitons in mode-locked fiber laser system. We have shown the capability of deep Q-learning algorithm to generalize knowledge about the laser system in order to find conditions for stable pulse generation. Region of stable generation was transformed by changing the pumping power of the laser cavity, while tunable spectral filter was used as a control tool. Deep Q-learning algorithm is suited to learn the trajectory of adjusting spectral filter parameters to stable pulsed regime relying on the state of output radiation. Our results confirm the potential of deep reinforcement learning algorithm to control a nonlinear laser system with a feed-back. We also demonstrate that fiber mode-locked laser systems generating data at high speed present a fruitful photonic test-beds for various machine learning concepts based on large datasets."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Useful materials must satisfy multiple objectives, where the optimization of one objective is often at the expense of another. The Pareto front reports the optimal trade-offs between these conflicting objectives. Here we use a self-driving laboratory, Ada, to define the Pareto front of conductivities and processing temperatures for palladium films formed by combustion synthesis. Ada discovers new synthesis conditions that yield metallic films at lower processing temperatures (below 200 °C) relative to the prior art for this technique (250 °C). This temperature difference makes possible the coating of different commodity plastic materials (e.g., Nafion, polyethersulfone). These combustion synthesis conditions enable us to to spray coat uniform palladium films with moderate conductivity (1.1 × 105 S m−1 ) at 191 °C. Spray coating at 226 °C yields films with conductivities (2.0 × 106 S m−1 ) comparable to those of sputtered films (2.0 to 5.8 × 106 S m−1 ). This work shows how a self-driving laboratoy can discover materials that provide optimal trade-offs between conflicting objectives."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Progress in the field of machine learning has enhanced the development of self-adjusting optical systems capable of autonomously adapting to changing environmental conditions. This study demonstrates the concept of self-adjusting optical systems and presents a new approach based on reinforcement learning methods. We integrated reinforcement learning algorithms into the setup for tuning the laser radiation into the fiber, as well as into the complex for controlling the laser-plasma source. That reduced the dispersion of the generated X-ray signal by 2-3 times through automatic adjustment of the position of the rotating copper target and completely eliminated the linear trend arising from the ablation of the target surface. The adjustment of the system was performed based on feedback signals obtained from the spectrometer, and the movement of the target was achieved using a neural network-controlled stepper motor. As feedback, the second harmonic of femtosecond laser radiation was used, the intensity of which has a square root dependence on the X-ray yield. The developed machine learning methodology allows the considered systems to optimize their performance and adapt in real time, leading to increased efficiency, accuracy, and reliability."
        },
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Optical microscopy is an indispensable tool in life sciences research, but conventional techniques require compromises between imaging parameters like speed, resolution, field-of-view, and phototoxicity. To overcome these limitations, data-driven microscopes incorporate feedback loops between data acquisition and analysis. This review overviews how machine learning enables automated image analysis to optimise microscopy in real-time. We first introduce key data-driven microscopy concepts and machine learning methods relevant to microscopy image analysis. Subsequently, we highlight pioneering works and recent advances in integrating machine learning into microscopy acquisition workflows, including optimising illumination, switching modalities and acquisition rates, and triggering targeted experiments. We then discuss the remaining challenges and future outlook. Overall, intelligent microscopes that can sense, analyse, and adapt promise to transform optical imaging by opening new experimental possibilities."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Autonomous methods to align beamlines can decrease the amount of time spent on diagnostics, and also uncover better global optima leading to better beam quality. The alignment of these beamlines is a high-dimensional expensive-to-sample optimization problem involving the simultaneous treatment of many optical elements with correlated and nonlinear dynamics. Bayesian optimization is a strategy of efficient global optimization that has proved successful in similar regimes in a wide variety of beamline alignment applications, though it has typically been implemented for particular beamlines and optimization tasks. In this paper, we present a basic formulation of Bayesian inference and Gaussian process models as they relate to multi-objective Bayesian optimization, as well as the practical challenges presented by beamline alignment. We show that the same general implementation of Bayesian optimization with special consideration for beamline alignment can quickly learn the dynamics of particular beamlines in an online fashion through hyperparameter fitting with no prior information. We present the implementation of a concise software framework for beamline alignment and test it on four different optimization problems for experiments on X-ray beamlines at the National Synchrotron Light Source II and the Advanced Light Source, and an electron beam at the Accelerator Test Facility, along with benchmarking on a simulated digital twin. We discuss new applications of the framework, and the potential for a unified approach to beamline alignment at synchrotron facilities."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Direct imaging of Earth-like exoplanets is one of the most prominent scientific drivers of the next generation of ground-based telescopes. Typically, Earth-like exoplanets are located at small angular separations from their host stars, making their detection difficult. Consequently, the adaptive optics (AO) system's control algorithm must be carefully designed to distinguish the exoplanet from the residual light produced by the host star. A promising avenue of research to improve AO control builds on data-driven control methods, such as reinforcement learning (RL). RL is an active branch of the machine learning research field, where control of a system is learned through interaction with the environment. Thus, RL can be seen as an automated approach to AO control, where its usage is entirely a turnkey operation. In particular, model-based RL has been shown to cope with temporal and misregistration errors. Similarly, it has been demonstrated to adapt to nonlinear wavefront sensing while being efficient in training and execution. In this work, we implement and adapt an RL method called policy optimization for AO (PO4AO) to the GPU-based high-order adaptive optics testbench (GHOST) test bench at ESO headquarters, where we demonstrate a strong performance of the method in a laboratory environment. Our implementation allows the training to be performed parallel to inference, which is crucial for on-sky operation. In particular, we study the predictive and self-calibrating aspects of the method. The new implementation on GHOST running PyTorch introduces only around 700 µs of in addition to hardware, pipeline, and Python interface latency. We open-source well-documented code for the implementation and specify the requirements for the RTC pipeline. We also discuss the important hyperparameters of the method and how they affect the method. Further, the paper discusses the source of the latency and the possible paths for a lower latency implementation."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "We describe the application of an AI-driven system to autonomously align and focus complex x-ray mirror systems. The system has been developed and studied on a digital twin of nanofocusing X-ray beamlines, built using advanced optical simulation tools calibrated with wavefront sensing data collected at the beamline. We experimentally demonstrated that the system is systematically capable of positioning a focused beam on the sample, both by simulating the life cycle of the beamline with random perturbations due to typical variations in the light source and optical elements over time, and by conducting similar tests on an actual focusing system."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "We present a method that lowers the dose required for an electron ptychographic reconstruction by adaptively scanning the specimen, thereby providing the required spatial information redundancy in the regions of highest importance. The proposed method is built upon a deep learning model that is trained by reinforcement learning, using prior knowledge of the specimen structure from training data sets. We show that using adaptive scanning for electron ptychography outperforms alternative low-dose ptychography experiments in terms of reconstruction resolution and quality."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "To close the gap between the rates of computational screening and experimental\nrealization of novel materials1,2\n, we introduce the A-Lab, an autonomous laboratory\nfor the solid-state synthesis of inorganic powders. This platform uses computations,\nhistorical data from the literature, machine learning (ML) and active learning to plan\nand interpret the outcomes of experiments performed using robotics. Over 17 days of\ncontinuous operation, the A-Lab realized 41 novel compounds from a set of 58 targets\nincluding a variety of oxides and phosphates that were identified using large-scale ab\ninitio phase-stability data from the Materials Project and Google DeepMind. Synthesis\nrecipes were proposed by natural-language models trained on the literature and\noptimized using an active-learning approach grounded in thermodynamics. Analysis\nof the failed syntheses provides direct and actionable suggestions to improve current\ntechniques for materials screening and synthesis design. The high success rate\ndemonstrates the effectiveness of artificial-intelligence-driven platforms for\nautonomous materials discovery and motivates further integration of computations,\nhistorical knowledge and robotics."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Self-driving laboratories (SDLs) combine artificial intelligence (AI), robotics and high-throughput experimentation to accelerate scientific discovery. By closing the loop between experimental design, execution and analysis, SDLs can navigate vast chemical and materials spaces more efficiently than traditional methods. In this Review, we present a comprehensive overview of the field, starting with a historical perspective on the evolution of automation in chemistry. We propose a definition of SDLs and a classification system for their levels of autonomy. We then dissect the anatomy of an SDL, discussing the interplay between the 'brain' (software and AI agents) and the 'body' (hardware and robotics). We survey the current state of SDLs, highlighting successful implementations in organic synthesis, materials science and other domains. Finally, we address the technical and non-technical challenges facing the field, such as data standardization, interoperability and the need for educational reform, and offer a perspective on the future of autonomous discovery."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "With the rise of self-driving labs (SDLs) and automated experimentation across chemical and materials sciences, there is a considerable challenge in designing the best autonomous lab for a given problem based on published studies alone. Determining what digital and physical features are germane to a specific study is a critical aspect of SDL design that needs to be approached quantitatively. Even when controlling for features such as dimensionality, every experimental space has unique requirements and challenges that influence the design of the optimal physical platform and algorithm. Metrics such as optimization rate are therefore not necessarily indicative of the capabilities of an SDL across different studies. In this perspective, we highlight some of the critical metrics for quantifying performance in SDLs to better guide researchers in implementing the most suitable strategies. We then provide a brief review of the existing literature under the lens of quantified performance as well as heuristic recommendations for platform and experimental space pairings."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Chiral functional films with large dissymmetry factors (g-factors) are highly desired for advanced optical and optoelectronic applications. However, the development of such materials is challenged by the immense design space and complex structure–property relationships. Here, we report an artificial intelligence (AI)-guided robotic system for the inverse design of chiral functional films. By integrating a mobile robot, an automated synthesis and characterization platform, and a Bayesian optimization (BO) algorithm, the system autonomously explores the chemical space of chiral films. We demonstrate the capability of this system by optimizing the g-factor of chiral films composed of perylene diimide (PDI) derivatives and binphthyl enantiomers. The AI-guided robot successfully identified a chiral film with a record-high g-factor of 0.23 within 6 weeks, which is significantly faster than human researchers. Furthermore, the system revealed the key factors governing the chiral amplification in the film formation process. This work not only provides a powerful tool for the accelerated discovery of chiral materials but also demonstrates the potential of AI-guided robotics in materials science."
        },
        {
            "paper_title": "Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning",
            "source_filename": "Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf",
            "content": "Microscopic imaging is a critical tool in scientific research, biomedical studies, and engineering applications, with an urgent need for system miniaturization and rapid, precision autofocus techniques[cite: 1306]. However, traditional microscopes and autofocus methods face hardware limitations and slow software speeds in achieving this goal[cite: 1307]. In response, this paper proposes the implementation of an adaptive Liquid Lens Microscope System utilizing Deep Reinforcement Learning-based Autofocus (DRLAF)[cite: 1308]. The proposed study employs a custom-made liquid lens with a rapid zoom response, which is treated as an \"agent\"[cite: 1309]. Raw images are utilized as the \"state\", with voltage adjustments representing the \"actions\"[cite: 1310]. Deep reinforcement learning is employed to learn the focusing strategy directly from captured images, achieving end-to-end autofocus[cite: 1311]. In contrast to methodologies that rely exclusively on sharpness assessment as a model's labels or inputs, our approach involved the development of a targeted reward function, which has proven to markedly enhance the performance in microscope autofocus tasks[cite: 1312]. We explored various action group design methods and improved the microscope autofocus speed to an average of 3.15 time steps[cite: 1313]. Additionally, parallel \"state\" dataset lists with random sampling training are proposed which enhances the model's adaptability to unknown samples, thereby improving its generalization capability[cite: 1314]. The experimental results demonstrate that the proposed liquid lens microscope with DRLAF exhibits high robustness, achieving a 79% increase in speed compared to traditional search algorithms, a 97.2% success rate, and enhanced generalization compared to other deep learning methods[cite: 1315]."
        }
    ],
    "Introduction": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Scanning microscopes are versatile instruments that use photons, electrons, ions, neutrons, or mechanical probes to interrogate atomic-scale composition, topography, and functionality of materials, with up to sub-atomic spatial resolution and sub-picosecond time resolution[cite: 3192]. Notwithstanding the variation in the probe modalities, these instruments all rely on a scan of the sample to generate spatially resolved signals that are then collected to form an image of the sample[cite: 3193]. Fine-resolution large-field-of-view scanning experiments, however, come with some significant drawbacks: the volume of data generated and the probe-induced damage to the sample can be prohibitively large[cite: 3196]. Meanwhile, the information of interest in these experiments is often concentrated in sparse regions that contain interfaces, defects, or other specific structural elements[cite: 3198]. Directing the scan to only these locations could greatly reduce the scan time and data volume, but it is difficult to obtain this information a priori[cite: 3199]. Addressing this challenge with a human-in-the-loop protocol can be tedious and prohibitively time-consuming[cite: 3200]. Given these factors, the development of autonomous acquisition techniques that can continuously analyze acquired data and drive the sampling specifically toward regions of interest is imperative [cite: 3201].\n\nDeep learning (DL) based inversion methods are enabling real-time data analysis, which is, in turn, opening the door to self-driving techniques that make real-time acquisition decisions based on real-time data streams[cite: 3209]. Specific to microscopy, a popular Bayesian search approach is to use unsupervised Gaussian Processes (GPs), but their computational cost tends to scale cubically with the number of points acquired[cite: 3214, 3215]. Specifically for scanning microscopy applications, Godaliyadda et al. have proposed to achieve computationally efficient autonomous sampling with the Supervised Learning Approach for Dynamic Sampling (SLADS) technique[cite: 3228]. Zhang et al. have incorporated a neural network (NN) within the SLADS method (for the SLADS-Net method) and shown in numerical experiments that it is sufficient to train the method on only a generic image, eschewing any prior knowledge about the sample[cite: 3233]. In this work, we report the Fast Autonomous Scanning Toolkit (FAST) that combines the SLADS-Net method, a route optimization technique, and efficient and modular hardware controls to make on-the-fly sampling and scan path choices for synchrotron-based scanning microscopy[cite: 3235]. We validate the FAST scheme through real-time demonstration at the hard X-ray nanoprobe beamline at the APS using a few-layer exfoliated two-dimensional WSe2 thin film[cite: 3239, 3240]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Laser systems are both important practical devices and complex physical systems where ML techniques can improve performance and offer control of the nonlinear dynamics of radiation. Designing ML algorithms for specific laser system requires rather elaborate efforts that includes data collection, signal processing, feature designing, tuning hyperparameters and so on. Most of the conventional ML approaches, both supervised and unsupervised learning, face various challenges when they are applied to building universal algorithms to control laser sources. The reason is that the process of improving laser performance is not straightforward and it requires to address sequential decision-making tasks involving set of trials. Thus, this technical and physical laser problem is perfectly suited for the application of the reinforcement learning technique, that has a potential to build the systems with the elements of the artificial general intelligence. The fusion of the reinforcement learning and deep neural networks, called deep reinforcement learning (DRL) is a powerful alternative to the supervised learning, replacing learning from the labelled examples by the trial and error approaches. Reinforcement learning has recently been demonstrated to have a wide application in optics. RL algorithms was applied in laser beam welding processes, for control optical tweezers, for reconstructing an unknown quantum state, and alignment of a seed laser for free-electron laser optimization. In the scope of mode-locked lasers there are already promising application of DRL to control the output radiation. Most of them are related to lasers based on nonlinear polarization effect (NPE). Kutz et al demonstrated that deep Q-learning algorithm is capable to learn how to operate with bi-stability in fiber cavity in order to achieve stable mode-locking. The work demonstrates a possibility to stabilize mode-locked regime of NPE laser under temperature and vibratory disturbance by actor-critic DRL algorithm."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Self-driving laboratories combine automation and artificial intelligence to accelerate the discovery and optimization of materials1,2,3. The increasing flexibility of laboratory automation is enabling self-driving laboratories to manipulate and measure a broader set of experimental variables4. Consequently, a growing number of self-driving laboratories are being developed across a range of fields.While many self-driving laboratories are able to test multiple experimental variables, most optimize for only a single objective (e.g., process parameter, material property). This situation is not consistent with most practical applications, where multiple objectives need to be simultaneously optimized. Consider, for example, how a solar cell must be optimized for voltage, current, and fill factor to yield a high power conversion efficiency; how an electrolyzer must form products at low voltages and high reaction rates and selectivities38; and, how structural alloys are optimized for both strength and toughness31,35. These and other applications motivate the emerging use of self-driving laboratories for multiobjective optimizationThe optimization of materials for multiple objectives can be challenging because improving one objective often compromises another (e.g., decreasing the bandgap of the light-absorbing material in a photovoltaic cell increases the photocurrent but decreases the voltage). As a result, there is often no single champion material, but rather a set of materials exhibiting trade-offs between objectives (Fig. 1). The set of materials with the best possible trade-offs lie at the Pareto front. Materials on the Pareto front cannot be improved for one objective without compromising one or more other objectives. Most self-driving laboratories used for multiobjective optimization, however, identify only a single optimal material based on preferences specified in advance of the experiment.Here, we use a self-driving laboratory to map out an entire Pareto front. We apply this approach to thin film materials for the first time by mapping out a trade-off between film conductivity and processing temperature. In doing so, our self-driving laboratory identifies previously untested conditions that decrease the temperature required for the combustion synthesis of palladium films from 250 to 190°C. This finding increases the scope of polymeric substrates that palladium can be deposited on by combustion synthesis to include Nafion, polyethersulfone, and heat-stabilized polyethylene napththalate. Our self-driving laboratory also identifies conditions suitable for spray coating homogeneous films on larger substrates with conductivities approaching those of films made by vacuum deposition methods. The approach presented here is highly relevant to the materials sciences because it identifies optimal materials for every preferred tradeoff between objectives."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Nowadays, neural networks are finding increasing applications in optics and photonics. In the earlier works of machine learning, genetic algorithms were primarily used for pattern recognition, image reconstruction, aberration correction, or optical component design. Subsequent studies focused on the analysis of large datasets and inverse problems, where the superior ability of machine learning to classify data, uncover hidden structures, and handle a large number of degrees of freedom led to significant breakthroughs. Neural networks have achieved great success in the design of nanomaterials, cell classification, super-resolution microscopy, quantum optics, and optical communications. In addition to their application in general data processing, machine learning methods have the potential to control ultrafast photonics technologies of the next generation. This is due to the growing demand for adaptive control and self-adjustment of laser systems, as well as the fact that many ultrafast phenomena in photonics are nonlinear and multidimensional, with dynamics highly sensitive to noise. Although advances in measurement techniques have led to significant developments in experimental methods, recent research has shown that machine learning algorithms provide new ways to identify coherent structures in large sets of noisy data and potentially determine fundamental physical models and equations based on the analysis of complex time series. Neural networks can also greatly facilitate and automate the alignment process of complex optical systems such as femtosecond oscillators. Machine learning with reinforcement has recently been used for this purpose. For example, it has been used for mode synchronization in femtosecond and picosecond oscillators, seed generation for free-electron lasers, soliton and chaos control in laser systems, and laser processing of silicon. In any case, machine learning provides the opportunity to automate the adjustment of complex systems under changing external conditions. This has made this approach attractive for optimizing (increasing stability and signal level) the created laser-plasma source. The physics of X-ray generation in a laser-plasma source can be described as follows. When a laser pulse is focused on the surface of a solid target, the laser pulse energy is absorbed in the skin layer, generating laser-induced plasma with a temperature of several hundred electron volts and a solid density. During this short period, X-ray radiation is generated, as electrons cannot transfer a significant portion of their energy back to the lattice. These fast electrons, arising from the interaction of the incident laser radiation with the plasma, lead to the generation of characteristic lines as the electron vacancy in the inner shell is filled from the outer shell. To achieve a high X-ray flux, it is necessary to tightly focus the laser pulse, resulting in a focal spot size of a few micrometers, making the setup extremely sensitive to target position. Therefore, target vibrations modulate the X-ray pulse. Additionally, due to the high intensity, surface ablation (removal of surface material) occurs, resulting in the reduction of the diameter of the rotating target over time, causing the focus to drift away from the target surface. As a result, the X-ray signal becomes unstable over time, and constant target-position adjustment is required due to target ablation. Therefore, the main aim of this study was to develop an experimental setup based on reinforcement learning that would stabilize and maximize the amplitude of the X-ray signal generated in a laser-plasma source when ultrashort laser pulses are focused onto the surface of a rotating copper target. Additionally, the neural network created can be used for automatic control of the laser pulse propagation direction."
        },
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Optical microscopy techniques, such as brightfield, phase contrast, fluorescence, and super-resolution imaging, are widely used in life sciences to obtain valuable spatiotemporal information for studying cells and model organisms. However, these techniques have certain limitations with respect to critical parameters such as resolution, acquisition speed, signal to noise ratio, field of view, extent of multiplexing, z-depth dimensions and phototoxicity. The trade-offs between these critical imaging parameters are often represented within a \"pyramid of frustration\". Although improving hardware can extend capabilities, optimal balancing depends on the imaging context. Especially, as scientific research delves into more complex questions, trying to understand the mechanisms of cell and infection biology at a molecular level in physiological context, traditional static microscopes may not be sufficient to capture relevant dynamics or rare events. Innovative efforts focus on overcoming these restrictions through integrated automation. Data-driven microscopes employ real-time data analysis to dynamically control and adapt acquisition. The core concept involves introducing automated feedback loops between image-data interpretation and microscope parameters tuning. Quantitative metrics extracted via computational analysis then dictate adaptive protocols tailored to phenomena of interest. The system reacts to predefined observational triggers by optimising imaging parameters - such as excitation, stage position, and objective lenses - to capture critical events efficiently. Image analysis algorithms are pivotal in data-driven methodologies with customised approaches serving a large variety of situations. These approaches can use machine learning techniques to perform tasks such as classification, segmentation, tracking, and reconstruction without the need for explicit programming. By integrating machine learning, intelligent microscopes can make contextual decisions by identifying subtle features that traditional rule-based software may miss. Thus, these data-driven principles are able to increase the efficiency of image acquisition and enrich the information contents in diverse scenarios, especially in high throughput and high content imaging. It enables to capture discrete and rare events at different temporal and spatial scales and relate it to population behaviour. This information cannot be accessed with classical imaging approaches, especially because they would require extended exposure to cell damaging imaging conditions. In this review, we will first introduce the concept of data-driven microscopy and the common methods used to address microscopy challenges. Then, we will explain the principles and frameworks that enable reactive machine learning-based data-driven systems. Finally, we will showcase various applications that benefit from the integration of data-driven microscopy to highlight new experimental possibilities."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Synchrotron light sources are invaluable scientific tools that allow the probing of materials across bulk, micrometre and nanometre scales. These facilities perform a wide variety of research, with applications in the study of catalysis, biological function and materials science. Several next-generation synchrotron and free-electron laser facilities are scheduled to receive upgrades which will increase their brilliance by several orders of magnitude. However, more advanced experiments will require more precise and complex optical setups. Beamlines consist of a large number of optical components (e.g. mirrors, magnets, apertures), each with many degrees of freedom (corresponding to e.g. motors that translate, rotate and bend the components). These degrees of freedom can be highly correlated or degenerate, making beamline alignment in essence a high-dimensional (D>=10) and highly nonlinear optimization problem. This is typically done manually, and the design of optical systems is typically done to separate some of these dimensions and make manual alignment more feasible, e.g. by prefocusing and refocusing with a secondary-source aperture and a pair of Kirkpatrick–Baez mirrors. Nevertheless, as the complexity and precision of beamlines grow, the development of efficient and robust automated alignment methods is necessary for the efficient operation of light sources now and in the future. Such methods allow us to reach an acceptable level of alignment more quickly and robustly than with manual methods when realignment is necessary, saving preparation and commissioning time which could be used for experiments. They further allow us potentially to find better global optima than an operator could discover manually by considering all dimensions of the beamline simultaneously. They also represent the first step toward a fully autonomous beamline."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "High contrast imaging (HCI) utilizes a combination of extreme adaptive optics (XAO) and coronagraphy to generate images of faint sources near bright point sources, such as exoplanets near their host stars. Direct imaging of exoplanets has been largely limited to only a few dozen very young and luminous giant exoplanets using existing HCI instruments. However, a greater number of planets could be directly imaged by enhancing the sensitivity in the vicinity of the host star, with the performance of the XAO system being the primary limiting factor in achieving such sensitivity. In HCI, when imaging in close proximity to the star, the main performance limitations of a well-tuned adaptive optics (AO) system controlled with the common integrator controller are photon noise and temporal error. The temporal delay error of AO systems controlled by standard methods arises from the integration of wavefront sensor detector data, detector readout, computation of the correction signal, and its application to the deformable mirror (DM). This delay amounts to at least two AO system operating cycles at the maximum camera framerate, where readout takes one entire frame, during which atmospheric turbulence has evolved and no longer matches the DM correction precisely. There are two ways to mitigate the adverse effect of the temporal delay error for HCI: by increasing the operating frequency of the AO system or by implementing predictive control. The acceleration of the AO system can be accomplished, for example, by adding a second stage downstream from a classical first-stage AO system. This second-stage system solely observes the residual from the first-stage AO system and can operate independently from the first-stage, employing DMs that can handle fast AO loops. One such example is the upgrade of SPHERE, which is referred to as SPHERE+, which is expected to provide a considerable enhancement in raw point-spread function (PSF) contrast close to the star. The other (not mutually exclusive) approach is to use a predictive control algorithm. A big part of the turbulence is presumably in frozen flow considering the millisecond timescale of AO control, and hence, a significant fraction of wavefront disturbances can be predicted. Moreover, if the predictive control algorithm is fast enough, both strategies can be combined by operating the faster second stage with predictive control. Besides the performance limitations induced by photon noise and temporal error, AO can suffer from dynamic modeling errors, such as misregistration, optical gain effect for the Pyramid wavefront sensor (WFS). Coping with these limitations usually requires external tuning and recalibration of a possible predictive control algorithm. This paper presents a laboratory demonstration of a data-driven predictive control algorithm called the policy optimizations for AO (PO4AO) implemented on a second stage AO system following a first stage running a classical integrator control. One of the main advantages of implementing fully data-driven control, such as PO4AO, is that it continuously learns a system model from the data rather than using a static calibration or synthetic model. Consequently, it is less affected by pseudo-open-loop reconstruction errors, such as misregistration or the optical gain effect. Our contributions are two-fold: first, we thoroughly test the performance and robustness of PO4AO in a laboratory setup under different conditions. Second, we open-source Python-based implementation of the method that can be implemented in any AO system that runs Python-based controllers and has GPUs. We also discuss how the method can be tuned and further developed for different AO systems."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "The fourth-generation synchrotron radiation facility represents a significant advancement in X-ray brightness, offering exciting possibilities for enhanced speed and resolution in X-ray characterization. For example, next-generation synchrotrons will enable nearly atomic-level studies of materials and devices in real-time, unlocking new applications like the analysis of defects in solar materials and batteries and tracking catalytic activity within individual catalyst particles. However, fully realizing this potential necessitates highly focused hard X-ray beams that exhibit minimal wavefront distortion, exceptional stability, and, in some applications, adjustable focal sizes. Among these requirements, preserving the wavefront and coherence of the beam is vital for applications like tomography and coherent X-ray scattering experiments. Wavefront distortions can lead to a deterioration of the sample speckle contrast that hinders data interpretation and may cause phase retrieval methods to fail. To meet these rigorous demands, beamline optical elements must be manufactured to exact specifications, be able to automatically and repeatably align and focus the beam, and provide real-time correction to wavefront deformations. A possible solution to these challenges is the implementation of adaptive optics (AO), a system that functions by dynamically correcting wavefront aberrations using modulating devices, such as deformable mirrors. Successful AO implementation requires high-precision components and sophisticated control systems, such as those needed for a nanofocusing AO mirror system that integrates deformable mirrors, in-situ surface profilers, specialized wavefront sensors, and advanced feedback control systems. The performance of AOs relies on the linearity, dynamics, and repeatability of the optics response to various actuator formats, including mechanical bending, piezoelectric bimorph, and thermal loading. The traditional iterative control method based on linear response models is often slow to converge, taking significant time. We have recently demonstrated the possibility of controlling a new-generation bimorph mirror using machine learning (ML), with a single-shot wavefront sensor based on a coded-mask technique. An NN-based controller was trained with the measured one-dimensional wavefront differential phase and achieved (with a response time of a few seconds) the desired wavefront shapes with sub-wavelength accuracy at 20 keV, a significant improvement compared with the traditional linear model. However, the success of such an ML system relies heavily on the stability and repeatability of the conditions under which the training data are collected. We concluded that the control system could be enhanced by coupling with an auto-alignment system that rapidly responds to beam property changes and restores the initial conditions. Furthermore, an auto-alignment/focusing system can be fundamental for future beamline operations, especially for beamlines with numerous degrees of freedom. Manual alignment and optimization, even when limited to a few degrees of freedom, often fail to achieve an optimal configuration, and stability may be affected by changes in electron beam conditions or environmental fluctuations. Given the complex and demanding nature of the fourth-generation synchrotron beamlines, manual approaches are considered inefficient and, in many cases, nearly impossible. Instead, emerging AI-driven auto-alignment control methods using ML and optimization algorithms are being developed. They aimed to reduce alignment time, allow dynamic adjustments to the coherent focal spot size, and conserve valuable experimental time, marking a promising direction for the next generation of synchrotron radiation facility operations. In this study, advanced and ultra-realistic OASYS simulations of two beamlines at the Advanced Photon Source (APS) of Argonne National Laboratory (ANL) were utilized to demonstrate that Bayesian optimization (BO) with Gaussian processes (GPs) is a robust and efficient approach for auto-aligning X-ray focusing systems. The BO-GP method allowed for an unbiased exploration of large parameter spaces, finding globally optimal solutions even in noisy situations, proved more data-efficient than traditional black-box optimization, and overcame the issues of methods like reinforcement learning that need large data volumes and lack robustness to significant upstream drifts in the beam structure. The approach was developed through the creation of a precise digital twin of the 34-ID-C beamline at the APS, fine-tuned using actual calibration measurements. Virtual alignments were studied on the digital twin before being experimentally tested at the 28-ID-B beamline. The entire control software was integrated into an object-oriented framework allowing dynamic and transparent switching between real and simulated hardware."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Ptychography is a coherent diffractive imaging (CDI) method that has found use in light, X-ray and scanning transmission electron microscopies (STEM). The method combines diffraction patterns from spatially overlapping regions to reconstruct the structure of a specimen for arbitrarily large fields of view, with many advantages over other imaging methods. The development of new hardware and reconstruction algorithms has led to ptychography becoming a mature electron microscopy technique. Current research to further improve it is driven by the desire to investigate thick samples as well as to lower the required electron dose. In order to lower the dose, researchers have tried to vary various experimental parameters while preserving information redundancy through overlapping probes. One approach involves a defocused probe rastered across the specimen with a less dense scan pattern. This uses a lower dose than focused probe ptychography, but introduces additional complications for the reconstruction algorithm due to an increased need to account for partial spatial coherence in the illuminating probe. Another approach is simply to scan faster by lowering the dwell time per probe position, an overall decrease in dose can be realized. However, this comes with its own challenges, as the physical limits of the electron source, microscope, and camera all must be considered. Finally, a third approach is the optimization of the scan pattern, deviating from a raster grid in favour of a generally more efficient pattern. This approach can, however, only yield a limited improvement in reconstruction quality as it is not capable of taking into account the structure of the specimen in the scan pattern.\n\nIn this work we present an approach particularly tailored for electron ptychography that enables reduction of the electron dose through adaptive scanning. It is based upon the idea that, at atomic resolution, ptychography requires an increased information redundancy through overlapping illuminating beams only in regions that contain the atomic structure of the scanned specimen. We present here a workflow that scans only the regions with the highest information content in order to strongly improve the ptychographic reconstruction quality while keeping low the total number of scan positions, and therefore the total dose. The scan positions are predicted sequentially during the experiment and the only information required for the prediction process is the diffraction data acquired at previous scan positions. The scan position prediction model of the workflow is a mixture of deep learning models, and the model training is performed with both supervised and reinforcement learning (RL). A schematic of the workflow is given in Fig. 1. The synergy of deep learning and RL has already shown strong performance in various dynamic decision-making problems, such as tasks in robotics or visual recognition. The success of this approach, despite the complexity of the problems to overcome, can be attributed to the algorithms' ability of learning independently from data. Similarly, the proposed algorithm here solves a sequential decision-making problem by learning from a large amount of simulated or, if available, experimental ptychographic data consisting of hundreds to thousands of diffraction patterns. Here, the focus of the learning is specifically designed to maximize the dynamic range in the reconstruction for each individual scan position. The algorithm then transfers the learned behaviour it developed offline to a realistic experimental environment.\n\nOur approach is conceptually related to the subfield of computer vision that focuses on identifying relevant regions of images or video sequences for the purpose of classification or recognition. However, there are fundamental differences not only in the purpose, but also in the solution strategy for our application in contrast to computer vision tasks. Differences include a lack of direct access to images (updated real space information is only accessible through a highly optimized reconstruction algorithm), non-optimal parameter settings of the reconstruction algorithm and experimental uncertainties such as imprecise scan positioning of the microscope or contamination of the specimen requiring pre-processing of the reconstructed image, and the necessity of a much larger number of measurements requiring methods that improve the performance of the sequential decision making process. Work in adaptive scanning for X-ray fluorescence imaging and for scanning probe microscopy has also recently been reported. However, the work in Ref. 25 is more closely related to previous work in scanning electron microscopy that divides the measurement into a low-dose raster scan and a subsequent high-dose adaptive scan. For the latter work in Ref. 26, it has been reported that its model suffers in performance as it lacks prior knowledge of the domain structure, which can be compensated by including a deep learning model with domain specific knowledge. Our proposed approach is the first application of adaptive scanning to ptychography, and is further unique in that the scan pattern is predicted using prior knowledge about the sample in the form of a pre-trained deep neural network, thereby improving performance."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "Although promising new materials can be identified at scale using\nhigh-throughput computations, their experimental realization is\noften challenging and time-consuming. Accelerating the experimental segment of materials discovery requires not only automation but\nautonomy—the ability of an experimental agent to interpret data and\nmake decisions based on it. Pioneering efforts have demonstrated\nautonomy in several aspects of materials research, including robotic\nand Bayesian-driven optimization of carbon nanotube yield3,4\n, photovoltaic performance5\n and photocatalysis activity6\n. In contrast to conventional ML algorithms used for optimization, human researchers\nbenefit from a wealth of background knowledge that informs their\ndecision-making, and it is increasingly recognized7–9\n that autonomy\nwill require a fusion of encoded domain knowledge, access to diverse\ndata sources and active learning.\nHere we present the A-Lab, an autonomous laboratory that integrates\nrobotics with the use of ab initio databases, ML-driven data interpretation, synthesis heuristics learned from text-mined literature data and\nactive learning to optimize the synthesis of novel inorganic materials\nin powder form. Although autonomous workflows based on liquid\nhandling have been demonstrated in organic chemistry10–13, the A-Lab\naddresses the unique challenges of handling and characterizing solid\ninorganic powders. These often require milling to ensure good reactivity between precursors, which can have a wide range of physical properties related to differences in their density, flow behaviour, particle\nsize, hardness and compressibility. The use of solid powders is well\nsuited for manufacturing and technological scaleup, and the approach\nof the A-Lab to synthesis produces multigram sample quantities that\nfacilitate device-level testing.\nGiven a set of air-stable target materials (that is, desired synthesis\nproducts whose yield we aim to maximize) screened using the Materials Project14, the A-Lab generates synthesis recipes using ML models\ntrained on historical data from the literature and then performs these\nrecipes with robotics. The synthesis products are characterized by X-ray\ndiffraction (XRD), with two ML models working together to analyse\ntheir patterns. When synthesis recipes fail to produce a high target\nyield, active learning closes the loop by proposing improved follow-up\nrecipes. Over 17 days of operation, the A-Lab successfully synthesized\n41 of 58 target materials that span 33 elements and 41 structural prototypes (Supplementary Fig. 1 and Supplementary Table 1). Inspection of\nthe 17 unobtained targets revealed synthetic as well as computational\nfailure modes, several of which could be overcome through minor\nadjustments to the lab’s decision-making. With its high success rate\nin validating predicted materials, the A-Lab showcases the collective\npower of ab initio computations, ML algorithms, accumulated historical\nknowledge and automation in experimental research."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Self-driving labs (SDLs) are a rapidly growing field that offers incredible potential in improving the rate and scope of research in chemistry and materials science. SDLs are novel tools that incorporate automated experimental workflows (physical world) with algorithm-selected experimental parameters (digital world). Such autonomous experimentation tools can navigate complex and exponentially expanding reaction spaces with an efficiency unachievable through human-led manual experimentation, thereby allowing researchers to explore larger and more complicated experimental systems. At their highest degree of autonomy, the efficiency of SDLs can be derived from continuous, automated experimentation, which includes model retraining between each experiment. Such models can navigate and learn complex parameter spaces at a higher efficiency than the traditional design of experiment (DOE) approaches. These benefits thereby enable the discovery and optimization of novel and improved materials and molecules, as well as effective ways to manufacture them at scale.\n\nDue to the nascency of the SDL field in chemistry and materials science, the wide range of potential reaction space complexities, and the diversity of SDLs applied in literature, there is a need for system standards which define the criteria necessary for a system to qualify as autonomous or high performing. It should be noted that prior efforts have been made towards developing an SDL autonomy classification system for synthetic biology. In this article, building on the prior efforts of autonomy classification in synthetic biology, we propose a set of characterization metrics to delimitate between autonomy levels of SDLs in chemistry and materials sciences. Specifically, our proposed system explicitly defines the role of a human researcher for autonomy classification of SDL platforms in chemistry and materials science. While there is notable difficulty in directly comparing SDLs across different experimental spaces, many system features can be quantified and correlated meaningfully."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Chirality is a ubiquitous phenomenon in nature and plays a vital role in biological systems and life activities. In recent years, chiral functional materials have attracted increasing attention due to their potential applications in circularly polarized luminescence (CPL), enantioselective sensing, and spintronics. Among them, chiral films based on supramolecular assembly are particularly promising because of their tunable chiroptical properties and ease of processing. The chiroptical activity of these films is usually evaluated by the dissymmetry factor (g-factor), which is defined as g = 2(AL - AR)/(AL + AR), where AL and AR are the absorbance of left- and right-handed circularly polarized light, respectively. However, most reported chiral films exhibit low g-factors (typically <0.05), limiting their practical applications. Constructing chiral films with high g-factors remains a significant challenge due to the lack of understanding of the complex relationships between molecular structures, assembly conditions, and chiroptical properties. The vast chemical space and the intricate interplay of various factors make the traditional trial-and-error approach inefficient and time-consuming. Recently, data-driven approaches combined with laboratory automation have emerged as a powerful paradigm for accelerating materials discovery. By closing the loop of design, synthesis, characterization, and data analysis, self-driving laboratories can autonomously explore the design space and identify optimal materials with desired properties. Herein, we present an AI-guided robotic system for the inverse design of chiral functional films. The system integrates a mobile robot, an automated synthesis and characterization platform, and a machine learning algorithm. We chose the self-assembly of perylene diimide (PDI) derivatives and binaphthyl enantiomers as a model system to demonstrate the capability of our platform."
        },
        {
            "paper_title": "Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning",
            "source_filename": "Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf",
            "content": "Microscopic observation serves as a crucial instrument in scientific research, biomedical studies, and engineering applications, and has been extensively utilized across various domains, including medicine, materials science, and industry[cite: 1317]. However, traditional microscopes, characterized by intricate optical systems and focusing mechanical structures, are often cumbersome and susceptible to damage, thereby imposing significant constraints on scientific operations and application scenarios[cite: 1318]. Microscopic imaging necessitates the precise capture of samples with intricate details across an extensive range, rendering the continuous maintenance of optimal focus critically important[cite: 1319, 1323]. Consequently, the miniaturization of microscopic imaging systems and the development of rapid autofocusing techniques have been longstanding objectives in relevant fields, aimed at addressing the continually evolving demands of scientific and technological advancement[cite: 1323]. The construction of traditional microscopes typically incorporates a combination of multiple fixed-focus lenses and mechanical structures, which are employed to achieve imaging functions such as magnification and focusing[cite: 1324]. Additionally, they necessitate an adequate optical path length to enable the requisite mechanical movement for focus adjustment[cite: 1325]. Consequently, these designs are inevitably encumbered by drawbacks including bulky volumes, sluggish focusing speeds, and difficulties in enabling rapid autofocusing or operation within confined spaces[cite: 1326]. In contrast, owing to the absence of mechanical components and the ability to achieve focusing by adjusting electrical signals, liquid lenses offer advantages such as compact size, rapid response, and low manufacturing costs[cite: 1327, 1334]. Microscopes equipped with liquid lenses do not require additional mechanical parts for focusing, which effectively reduces the overall volume and enhances the efficiency of autofocusing[cite: 1335].  The field of microscope autofocus technology has witnessed considerable advancements over the past few decades[cite: 1336]. The advent of artificial intelligence and new optical components in recent years has led to the emergence of novel research trends in this field[cite: 1337]. Active autofocus microscopes employ the transmission and reception of specific signals to measure the distance to the object and achieve focus[cite: 1338]. While these methods are highly reliable and have rapid focusing speeds, they necessitate specialized hardware support, high system complexity, and high cost[cite: 1341, 1342]. Furthermore, passive microscope autofocus methodologies, which assess the extent of out-of-focus and identify the focusing position exclusively through images, have been extensively investigated due to their advantageous simplicity and cost-effectiveness[cite: 1343]. However, these methods usually require multiple image acquisitions and evaluations, and the focusing speed is slow[cite: 1346]. To enhance the speed and precision of focusing, some scholars have proposed the integration of deep learning into microscope autofocusing techniques[cite: 1347]. Nevertheless, these approaches are contingent upon the quality and quantity of the training data, which may limit its efficacy in scenarios that are not previously encountered[cite: 1353, 1354]. Furthermore, these methodologies fail to consider the sequence data embedded within the focusing process, which is challenging to fully utilize and optimize[cite: 1354]. Deep Reinforcement Learning is capable of learning the optimal decision-making strategy through continuous trial-and-error interaction with the environment[cite: 1355]. In addition, the system is capable of utilizing sequential information to derive a more objective strategy, which is particularly well-suited to the autofocus task[cite: 1356]. While these studies have contributed to the advancement of autofocusing techniques for microscopes, several challenges remain[cite: 1360]. (1) System complexity and cost: The autofocus of liquid lenses still necessitates the establishment of a focal-voltage model to adjust the electrical signal focus following the calculated out-of-focus distance, which consequently increases the pre-calibration workload[cite: 1361, 1363]. (2) Dependence on evaluation metrics: The annotation of some datasets still relies excessively on image quality evaluation values[cite: 1365]. Consequently, a reliance on image quality ratings alone may result in the model acquiring a restricted range of knowledge[cite: 1367]. (3) Insufficient generalization ability: Deep learning-based models are dependent on training data and require a substantial quantity of diverse data to enhance network performance[cite: 1368]. This results in a homogeneous nature of the training data, which in turn affects the model's generalization ability[cite: 1371]. To address the aforementioned issues, we proposed the implementation of an adaptive Liquid Lens Microscope System that utilizes Deep Reinforcement Learning-based Autofocus (DRLAF)[cite: 1372, 1394]. By leveraging the rapid response and electrical adjustment advantages of liquid lenses, this methodology employs sequential raw images as the \"state\" input for the deep reinforcement learning agent, to enable the model to discern objective focusing knowledge from them[cite: 1394]. Concurrently, different voltage adjustments are regarded as executable \"actions\" and deep reinforcement learning is employed to optimize the focusing policy[cite: 1395]. Additionally, the model is enhanced through the utilization of random sampling from parallel \"state\" datasets during the training phase, thereby facilitating its ability to generalize and adapt to unknown samples[cite: 1396]."
        }
    ],
    "Results": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Self-driving scanning microscopy workflow\nFigure 2A broadly illustrates the FAST workflow for the experiments reported here[cite: 3277]. To initiate the workflow, a low-discrepancy quasi-random selection of sample position is measured corresponding to 1% of the total area of interest[cite: 3278]. The integrated intensities are transferred to an edge device, which uses Inverse Distance Weighted (IDW) interpolation to estimate the dark-field image, which serves as input for decision-making[cite: 3278]. This workflow adopts the SLADS-Net algorithm, which uses current measurements to identify unmeasured points that would have the greatest effect on reconstruction quality (Expected Reduction in Distortion or ERD)[cite: 3279, 3280, 3283]. We select a batch of 50 points with the highest ERD to minimize experimental dead-time[cite: 3292]. The coordinates of these 50 points are passed to a route optimization algorithm based on Google’s OR-Tools to generate the shortest path[cite: 3293]. For the 200 × 40 pixels object, the workflow required ≈ 0.15 s to compute new positions and ≈ 42 s to scan the set of 50 positions, representing an overhead of ≤ 2% [cite: 3342, 3343].\n\nNumerical demonstration for scanning dark-field microscopy\nWe validated the performance through a numerical experiment on pre-acquired dark-field microscopy data, comparing FAST with Raster grid (RG), Uniform random (UR), and Low-discrepancy random (LDR) sampling [cite: 3348-3356]. The test dataset is a dark field image of size 600 × 400 pixels representing WSe2 flakes[cite: 3357, 3358]. FAST sampling reproduces with high fidelity the flake boundaries and bubbles at 10% sampling, whereas LDR and raster schemes produce lower-quality reconstructions[cite: 3365, 3422]. The FAST reconstruction stabilizes at 27% coverage[cite: 3353]. FAST preferentially samples regions with significant heterogeneity over homogeneous regions [cite: 3428].\n\nExperimental demonstration\nWe demonstrated the application of the FAST workflow in a live experiment at a synchrotron beamline on a WSe2 sample with a field of view of 20 × 4 µm (200 × 40 points)[cite: 3431, 3435]. FAST identifies heterogeneity (bubble edges) within 5% coverage and is extensively sampled by 15% coverage[cite: 3437, 3438]. Reconstruction stabilizes between 15 and 20%[cite: 3440]. A partially scanned bubble appearing only in the 20% scan highlights the exploration-exploitation tradeoff in Bayesian search [cite: 3441-3444]. Furthermore, quantitative properties like the Center of Mass (CoM) calculations for film curvature were faithfully reproduced by FAST with just 20% scan coverage compared to the full 100% raster scan [cite: 3447-3456]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "First, we demonstrated the process of training of a deep reinforcement learning agent on a model of environment. Next, we used a trained agent to tune a real laser system. Finally, we modified the environment by changing the pump current of the laser diode and showed that the strategies that the agent has learned also allow tuning of such systems. We collected the data of the regimes for the range of admissible values of the spectral filter at a fixed value of the pump current of the laser diode of 2.7 A. The model of the environment was created using the measured data. Figure 3 shows variations of reward during the training process of the deep RL agent. In this case, one epoch consisted of 100 sessions with 200 actions each. The session started with a random initial value of the spectral filter parameters, which made the learning curve look noisy. The graph shows that after the 400th epoch, the algorithm gained on average a cumulative reward equal to 42. It should be noted that since the agent received a reward at each step, the learning curve starts from a nonzero value. Because the laser was controlled by varying only two parameters, the data itself, as well as the trajectory of the laser adjustment, can be displayed on a two-dimensional map. In Fig. 4a, the color indicates the value of the reward function R for collected data at the pump current 2.7 A. The black dotted lines show the agent trajectories obtained on the model of environment. Since the model was a deterministic system, these trajectories converge into one, and the trajectories for the same starting points will always repeat themselves. In the Fig. 4a, colored lines represent the trajectories of the agent when tuning a real laser system. The starting points were chosen in such a way that they had non-pulsed generation regimes. One can notice that even though the starting positions of the from the model and from the real system were the same, the trajectories themselves were different. This is because the real environment is a stochastic system with unknown dynamics of transitions from one state to another further complicated by the presence of noise. Despite this fact, the deep reinforcement learning agent was able to tune a real laser system. Figure 4b shows the dependency of the immediate reward on the step for three trajectories from the real system. One may notice that the reward increases throughout the tuning session. After computing the weights of the deep RL agent's neural network at the laser diode pumping current of 2.7 A, we applied this agent to environments with different currents - first to 2.1 A and then to 1.7 A. We demonstrated that even in these cases the agent was able to find mode-locking regimes. Figure 5a shows the trajectories that were obtained for different pump currents of the laser diode with the same starting point of the system. To demonstrate the difference of the environment for the pump current different than 2.7 A, Fig. 5a shows a map corresponding to the current of 2.1 A. It should be noted that since the reward depends on the pulse power (Eq. 1), an increase in the pump gain leads to a decrease of the reward. Comparison of the trajectories of the agent in Fig. 5a shows that starting from about the 150th step, they are close. At this stage, the agent has already found a stable mode locked regime, and continued to search for a state with a maximum reward. However, in the beginning the trajectories are very different despite the same starting point. The reason is that the algorithm tries to cling to a stable generation regime, but the stable lasing field decreases with the decay of the pump current, which can be seen through the comparison of Figs. 4a and 5a. The trajectories in Fig. 5a show that the algorithm adapts to these changes and allows tuning the laser even in the case of the changed environment."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Autonomously discovering a Pareto front. We upgraded the\nhardware and software of our existing self-driving laboratory,\nAda8, (Fig. 2) to study the combustion synthesis of conducting\npalladium films. This upgraded self-driving laboratory was\ndesigned to map out a Pareto front that shows the tradeoff\nbetween the temperature at which the films are processed and the\nfilm conductivity. We selected combustion synthesis as an optimization problem because it is a solution-based method for\nmaking functional metal coatings. This method, however, has not\nyet been scaled and has not been proven for making high-quality,\nconductive metal films40,43,44. Combustion synthesis can form\ncoatings at lower temperatures, enabling the potential use of\ninexpensive polymeric substrates45,46, but film conductivity\ntypically decreases with processing temperature43. This situation\npresents a trade-off: to what extent can the conductivity be\nmaximized while the processing temperature is minimized? The\nanswer to this question would enable the researcher to determine,\nfor example, what types of substrates could be layered with a\nmetal coating of certain conductivity. We therefore leveraged Ada\nto effectively study the numerous compositional47,48 and processing variables43,49 that influence processing temperatures and\nthe corresponding conductivities.\nFor this study we configured Ada to manipulate four variables:\nfuel identity, fuel-to-oxidizer ratio, precursor solution concentration, and annealing temperature (Fig. 3a). We confined the study\nto mixtures of two fuels, glycine and acetylacetone, that we\nindependently identified to yield conductive films at temperatures\nbelow 300 °C. The fuel-to-oxidizer ratio was varied because it\ncontrols product oxidation in bulk combustion syntheses50,51.\nThe precursor concentration influences the morphology of the\ndrop-casted films. Finally, we varied the processing temperature\nwhich may influence the conductivity through solvent removal,\nprecursor decomposition, film densification, impurity removal,\ngrain growth, oxidation, or cracking52–54.\nFlexible automation4 enabled us to upgrade Ada (Fig. 2a) by\ncoupling a larger, 6-axis robot to the existing smaller, 4-axis\nrobot. The smaller robot (Fig. 2b) deposited and characterized the\nthin films8, while the larger robot transported the samples to a\ncommercial X-ray fluorescence (XRF) microscope for elemental\nanalysis. These two robots jointly executed a 7-step experimental\nworkflow (Fig. 2c, see “Methods” section). First, a combustion\nsynthesis precursor solution was formulated from stock solutions\nand then drop-cast onto a glass microscope slide. The resulting\nprecursor droplet was imaged and then annealed in a forcedconvection oven to form a film. The film was subsequently\ncharacterized by XRF microscopy, imaging, and 4-point probe\nconductance mapping. The conductivity of each film was\ndetermined by combining the conductance with a film thickness\nestimated by XRF (see “Autonomous workflow step 7” in\n“Methods” section, Supplementary Fig. 2). Finally, the conductivity and processing temperature for each film were passed to a\nmultiobjective Bayesian optimization algorithm55 to plan the next\nexperiment based on all the available data (see “Autonomous\nworkflow step 8” in “Methods” section). The algorithm we used is\ncalled q-expected hypervolume improvement (qEHVI)55. All of the steps in the autonomous workflow were performed\nwithout human intervention at a typical rate of two samples an\nhour. Ada could run unattended for 40–60 experiments until the\nnecessary consumables (e.g., pipettes tips, mixing vials, glass\nsubstrates, and precursors; see “Methods” section) were\nexhausted. We used Ada to execute a total of 253 combustion\nsynthesis experiments that explored a wide range of pertinent\ncomposition and processing variables.\nThe qEHVI algorithm is one of a number of a posteriori\nmultiobjective optimization algorithms designed to identify the\nPareto front55–58. These multiobjective optimization methods are\nknown as a posteriori methods because preferred solutions are\nselected after the optimization. We chose to use an a posteriori\nmethod for this exploratory study, because we sought to identify a\nrange of Pareto-optimal outcomes rather than a single optimal\npoint. We selected the qEHVI algorithm because previously\nreported benchmarks show that the qEHVI algorithm often\nresolves the Pareto front in fewer experiments than other\nalgorithms.55\nThe qEHVI algorithm directed our self-driving laboratory to\nquantify the trade-off between film conductivity and annealing\ntemperature (Fig. 3). We manually selected eight synthesis\nconditions spanning most of the design space to provide\ninitialization data for the qEHVI algorithm (Supplementary\nTable 1). After executing these initial experiments, Ada executed\nmore than 50 iterative qEHVI-guided experiments to map the\nPareto front of annealing temperature and conductivity. We\nperformed this autonomous optimization campaign in quadruplicate. Each replicate generated a Pareto front showing a clear\ntrade-off between temperature and conductivity (Fig. 3b).\nThe synthesis conditions tested during the optimization are\nshown in Fig. 3a; the conditions that created materials on the\nPareto front are highlighted. The data revealed that the optimal\nprecursors typically were those of concentrations near 6 mg mL−1,\nfuel-to-oxidizer ratios below 1, and fuel blends consisting\nprimarily of acetylacetone. Notably, our experiments did not\nreveal a single optimal synthesis condition. The conditions\nrequired to obtain the maximum conductivity depended in part on the annealing temperature. Specifically, conductive films\ncreated below 200 °C required precursors with predominantly\nacetylacetone fuel. At higher temperatures, however, glycine-rich\nfuel blends also yielded samples on the Pareto front. The data\nshows how the fuel-to-oxidizer ratio could vary widely for fuels\nrich in acetylacetone yet still yield films on the Pareto front. The\nPareto-optimal samples resulting from glycine-rich fuel blends,\nhowever, did not exhibit a wide range of fuel to oxidizer values.\nThese observations highlight the richness of the data generated by\nthe self-driving laboratory."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "The result of adaptive scanning on experimentally acquired MoS₂ data and its comparison to the result of a sparse grid scanning and the conventional grid scanning procedure is shown in Fig. 2. The data used for the comparison was not part of the training data for the adaptive scanning model. However, the entire data was acquired from the same sample and includes multiple data sets that were recorded from different regions of the sample. This data can be found in Ref. 28. In our comparison, a ground truth reconstruction is obtained from one of the data sets each consisting of 10,000 diffraction patterns, while only 250 diffraction patterns have been used for the adaptive scanning as well as the sparse grid scanning reconstruction. Figure 2a shows the ptychographic reconstruction when using a sparse grid scanning procedure. The structure of the material is not clearly resolved and/or shows ambiguous features. Figure 2b shows the reconstruction when the scan positions are predicted through adaptive scanning. Although without the same homogeneous reconstruction quality throughout the entire field of view, the structure of the MoS₂ material is now much better resolved and is closer to the ground truth reconstruction of the full data grid scanning procedure, shown in Fig. 2c.\n\nFurther examples of reconstructions and their corresponding scan sequences are shown in Fig. 3. The results suggest that probe delocalization due to scattering plays an important role as to why an improved ptychographic reconstruction can be achieved by distributing the scan positions predominantly on the atoms of the specimen. This implies that similar results could be achieved by using RL with a reward function that specifically emphasizes the scattered electrons in the recorded diffraction patterns, which is an interesting area for future research. The final point of our experimental investigation into adaptive scanning in ptychography evaluates the performance of the method for various prediction settings. We compare the Fourier ring correlation (FRC) as well as the structural similarity index measure (SSIM) between the reconstruction obtained from the reduced data and the ground truth reconstruction obtained from the full data to quantify the improvement of the effective image resolution and image quality when using adaptive scanning. In the first comparison, shown in Fig. 2d, we apply the FRC to the sparse grid scan and adaptive scan averaged over 25 data sets, respectively. For both cases, there is a sharp frequency cut off in the proximity of atomic resolution (1.2 Å). However, while the cross-correlation value almost disappears in the case of the sparse grid scan, it plateaus at a value of about 0.2 in the case of the adaptive scan. This result indicates the benefit in terms of achievable resolution of adaptive scanning in contrast to other low dose alternatives. In the latter comparison, SSIM_a and SSIM_s refer to reconstructions of reduced data obtained with the adaptive scanning and the sparse grid scanning procedure, respectively. Table 1 shows the relative reconstruction quality improvement Q_SSIM = (SSIM_a − SSIM_s)/(SSIM_s) for different experimental settings averaged over the same 25 data sets as used before. In the case of 250 scan positions, which corresponds to a dose reduction by a factor of 40 with respect to the original data, tests were performed for different total numbers T of sub-sequences and therefore different amounts of scan positions included in each sub-sequence R^P_t. The quality improvement ranges from 9.89% to 15.84% for 2 to 5 sub-sequences, respectively. Note that the scan positions of the first sub-sequence R^P_0, provided to the RNN as part of the initial input, follow the sparse grid sequence and that the scan positions of each sub-sequence only cover a part of the sample, as can be seen in Fig. 6b. Further tests were performed using a larger number of total scan positions and 5 sub-sequences. However, the difference in quality between the reconstruction generated with the positions of the adaptive scan and the sparse grid scan decreases with the total number of positions used, as can be expected, since the sparse grid sampling covers the sampled area in an increasingly complete manner. These results indicate that the reconstruction quality improves with the frequency by which the positions are predicted, and that low dose experiments benefit the most from the adaptive scanning scheme.\n\nIn Fig. 4, we compare the results of various scanning procedures using simulated double-walled carbon nanotube (DWCNT) data. This data is publicly available. Figure 4a shows a ptychography reconstruction that considers 840 diffraction patterns that have been selected through the adaptive scanning procedure. Most of the unit cells of the structure can be resolved with a high contrast and therefore the configuration of the DWCNT can be easily deduced. The predicted scan positions coincide to a high degree with the structure of the DWCNT. Note that the initial scan sub-sequence visualized at the bottom of the reconstruction follows a sparse grid scan sequence. Figure 4b shows the reconstruction when using 840 diffraction patterns obtained from a sparse grid scanning procedure. The field of view is now much better covered with scan positions, but the periodicity at which the scan positions are spaced seems to be also present in the reconstruction. Hence, the reconstruction shows ambiguous features that make the interpretation of the structure more difficult compared to the previous case. Figure 4c shows the reconstruction of an alternative low-dose scanning procedure which has been described conceptually in Ref. 27. Here, two consecutive scans are performed. The first scan is a conventional dense grid scan consisting of 13,225 diffraction patterns with an electron dose of 4e3 e⁻/Å⁻² compared to 1e5 e⁻/Å⁻². The same scan with the latter dose has also been used for the dense grid scan in Fig. 4d. The second scan of the alternative low-dose scanning procedure was limited to 311 scan positions as to match the total electron dose of the procedures in Fig. 4a and b. An atom finding method was applied to the ptychography reconstruction generated after the first scan to adapt these 311 scan positions to the atomic structure of the DWCNT specimen. Although the predicted scan positions in this approach match the atomic structure almost perfectly, their contribution to the final reconstruction seems to be not sufficient given that most of the total electron dose is required for their optimal prediction. Quantitatively, we obtain a relative reconstruction quality improvement Q_SSIM of 2.60 ± 3.38% and 16.97 ± 3.49% when using adaptive scanning with respect to the sparse grid scanning and alternative low-dose scanning procedure, respectively."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "The AI-guided robotic system consists of a mobile robot (armed with a collaborative robotic arm), an automated liquid handling station, a spin-coater, a hotplate, and a circular dichroism (CD) spectrometer (Fig. 1). The mobile robot acts as a courier, transporting samples between different stations. The liquid handling station performs the preparation of precursor solutions and the mixing of chiral and achiral components. The spin-coater and hotplate are used for film fabrication and thermal annealing, respectively. The CD spectrometer is employed to characterize the chiroptical properties of the films. The entire workflow is orchestrated by a central control software, which coordinates the actions of the hardware modules and manages the data flow. The brain of the system is a BO algorithm, which iteratively updates the surrogate model based on the experimental feedback and suggests the next batch of experiments to maximize the g-factor. To validate the performance of our system, we focused on the optimization of chiral films formed by the co-assembly of an achiral PDI derivative (PDI-1) and a chiral binaphthyl dopant (R-1 or S-1). The chemical structures are shown in Fig. 2. We selected four experimental variables for optimization: the concentration of PDI-1, the molar ratio of dopant to PDI-1, the spin-coating speed, and the annealing temperature. The objective was to maximize the absolute value of the g-factor at the characteristic absorption band of PDI aggregates. The optimization process started with a random initialization of 10 experiments. Then, the BO algorithm took over and iteratively guided the robot to explore the design space. Fig. 2a shows the evolution of the maximum g-factor as a function of the number of experiments. It can be seen that the g-factor increased rapidly in the first few iterations and reached a plateau after about 50 experiments. The robot successfully identified a champion film with a g-factor of -0.23, which is the highest value reported for PDI-based chiral films to date. In contrast, a random search strategy with the same number of experiments only yielded a maximum g-factor of -0.08. We also compared the efficiency of the robot with human researchers. It took the robot about 6 weeks to complete 100 experiments, while a human researcher would typically need several months to achieve the same amount of work, considering the time for data analysis and experiment planning. The high efficiency of the robotic system is attributed to its 24/7 continuous operation and the intelligent sampling strategy of the BO algorithm."
        }
    ],
    "Discussion": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "In this work, we have showcased the FAST workflow that combines a sparse sampling algorithm with route planning to drive a scanning diffraction microscopy experiment[cite: 3480]. For our live demonstration, the FAST decision-making time was negligible, leading to an overall saving of ≈ 80 min (65%) of the experiment time[cite: 3482]. The generalizability of the FAST method comes from the fact that the key NN-based component is trained on just the standard cameraman image, not on close analogs of a sample of interest[cite: 3485]. The computational time complexity is O(2N log N + kM log N), which stands in contrast to O(N^3) for Gaussian Processes[cite: 3493, 3496]. For a 200 × 40 image, our workflow performs calculation within 1.5 s on a low-power CPU, compared to 6 s on a GPU for a smaller image using GPs[cite: 3497, 3498]. Challenges include the potential to miss isolated small features during initial sampling and the time required for motor movement[cite: 3504, 3510]. Future extensions include 3D imaging, fly scans, and ptychography[cite: 3518]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Deep Reinforcement Learning is a powerful tool that can be used to setting up a laser system. In this paper, we have demonstrated how DDQN algorithm may be implemented for self-tuning task of an experimental mode-locked fiber laser with a nonlinear loop mirror. The algorithm successively have found physical patterns. In our laser, the spectral profile of gain and losses is unevenly distributed along the wavelength and reaches its maximum at a wavelength of 1068 nm. Based on the results obtained, we conclude that the algorithm has mastered this feature of the system and, when tuning, tried to find solutions in the vicinity of this wavelength. Note that Figs. 4 and 5 show that the steepest path from the initial point to the resulting solution is a straight line. However, this trajectory passes through a large area of unstable generation. Too large or too narrow band-passes of the filter lead to unstable pulse generation or generation of the soliton molecules. Considering the tuning trajectories, the algorithm learned to bypass these areas, which allowed it to find the area of stable solutions immediately and continue tuning already in it. In this work, we used the universal reward Paverage/Pnoise, which is not tied to a certain laser. Thus, the proposed deep reinforcement learning algorithm can potentially be used to solve the problem of self-tuning of other laser systems. We anticipate that further study will unlock true potential of the proposed technique in the complex laser systems."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Here, we mapped out a Pareto front between film processing\ntemperature and conductivity using a self-driving laboratory\nguided by the qEHVI multi-objective optimization algorithm.\nThis tradeoff is just one example of the conflicting objectives\nroutinely faced by materials scientists to which our method could\nbe applied. Our approach eliminates the need for the researcher\nto specify preferences between competing objectives in advance of\nthe experiment, and also produces a richer, more valuable data\nset. In this case, the temperature–conductivity Pareto front is\nmore useful than optimizing conductance for a fixed temperature\nlimit because processing temperature limits vary depending on\nthe application. Our self-driving laboratory also identified\nsynthesis conditions that translated to a scalable spray-coating\nmethod for depositing high-quality, high-conductivity palladium\nfilms at temperatures above 190 °C. This work shows how selfdriving laboratories can potentially accelerate the translation of\nmaterials to industry, where satisfying multiple objectives is\nessential "
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "To understand the mechanism underlying the high g-factor, we analyzed the experimental data collected by the robot. Fig. 3 shows the distribution of g-factors in the parameter space. We found that high g-factors are concentrated in a specific region defined by a moderate PDI concentration, a high dopant ratio, a low spin-coating speed, and a high annealing temperature. This suggests that the formation of highly dissymmetric structures requires a delicate balance between the aggregation rate and the chiral induction efficiency. We further characterized the morphology and molecular packing of the optimal film using atomic force microscopy (AFM) and X-ray diffraction (XRD). The AFM images (Fig. 4) revealed that the film consists of interconnected nanofibers with a helical twist. The XRD patterns indicated a highly ordered lamellar packing of PDI molecules. Based on these observations, we propose a mechanism for the chiral amplification in the film formation process. The chiral dopant induces a slight twist in the PDI stacks during the initial nucleation stage. This chiral bias is then amplified during the subsequent growth and annealing processes, leading to the formation of long-range ordered helical structures with strong chiroptical activity. The BO algorithm effectively navigated the complex energy landscape to find the optimal conditions for this chiral amplification. Our results demonstrate that AI-guided robotics can not only accelerate the discovery of new materials but also provide physical insights into the structure–property relationships."
        }
    ],
    "Methods": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "The SLADS-Net algorithm: The SLADS-Net algorithm is an adaptation of the SLADS algorithm, using a supervised procedure to estimate the reduction in distortion (RD)[cite: 3521, 3531]. It uses feature vectors capturing the local neighborhood of a pixel (spatial gradients, deviation from nearby values, measurement density)[cite: 3535, 3536]. These features are transformed using a radial basis function (RBF) kernel and processed by a nonlinear fully connected neural network (5 hidden layers, 50 nodes each) to predict the Expected Reduction in Distortion (ERD) [cite: 3538, 3560].\n\nTraining: We train the SLADS-Net neural network on only the standard cameraman image without using any a priori information about the sample[cite: 3540]. We generate training pairs for 10 different sample coverage percentages between 1% and 80% and train for 100 epochs using the Adam optimizer [cite: 3541-3543].\n\nExperimental measurements: At each measurement point, a tight region of interest (RoI) around the expected position of the thin film Bragg peak was extracted, and integrated intensities were used to guide the NN prediction [cite: 3552, 3553].\n\nStatistics: The imaged region was selected through visual inspection; no statistical method was used to predetermine sample size, and investigators were not blinded[cite: 3572, 3574, 3578]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Reinforcement learning. Reinforcement Learning (RL) is a field of machine learning in which an agent is trained in an environment so that its behavior maximizes the cumulative reward. RL can be attributed to one of the machine learning paradigms, along with supervised learning and unsupervised learning. In addition to concepts such as agent and environment, reinforcement learning theory uses terms such as policy and reward signal R. The policy defines the way a learning agent behaves at a given moment. In other words, politics is a mapping of perceived states of the environment and the actions to be taken in those states. The reward signal defines the goal of the reinforcement learning task. At each time step, the environment sends a single number to the agent, called a reward. The agent's sole goal is to maximize the overall reward he receives over the long term. Thus, the reward signal determines which events are good and bad for the agent. Q-learning. Q-learning is an off-policy temporal difference (TD) learning algorithm that approximates the value of a state-action value function or Q-function based on previously obtained estimates of this function. The Q-function is defined as the expected discounted reward that an agent will receive when starting the game in states s with action a and then acting following policy π. Deep Q-learning. We obtain a deep reinforcement learning algorithm when we use deep neural networks to approximate the policy, Q-functions or anther RL function. This approach is used when we are dealing with continuous environments in which the number of states or actions is unlimited. In the Deep Q-Network (DQN) algorithm, neural networks (NN) are used to approximate the values of the function Q(s, a, θ) where the parameters θ are the weights in the deep neural network. In our case, we use a multilayer neural network, which receives states s as input, and the output of this NN is a vector of values Q(s, ai, θ) for each of the possible actions ai."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Materials. MeCN (CAS 75-05-8; high-performance liquid chromatography\n(HPLC) grade, ≥99.9% purity), glycine (CAS 56-40-6, ACS reagent grade, >98.5%\npurity) and acetylacetone (CAS 123-54-6; ≥99% purity) were purchased from\nSigma-Aldrich. Urea (CAS 57-13-6, ultra-pure; heavy metal content 0.01 ppm) was\npurchased from Schwarz/Mann. Palladium(II) nitrate hydrate (Pd(NO3)2•H2O; Pd\n~40% m/m; 99.9% Pd purity, CAS 10102-05-3) was purchased from Strem Chemicals, Inc. All chemicals were used as received without further purification. Manual preparation of stock solutions. The self-driving laboratory is provided\nwith starting materials in the form of stock solutions which are prepared manually\nand then placed in capped 2 mL HPLC vials in a tray where they can be accessed by\nthe self-driving laboratory. All solutions were prepared at a concentration of\n12 mg mL−1. The Pd(NO3)2•H2O solution was prepared using MeCN as a solvent\nwhile all other solutions were prepared using deionized H2O.\nPreparation of glass substrates and other consumables. In addition to stock\nsolutions, the self-driving laboratory uses consumable glass substrates (75 mm ×\n25 mm × 1 mm microscope slides; VWR catalog no. 16004-430), 2 mL HPLC vials\n(Canadian Life Science), and 200 µL pipettes (Biotix, M-0200-BC). These are placed\nin appropriate racks and trays for access by the robotics.\nThe HPLC vials and pipettes were used as received, whereas the microscope\nslides were cleaned by sequential sonication in detergent, deionized water, acetone,\nand isopropanol for 10 min each8. Wells of 18 mm diameter were then created on\nthe microscope slides using a sprayed enamel coating (DEM-KOTE enamel finish)\nand circular masks placed at the center of each slide (Supplementary Fig. 1). The\nwells serve to confine the precursor solution before it dries.\nSelf-driving laboratory. The self-driving laboratory consists of a precision 4-axis\nlaboratory robot (N9, North Robotics) coupled with a 6-axis collaborative robot\n(UR5e, Universal Robotics). The 4-axis robot is equipped to perform entire thin\nfilm deposition and characterization workflows and is described in our previous\nwork8. The 6-axis robot enables samples to be transferred to a variety of additional\nmodules, including the XRF microscope used here. Both robots are equipped with\nvacuum-based tools for substrate handling. All robots and instruments were\ncontrolled by a PC with software written in Python.\nOverview of autonomous robotic workflow. The majority of operations in the\nautonomous robotic workflow are performed by the 4-axis laboratory robot.\nSamples are transported between the 4-axis robot and the XRF microscope by the\n6-axis robot.\nThe 4-axis robot prepared each sample by combining stock solutions to form a\nprecursor mixture, drop casting this precursor onto a glass slide, and then\nannealing the sample in a forced convection oven (Supplementary Fig. 1). The\nsamples were characterized by white light photography before and after annealing,\nX-ray fluorescence microscopy, and 4-point-probe conductance measurements.\nThe resulting data was then automatically analyzed using a custom data pipeline\nimplemented in Python. Finally, the result of the experiment was fed to a Bayesian\noptimizer which used an expected hypervolume improvement acquisition function\nto select the next experiment to be performed. Each of these steps is described in\nfurther detail below.\nAutonomous workflow step 1: mix precursors. The 4-axis robot formulated each\nprecursor by pipetting varying volumes of the stock solutions described above into\na clean 2 mL HPLC vial. Gravimetric feedback from an analytical balance (ZSA120,\nScientech) was used to minimize and record pipetting errors. The precursor was\nmixed by repeated aspiration and dispensing.\nAutonomous workflow step 2: drop cast precursor. The 4-axis robot used a\nvacuum-based substrate handling tool to place a clean glass slide onto a tray. This\nrobot then created a thin film sample by using a pipette to drop cast 98 µL of the\nprecursor into a predefined well on the slide. The solution was ejected from the\npipette at a rate of 5 µL s−1 from a height of approximately 1.5 mm above the top\nsurface of the substrate.\nAutonomous workflow step 3: image precursor droplet. The 4-axis robot\nacquired visible-light photographs of each sample before annealing. This robot\npositioned samples 90 mm below a camera (FLIR Blackfly S USB3; BFS-U3-\n120S4C-CS) using a Sony 12.00 MP CMOS sensor (IMX226) and an Edmund\nOptics 25 mm C Series Fixed Focal Length Imaging Lens (#59–871). The C-mount\nlens was connected to the CS-mount camera using a Thorlabs CS- to C-Mount\nExtension Adapter, 1.00″-32 Threaded, 5 mm Length (CML05). The sample was\nilluminated from the direction of the camera using a MIC-209 3-W ring light. For\nimaging, the lens was opened to f/1.4, and black flocking paper (Thorlabs BFP1)\nwas placed 10 cm behind the sample.\nAutonomous workflow step 4: annealing. After drop casting, the 4-axis robot\nused the substrate handling tool to transport the precursor-coated slide into a\npurpose-built miniature convection oven for annealing at a variable temperature\nbetween 180 and 280 °C. The most important features of the oven are a lowthermal mass construction (lightweight aluminum frame with glass-fiber insulation) and internal and external fans. These features enable rapid heating and\ncooling of the sample (Supplementary Fig. 12). A pneumatically actuated lid\nenables robotic access to the sample. The oven employs a ceramic heating element\n(P/N 3559K23, McMaster Carr) controlled by a PID temperature controller (P/N\nCN7523, Omega Engineering). A type-K thermocouple located in the oven air\nspace provides temperature feedback to the controller. In the experiments performed here, the sample was inserted into the oven which was then ramped at\n40 °C per minute to the temperature set point, which was then held for 450 s. Upon\ncompletion of the hold, the oven lid was opened and a cooling fan turned on to\nblow ambient temperature air through the oven and over the sample. The sample\nwas removed from the oven after the temperature dropped below 60 °C. The oven\nwas further cooled to below 40 °C prior to loading of the next sample.\nAutonomous workflow step 5: XRF imaging and data analysis. The self-driving\nlaboratory acquired hyperspectral X-ray fluorescence (XRF) images of each sample\nusing a Bruker M4 TORNADO X-ray fluorescence microscope equipped with a\ncustomized sample fixture. Samples were transported to the XRF microscope by the\nUR5e 6-axis robotic arm equipped with a vacuum-based substrate handling tool\nsimilar to the one used by the 4-axis N9 robot. A dedicated exchange tray accessible\nto both robots enabled samples to be passed from one robot to the other.\nThe XRF microscope has a rhodium X-ray source operated at 50 kV/600 µA/\n30 W and polycapillary X-ray optics yielding a 25 µm spot size on the sample. The\ninstrument employs twin 30 mm2 silicon drift detectors and achieves an energy\nresolution of 10 eV. Hyperspectral images were taken over a 20 mm × 20 mm area\nat a resolution of 125 × 125 pixels. The XRF spectra obtained (reported in counts)\nwere scaled by the integration time (50 ms) and the energy resolution (10 eV) to\nyield units of counts s−1 eV−1.\nTo quantify the relative amount of palladium in the film, the palladium Lymanalpha X-ray fluorescence line (2.837 keV) was integrated from 2.6 to 3.2 keV. The\nresulting counts were converted to film thickness estimates by applying a\ncalibration factor obtained using reference samples (see below). Ninety-seven\npoints of interest are defined within the XRF hypermap of the sample, as defined in\nSupplementary Fig. 3. For each point of interest, the average XRF counts\nper second were calculated over a 3 mm × 3 mm area.\nAutonomous workflow step 6: image annealed film. The self-driving laboratory\nacquired visible light photographs (as described in step 3) of each sample after\nannealing.\nAutonomous workflow step 7: film conductivity measurement. After hyperspectral XRF imaging, the sample was returned by the UR5e robot to the N9 robot for\nfilm conductance measurements. Four-point probe conductance measurements were\nperformed with a Keithley Series K2636B System Source Meter instrument connected\nto a Signatone four-point probe head (part number SP4-40045TBN; 0.040-inch tip\nspacing, 45 g pressure, and tungsten carbide tips with 0.010-inch radii) by a Signatone\ntriax to BNC feedthrough panel (part number TXBA-M160-M). The source current\nwas stepped from 0 to 1 mA in 0.2 mA steps. After each current step, the source meter\nwas stabilized for 0.1 s and the voltage across the inner probes was then averaged for\nthree cycles of the 60 Hz power line (i.e., for 0.05 s) and recorded. Conductance measurements were made on the same 97 points of interest as analyzed in the XRF\ndata, as defined in Supplementary Fig. 3.\nThe film conductivity was calculated using a custom data analysis pipeline\nimplemented in Python using the open-source Luigi framework66. This pipeline\ncombined conductance data and XRF data to estimate the film conductivity at each\nof the 97 points of interest on the sample.\nFor each set of current–voltage measurements at each position on each sample,\nthe RANSAC robust linear fitting algorithm67 was used to extract the conductance\n(dI/dV). The voltage compliance limit of the K2636B was set to 10 V and voltage\nmeasurements greater than 10 V were therefore considered to have saturated the\nSource Meter instrument and automatically discarded by the data analysis pipeline.\nThe conductivity of the thin films was then calculated by combining the 4-\npoint-probe conductance data with the film thicknesses estimated by XRF:\nσ ¼ ln2\nπ ´ dI\ndV ´ t\n1\n; ð1Þ\nwhere dI/dV is the conductance from the 4-point-probe measurement, t is\nestimated film thickness from the XRF measurements, and σ is conductivity.\nDue to the poor morphology of the drop-cast films, a robust conductivity\nestimation scheme was employed. First, conductance data was excluded for any\nmeasurement positions with zero conductance. Next, outliers were excluded from\nthe remaining conductance data using a kernel density exclusion method (see\nbelow). Outliers were also excluded from the XRF film thickness estimates using\nthe same exclusion method. Conductivities were calculated for each position on the\nsample for which neither conductance nor XRF data was excluded. The mean of\nthese conductivities was returned to the optimizer (see below). In cases where all\npoints were discarded, a mean conductivity of 0 was reported.\nThe outlier kernel density exclusion method was performed by calculating\nGaussian kernel density estimates for the conductance and XRF data, normalizing\nthe density between 0 and 1, and rejecting data points with a kernel density below\n0.3. Bandwidths of 5 × 10−3 μΩ−1 m−1 and 5 × 103 cps were used for the\nconductance and XRF data, respectively.\nAutonomous workflow step 8: algorithmic experiment planning. The experiment parameters for each optimization experiment performed on the autonomous\nlaboratory were determined by the qEHVI55 multiobjective Bayesian optimization\nalgorithm. In brief, this algorithm proposes experiments expected to increase the\narea underneath the Pareto front by the largest amount. More formally, the\nalgorithm proposes a batch of q experiments (q = 1 here, but q could be increased\nto exploit parallelized experimentation), which are collectively expected to increase\nthe hypervolume between the Pareto front and a reference point by the largest\namount. The hypervolume is a generalization of volume to an arbitrary number of\ndimensions; this generalization supports optimization with more than two objectives. The reference point must be specified prior to the optimization and specifies a\nminimum value of interest for each objective.\nThe algorithm involves two major conceptual steps: modeling the objectives\nfrom data and proposing the next experiment. In the configuration used here, each\nobjective is assumed to be independent and is modeled with an independent\ngaussian process (see Supplementary Figs. 5 and 6). Based on the models for each\nobjective, the expectation value of the hypervolume improvement associated with\nany candidate experiment can be computed; the candidate experiment with the\nlargest expected hypervolume improvement is selected. We ran the qEHVI\nalgorithm using the implementation available in the open-source BoTorch\nBayesian optimization library68,69. We used a temperature reference point at the\nupper limit of the experiment (280 °C) so that any outcome with a processing\ntemperature below this upper limit would be targeted. We used a dynamic\nconductivity reference point set to 5% of the running observed maximum\nconductivity. This dynamic reference point ensured that the optimization would\nidentify Pareto-optimal outcomes over a wide range of conductivity values and did\nnot require prior knowledge of the scale of conductivity values expected. We used\nheteroskedastic Gaussian processes to model both the conductivity and the\ntemperature68. We assigned each conductivity point an uncertainty equal to 20% of\nits value, which is comparable to the repeatability of the experiment\n(Supplementary Fig. 13). Zero uncertainty was assigned to the temperature values,\nwhich were manipulated rather than responding variables and were trivial\nto model.\nCalibration of XRF signal against reference samples. To enable palladium film\nthickness to be estimated from the XRF signal, a calibration procedure was performed on sputtered palladium reference samples having four different nominal\nthicknesses (10, 50, 100, and 250 nm). These samples were characterized by profilometry and XRF. A linear relationship between the film thickness and the XRF\ncounts was observed (see Supplementary Fig. 2). This relationship was used to\nestimate the thickness of each sample from the XRF data.\nThe reference samples were sputtered onto clean glass microscope slides (see\ncleaning procedure above) using a Univex 250 sputter deposition system with a DC\nmagnetron source at 100 W and an argon working pressure of 5 × 10−6 bar. The\ndeposition chamber base pressure is 5 × 10−9 bar. Films were deposited after 1 min\nof pre-sputtering. The substrate holder rotated at 10 rpm. Nominal film thickness\nwas monitored using a quartz crystal microbalance mounted in the sputter\nchamber. A 2-inch diameter palladium sputter target was used (99.99%, ACI\nAlloys). Step edges for profilometry were obtained by placing strips of Kapton™\ntape onto the substrates prior to sputtering and removing these after sputtering.\nThe substrates were rinsed with acetone and IPA to remove any Kapton™ tape\nresidue prior to performing profilometry.\nProfilometry was performed on the reference samples using a Bruker DektakXT\nstylus profilometer. XRF was performed on the reference samples using the same\nsettings used for the drop-casted samples during the optimization campaigns (see\nabove).\nDeposition and characterization of spray-coated samples. The spray coater was\nbuilt from an ultrasonic nozzle (Microspray, USA) mounted to a custom motorized\nXYZ gantry system (Zaber Technologies Inc., Canada) above a hot plate (PC-420D,\nCorning, USA). Precursor ink was fed to the nozzle by a syringe pump (cavro\ncentris pump PN: 30098790-B, Tecan Trading AG, Switzerland). The ultrasonic\nspray nozzle was operated at 3 W and 120 kHz. For each recipe, a total of 700 µL of\nprecursor was sprayed onto a glass substrate (75 mm × 25 mm × 1 mm microscope\nslides; VWR catalog no. 16004-430) placed on a custom aluminum fixture\nmounted to the hotplate. Approximate substrate temperatures were measured\nusing a thermocouple attached to a glass substrate with thermal cement. This\ninstrumented substrate was placed at a position on the hotplate fixture symmetrical\nto the position where the substrates to be coated were placed. The hotplate power\nwas adjusted until the steady-state temperature of the instrumented substrate was\nwithin 4 °C of the desired temperature before spray coating each of the recipes\nreported here. To achieve consistent thermal contact between the substrates and\nthe hotplate fixture, both the instrumented substrate and substrate to be coated\nwere affixed to the hotplate with thermal paste (TG-7, Thermaltake Technology\nCo., Taiwan). The spray coater nozzle speed was 5.1 mm s−1, the nozzle-tosubstrate distance was 15 mm, the spray flow rate was 2 µL s−1, and the carrier gas\nflow rate was 7 L min−1. When spraying, the nozzle moved in a serpentine pattern\nconsisting of twelve 50 mm lines with 25 mm spacing (see an illustration of the\npattern in Supplementary Fig. 14). The coating on each sample was produced by\nrepeating this spray pattern three times with no delay between passes. After spray\ncoating, the samples were left to anneal on the hot plate for 5 min.\nThe spray-coated palladium films were characterized at 26 locations on a 2 × 13\ngrid within an 8 × 20 mm region of interest at the center of the film (see\nSupplementary Fig. 14). The amount of palladium at each location was measured\nusing the XRF microscope and converted to a film thickness estimate by applying\nthe same calibration method used for the drop-cast films. The film conductance at\neach location was measured using the 4-point probe system on the robot described\nabove. The film conductivity was calculated at each of the 26 measurement\nlocations by combining the 4-point probe conductance and XRF film thickness\nvalues for that location using Eq. (1). The mean and standard deviations of the 26\nresulting thickness and conductivity values are reported for each film in\nSupplementary Table 3.\nComputer simulations of optimization algorithm performance. Computer\nsimulations were used to study the performance of the qEHVI algorithm for\noptimizing the combustion synthesis experiments. A model of the experimental\nresponse surface was built from the experimental data. Experimental optimizations\nwere then simulated by sampling the model using grid search, random search,\nSobol sampling, and the qEHVI and qParEGO algorithms. Optimization performance was quantified with and without simulated experimental noise. The performance of qEHVI relative to random sampling was quantified using the\nacceleration factor (AF) and enhancement factor (EF) metrics59. These simulation\nprocedures are described in more detail below.\nModels of the experimental response surface and noise. Gaussian process\nregression was used to create a model of the experimental response surface using\nthe combined data from all four optimization campaigns. This model predicts the\nexperimental outputs (i.e., annealing temperature and conductivity) from the\nexperimental inputs (i.e., fuel-to-oxidizer ratio, fuel blend, total concentration, and\nannealing temperature). Our model is composed of two separate Gaussian processes, as implemented by the scikit-learn Python package67. Each Gaussian process is regressed on a single experimental output (i.e., either temperature or\nconductivity) and all four of the experimental inputs. The kernels used for the\nconductivity (kcond) and temperature (ktemp) models are:\nkcond x; x0 ð Þ ¼ klin x; x0 ð Þ ´ kSE x; x0 ð Þ þ knoise x; x0 ð Þ; ð2Þ\nktemp x; x0 ð Þ¼ klin x; x0 ð Þ ´ kSE x; x0 ð Þ\n; ð3Þ\nknoise x; x0 ð Þ¼ noise if x ¼ x0\nelse 0; ð4Þ\nwhere klin is a constant kernel, kSE is a squared exponential kernel, and knoise is a\nwhite noise kernel, and * indicates that the lengthscale of the kernel is fixed to 1.\nThe four types of input data and two types of output data were each normalized\nprior to training the model. To simplify the optimization to be strictly a\nmaximization problem, the temperature values (which must be minimized) were\nmultiplied by negative one. The leave-one-out cross-validation residuals (LOOCV; Supplementary Figs. 4, 9, and 10 and Supplementary Table 4) are comparable to\nthe measured experimental uncertainties (Supplementary Table 1). We also plotted\nthe LOOCV residuals as a function of each input (Supplementary Fig. 6), each\nmodeled output (Supplementary Fig. 7), and sampling order (Supplementary\nFig. 8) and observed that the distribution of the residuals was largely random.\nFor the simulations with no noise, the model posterior means were used directly\nto represent the experiment. For simulations with experimental noise, noisy\nconductivity values were simulated by randomly sampling a modified\nMaxwell–Boltzmann distribution. First, the Maxwell–Boltzmann distribution was\nflipped across the y-axis by negating the x term. Second, the mean of this\nMaxwell–Boltzmann distribution was set to the noiseless model posterior mean.\nFinally, the variance of the Maxwell–Boltzmann distribution was set to be equal to\nthe noise level (or variance) of the white noise kernel. We chose to employ\nMaxwell–Boltzmann noise to model the experimental noise because of the\ntendency of drop-casted samples to exhibit a wide range of downwards deviations\nin the apparent conductivity due to the poor sample morphology.\nThe Maxwell–Boltzmann probability density function is\nPBð Þ¼ x\nffiffiffi\nπ\n2\nr x2exp x2\n2a2\n \u0004\na3 ; ð5Þ\nwhere a is the distribution parameter. Since we set the variance of\nMaxwell–Boltzmann distribution (σ2\nB) equal to the white noise kernel noise level\n(σ2\nnoise)\nσ2\nnoise ¼ σ2\nB ¼ a2ð3π  8Þ\nπ : ð6Þ\nWe computed the distribution parameter for the Maxwell–Boltzmanndistributed simulated experimental noise as:\na ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffi\nπσnoise\n3π  8\nr\n: ð7Þ\nIf subtracting the Maxwell–Boltzmann noise from the posterior mean resulted\nin a value less than zero, the noisy model value was set to zero.\nSampling strategies. To compare the performance of closed-loop and open-loop\napproaches, several sampling strategies (grid search, random search, Sobol sampling, and the qEHVI and qParEGO algorithms) were used to sample both the\nnoise-free and noisy experimental models. Random sampling was performed by\ngenerating samples from a uniform distribution across the entire normalized input\nspace. Sobol sampling was performed with a scrambling technique such that each\nSobol sequence is unique to yield a statistically meaningful distribution of\noptimizations70. qEHVI and qParEGO are initiated with ten scrambled Sobol\npoints. The qEHVI algorithm was configured as it was for the physical experiments\ndetailed above. The qParEGO algorithm was configured using the default settings,\nexcept for the reference point which was configured in the same way done for\nqEHVI. The complete benchmarking results are shown in Supplementary Fig. 11.\nSimulated optimization campaigns. The performance of each sampling strategy\n(grid, random, Sobol, qParEGO, and qEHVI) was determined both with and\nwithout experimental noise. Each simulated optimization campaign was performed\nfor 1000 replicates and 100 experimental iterations, except for random which was\nperformed for 100,000 iterations for use in the acceleration calculations (shown in\nFig. 4b). If an optimization algorithm produced an error during optimization, then\nthat replicate was removed and repeated.\nPareto front and hypervolume. The Pareto front is defined by the set of samples\nfor which no other sample simultaneously improves all the objectives. When\nassessing the performance of a simulation, the hypervolume was computed using\nthe least desirable value of each objective function (the nadir objective vector) as\nthe reference point (i.e., zero conductivity and 280 °C temperature). This reference\npoint was held constant for evaluating all of the simulated optimization campaigns.\nWe assessed the performance of the noisy optimizations such that the only difference between the noisy and noise-free optimizations was the information provided to the optimization algorithm. To perform this assessment, we followed the\nprocedure described by Bakshy and coworkers58 wherein the hypervolume for\noptimizations using the noisy experimental model were calculated from the\nequivalent point from the noiseless model. Since each objective of the model is\nnormalized to the range [0, 1], the hypervolume of the model is in the range [0, 1].\nFor each simulated optimization campaign, the normalized hypervolume was\ncalculated at each iteration. When calculating acceleration and enhancement factors, each of the 1000 simulated qEHVI campaigns was compared to each of the\n1000 simulated random campaigns, resulting in 1,000,000 comparisons.\nCalculation of acceleration and enhancement factor. The acceleration factor\nquantifies how much faster one sampling technique is than another (Eq. 5). For\nexample, if sampling technique B requires 40 samples to reach the performance\nattained by technique A after 20 samples, the acceleration factor of A relative to B\nat 20 samples is 2.\nAFA:B na\n \u0004 ¼ nb\nna\n;\ns:t: PB nb\n \u0004 ≥ PA na\n \u0004; min nb; ð8Þ\nwhere AFA:B na\n \u0004 is the acceleration of technique A with respect to B at na samples,\nand PiðnÞ is the performance of technique i at n samples. Note that it is possible for\nsampling technique A to outperform B such that there exists no value of nb where\nPB ≥ PA. In these cases, more samples with technique B are required to make the\ncomparison, otherwise AFA:B is not calculable. If AFA:B is not calculable, then a\nlower bound acceleration factor is calculated by assuming that the slow sampling\ntechnique would beat the fast sampling technique if it observed one more sample.\nThe acceleration factor in Fig. 4b was reported until these lower bound estimates\ncompose more than 25% of all of the acceleration comparisons.\nThe enhancement factor of one sampling technique with respect to another for\na given number of samples is defined as the ratio of their performance values for\nthe same number of observations (Eq. 6). For example, if sampling technique A\nreaches a performance value of 7 after 20 samples, and technique B reaches a\nperformance value of 2 after 20 samples, the enhancement of technique A is 3.5 at\n20 samples.\nEFA:Bð Þ¼ n PAðnÞ\nPBðnÞ\n; ð9Þ\nwhere EFA:BðnÞ is the acceleration factor of sampling technique A with respect to B\nafter n samples. When PAð Þ¼ n 0 and PBð Þ¼ n 0, then EFA:Bð Þ¼ n 1. When\nPAð Þ n > 0 and PBð Þ n > 0, then EFA:Bð Þ n is not calculable. To compare the AF and EF\nfrom the repeated simulations, the median, geometric mean and interquartile range\nwere calculated."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Image formation in ptychography. The approach described in this paper is compatible with multisclice ptychography, but in light of the application to a 2D material we constrain ourselves to single-slice ptychography. Here, ptychography can be expressed by a multiplicative approximation that describes the interaction of a wavefunction ψ^in_p(r) of an incoming beam with the transmission function t(r) of a specimen. For each measurement p, the beam is shifted by R_p and a diffraction pattern is acquired with the intensity I_p that is expressed by\n\nI_p = |Ψ^ex_p(k)|² = |F[ψ^in_p(r − R_p)t(r)]|²,\n\nwhere F is the Fourier propagator, r the real space coordinate, k the reciprocal space coordinate and Ψ^ex_p(k) the exit wavefunction at the detector. The transmission function can be defined as t(r) = e^(iσV(r)), with the interaction constant σ and the complex potential V(r). Throughout this treatment, σ is absorbed into V(r). X-ray and optical ptychography is mathematically described similarly with the only difference that the transmission function t(r) is related to the complex refractive index of the specimen. Figure 1a illustrates the experimental configuration of conventional ptychography. The potential of the specimen is recovered from data of experimentally acquired diffraction patterns J_p using a reconstruction algorithm. Here, we apply a gradient based algorithm with a gradient decent optimization and the potential is retrieved by iteratively minimizing the loss function\n\nL(V) = (1/P)∑^P_(p=1) ||I_p(V) − J_p||²_2.\n\nGeneration of scan sequences. We use a recurrent neural network (RNN) for the generation of scan sequences. Its network architecture is designed to model temporal sequences with recurring input information. Memory cells combine the current input information X_t with the hidden state H_t and map it to the next hidden state H_(t+1). These hidden states represent the memory gathered from all the previous time steps. Gated recurrent units (GRU)s, which allow a computationally fast mapping with a high performance, are used in this work. At every time step t, an output is generated on the basis of the current hidden state. In the implementation shown here, the output corresponds to multiple scan positions, i.e. a sub-sequence of scan positions, given by a vector of 2D coordinates R̃^P_t. In principle, the output can be reduced to a single scan position R^p_t, but we do not do so for practical reasons that involve a reduced training performance of the network and also a greatly increased acquisition time due to, e.g., more frequent data transfer and pre-processing of these intermediate data chunks. The sub-sequence is predicted via a fully connected layer (FC) that is parameterized by the layer weights θ_H:\n\nR̃^P_t = FC_(θ_H)(H_t).\n\nAt the predicted scan positions R̃^P_t, diffraction patterns J^P_t are acquired by the microscope and from these diffraction patterns a potential V_t(r) is reconstructed minimizing Eq. (2). The intermediate reconstruction V_t(r) combined with its corresponding sub-sequence of scan positions R̃^P_t can then be used for the input information X_t of the RNN. However, the bandwidth of the information given in V_t(r) and R̃^P_t differs strongly and thus pre-processing is required before the two components can be concatenated and mapped to X_t. For the processed location information L_t based on the sub-sequence R̃^P_t, a FC that is parameterized by the weights θ_R is used:\n\nL_t = FC_(θ_R)(R̃^P_t).\n\nFor the processed structure information C_t based on the reconstructed potential V_t(r), a compressed representation z_t is generated by using the encoder part of a convolutional autoencoder. This processing step is shown in Fig. 1b and the training of the convolutional autoencoder is described in the Supplementary Information. The compressed representation z_t is then fed into a FC that is parameterized by the weights θ_z:\n\nC_t = FC_(θ_z)(z_t).\n\nThe processed location information L_t is subsequently concatenated with the processed structure information C_t and mapped to the input information X_t with a FC that is parameterized by the weights θ_LC. The whole process of predicting sub-sequences of scan positions and acquiring the corresponding diffraction patterns is repeated until a ptychographic data set of desired size is reached. Finally, backpropagation through time (BPTT) is used to generate the required gradients to update the network weights θ = {θ_H, θ_GRU, θ_LC, θ_R, θ_z} of the RNN. Figure 1c shows the prediction process modeled by the RNN in full detail.\n\nTraining through reinforcement learning. A RNN can be combined with RL to provide a formalism for modelling behaviour to solve decision making problems. In the case of adaptive scanning in ptychography, where we seek to predict multiple scan positions at each time step, RL can be formalized by a partially observable stochastic game (POSG) that is described by a 8-tuple, ⟨M, S, {A_m}_(m∈M), ρ, {r_m}_(m∈M), {O_m}_(m∈M), ω, γ⟩, with multiple agents M. At each time-step t an agent m selects an action a^m_t ∈ A_m and makes an observation o^m_t ∈ O_m given the state s_t ∈ S. Thus, joint actions a_t = ⟨a¹_t, ..., a^m_t⟩ from the joint action space A = A_1 × ··· × A_M are executed and joint observations o_t = ⟨o¹_t, ..., o^m_t⟩ from the joint observation space O = O_1 × ··· × O_M are received from the environment at every time step. The next state s_(t+1) is generated according to a transition function ρ : S × A × S → [0, 1], the observations o_(t+1), containing incomplete information about the state s_(t+1), are generated according to an observation function ω : A × S × O → [0, 1] and each agent receives its immediate reward defined by the reward function r_m : S × A → R. This reward r_m contributes to the total reward computed at the end of the sequence, G_m = ∑^T_(t=0) γ^t r_m(a_t, s_t), also known as the return. The discount factor γ ∈ [0, 1] controls the emphasis of long-term rewards versus short-term rewards. The entire history of observations and actions up to the current time h_t = {o_1, a_1, ..., o_(t−1), a_(t−1), o_t} is used as basis for optimal or near-optimal decision making. A stochastic policy π^m_θ(a^m_t|h_t) maps the history of past interactions h_t to action probabilities. Given a continuous action space, the policy can be represented by a two-dimensional Gaussian probability distribution:\n\nπ^m_θ(a^m_t|h_t) = N(μ^m_θ(h_t), Σ),\n\nwith its mean vector μ^m_θ(h_t) corresponding to R^p_t, where the history h_t is summarized in the hidden state H_t of the RNN and the covariance matrix Σ with fixed variances σ²_x ∈ [0, 1] and σ²_y ∈ [0, 1]. The joint policy of all agents is then defined as π_θ(a_t|h_t) = ∏^M_(m=1) π^m_θ(a^m_t|h_t), with θ = {θ_m}_(m∈M). The goal of RL is now to learn a joint policy that maximizes the expected total reward for each agent m with respect to its parameters θ_m:\n\nJ_m(θ) = E_(π_θ(τ))[G_m] ≈ (1/N)∑^N_(n=1) G^m_n,\n\nwhere the expected total reward can be approximated by Monte Carlo sampling with N samples. In this paper, improvement of the policy is achieved by updating the policy parameters θ_m = {θ^m_H, θ_GRU, θ_LC, θ_R, θ_z} with 'REINFORCE', a policy gradient method:\n\n∇_(θ_m) J_m(θ) = E_(π_θ(τ))[∇_(θ_m) logπ_θ(τ)G_m] ≈ (1/N)∑^N_(n=1) ∑^T_(t=0) ∇_(θ_m) logπ^m_θ(a^m_(n,t)|h_(n,t))G^m_n.\n\nThe derivation of ∇_(θ_m) J_m(θ) is given in the Supplementary Information.\n\nLearning to adaptively scan in ptychography. While policy gradient methods are the preferred choice to solve reinforcement learning problems in which the action spaces are continuous, they come with significant problems. Like any gradient based method, policy gradient solutions mainly converge to local, not global, optima. In this paper, we reduce the effect of this problem during training by splitting the training of the RNN into supervised learning and RL. While training in RL can be performed with a policy whose parameters are arbitrarily initialized, this is not ideal. Having an adequate initial guess of the policy and using RL subsequently to only fine-tune the policy is a much easier problem to solve. A sparse grid scan sequence is a reasonable initialization as it follows the current scanning convention used in a microscope. Pre-training of the parameterized policy for the RL model can then be performed by supervised learning applied on the RNN such that the discrepancy between the predicted scan positions R̃^P_t = μ_θ(h_t) and the scan positions of the initialization sequence R^init,P_t is minimized:\n\nK(θ) = ∑^T_t ||R̃^P_t − R^init,P_t||²_2.\n\nFigure 5 illustrates the scan positions during the fine-tuning of the policy through RL for the first 10,000 iterations when either (a) a policy that has not been initialized via supervised learning or (b) an initialized policy is used. While the scan positions in both cases converge to the atomic structure, the positions predicted by the non-initialized policy are distributed only within a small region of the field of view during the entire training. Note that once the predicted scan sequence mimics the sparse grid scan sequence as a result of the supervised learning based initialization, all further improvements in performance are the result of the subsequent training through RL.\n\nA high variance of gradient estimates is another problem that particularly strongly affects the Monte Carlo policy gradient method. Due to this, the sampling efficiency is relatively low, which causes a slow convergence to a solution. This makes deep RL applied to ptychography challenging as the image reconstruction itself requires iterative processing. The high variance can be in part attributed to the difficulty of assigning credit from the overall performance to an individual agent's action. Here, we introduce a way to estimate the reward function in order to tackle the credit assignment problem for adaptive scanning in ptychography. The reward function should naturally correspond to the quality of the ptychographic reconstruction. We have found empirically that a high reconstruction quality correlates positively with a high dynamic range in the phase. Therefore, the reward function could intuitively be formalized by r_m(a_t|s_t) = P⁻¹∑_(r∈FOV) V(r), where P is the total number of scan positions. This formulation, however, does not solve the credit assignment problem and results in an insufficient training performance, as shown in Fig. 6a. To estimate the reward for the actions of each individual agent, we use a tessellation method that partitions the atomic potential into small segments. A Voronoi diagram, where each position corresponds to a seed for one Voronoi cell, enables assignment of only a part of the total phase to each position. More precisely, the Voronoi diagram formed by the predicted scan positions is overlaid with the corresponding ptychographic reconstruction at the end of the prediction process and the summed phase within each Voronoi cell is the reward for that cell's seed position. The reward function can be expressed by r_m(a_t|s_t) = P⁻¹∑_(r∈Cell_m) V(r). Figure 6b shows a Voronoi diagram generated by predicted scan positions.\n\nSettings. For the experimental investigation, we acquired multiple ptychographic data sets from a monolayer molybdenum disulfide (MoS₂) specimen with a Nion HERMES microscope. The microscope was operated with a 60 kV acceleration voltage, a convergence angle of 33 mrad and diffraction patterns with a pixel size of 0.84 mrad were acquired using a Dectris ELA direct-electron detector mounted at the electron energy-loss spectroscopy (EELS) camera port. Distortions induced by the EEL spectrometer were corrected with in-house developed software. For the ptychographic data set acquisition, a conventional grid scan with a scanning step size of 0.02 nm was used. From the experimentally acquired data sets we created 200 smaller data sets, each with 10,000 diffraction patterns and located at different regions of the sample. 175 of these small data sets were dedicated to the training of the network model, while the remaining 25 data sets were used to test the trained model and to generate the results shown here. The diffraction patterns were binned by a factor of 2 to 64 × 64 pixels. The adaptive scanning model was trained on the small data sets with the goal of predicting optimal scan sequences of 250 to 500 probe positions, out of the possible 10,000, which corresponds to a dose reduction by a factor of 40 to 20. Each sub-sequence contains 50 to 100 positions, where the initially given first sub-sequence follows a sparse grid scanning sequence.\n\nWe conducted a second investigation of the model's performance on simulated data sets of a DWCNT and compared it to the performance of alternative low-dose data acquisition methods in ptychography. The data sets were generated using the simulation tool abTEM where we set the acceleration voltage to 60 kV, the convergence angle to 40 mrad and the scanning step size to 0.02 nm. The size of the diffraction patterns is 86 × 86 pixels with a pixel size of 0.91 mrad. 962 data sets have been used to train the network model and from the 13,225 diffraction patterns in each data set only 840 were chosen within the prediction process of the adaptive scanning workflow. 25 data sets were used to compare the different scanning procedures and analyse their performance using the Q_SSIM metric. The simulated DWCNT consisted of an inner and an outer nanotube with a diameter of 9.78 Å and 16.44 Å and a chiral angle of 44° and 60°, respectively. For each data set, a unique rotation between the two nanotubes and a translation of the entire DWCNT within the field of view was applied.\n\nThe ptychographic reconstructions were performed with an optimized version of ROP that allows simultaneous reconstruction from a batch of different data sets and therefore the parallel hardware architecture of a NVIDIA V100 GPU could be optimally used to efficiently train the model. For a batch size of 24, reconstructions were retrieved in about 35 s. A gradient descent step size α_ROP of 525 was chosen and the potential was retrieved at iteration 5. In the experimental investigation, the reconstructed potential was 200 × 200 pixels with a pixel size of 0.0154 nm, for a field of view of 2 × 2 nm, while for the simulation, the reconstructed potential was 200 × 200 pixels with a pixel size of 0.0140 nm, for a field of view of 2.3 × 2.3 nm. For the generation of the reward function, Voronoi diagrams were generated with the Jump Flooding Algorithm and for the implementation of the network models, PyTorch was used. For the compression of structure information, we used a convolutional autoencoder consisting of 6 convolutional layers with kernels of dimension 3, a stride of 1 and channels that ranged from 16 to 512 for the encoder and decoder part, respectively. The input of the autoencoder had a dimension of 512 with a pixel size of 0.0064 nm and thus a scaling and an interpolation was required before the potential generated by ROP could be compressed. In addition, the value of the potential V_i at each pixel i was transformed to zero mean and unit variance. For the prediction of the scan sequences, pre-training and fine-tuning was performed with a RNN model composed of 2 stacked GRU layers with hidden states H_t of size 2048, the Adam optimizer with a learning rate α_RNN of 1e−5 and a batch size of 24. For the fine-tuning, a policy with variances of σ²_x = σ²_y = 0.0125² was chosen and a myopic behavior was enforced by setting the discount factor for the return, G, to γ = 0. In the case of the experimental investigation, training of the autoencoder involved 100,000 iterations, while the training of the RNN with supervised learning and RL has been performed with 800 and 20,000 iterations, respectively. In the second investigation with the simulated data, the training of the autoencoder has been done with 30,000 iterations and for the training of the RNN with supervised learning and RL, 200 and 2800 iterations were used, respectively."
        }
    ],
    "Data availability": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "The numerical data used for this work are publicly available at https://github.com/saugatkandel/fast_smart_scanning[cite: 3580]. The raw experimental data is publicly available at https://doi.org/10.5281/zenodo.7939730[cite: 3581]. Source data are provided with this paper[cite: 3583]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "All datasets used and/or analysed during the current study available from the corresponding author on reasonable request."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "The raw and processed data generated by the self-driving laboratory in this study is\navailable at https://github.com/berlinguette/ada. All other data related to this paper is\navailable from the corresponding author upon request."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "The code is open sourced and can be found at https://github.com/EvgMar/X_ray_Al.git. Data underlying the results presented in this paper are not publicly available at this time but may be obtained from the authors upon reasonable request."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "The codes used in this paper are available on GitHub repository [https://github.com/jnousi/PO4AO.git]. The code is documented and annotated to help readers understand the methodology and reproduce the results. We encourage readers to use the data and codes for their own research and to cite this paper as the source of the data."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "The source data generated in this study have been deposited in the repository \"SDL\" (https://github.com/AbolhasaniLab/SDL)."
        }
    ],
    "Code availability": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "The FAST software and the code for the numerical simulations are publicly available at https://github.com/saugatkandel/fast_smart_scanning[cite: 3585]. The code used to analyze the experimental data is available at https://doi.org/10.5281/zenodo.7942774[cite: 3586]."
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "All code used in this study was based on open-source Python packages listed in the\nsupplementary information."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "The source code of the presented work is available at Gitlab and the experimental and simulated data can be found in Ref. 28. All other code is available from the corresponding author on reasonable request."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "The source code for the noise benchmarking plots and surrogate models have been deposited in the repository \"SDL\" (https://github.com/AbolhasaniLab/SDL)."
        }
    ],
    "A 1.1": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Figure 1: Artist's representation of the autonomous dark-field scanning microscopy experiment at the Advanced Photon Source (APS). The APS synchrotron produces a coherent X-ray beam focused on a WSe2 film on a Si substrate, generating diffraction patterns collected by a 2D detector [cite: 3219-3221]. The beam position and detector acquisition are autonomously controlled by the FAST AI-based workflow[cite: 3223]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Figure 1: Experimental setup of the laser and measurement systems. AC autocorrelator, OSC oscilloscope trace, OSA optical spectrum analyser, BTF bandwidth tunable filter. [cite: 5541, 5542]"
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 1: A Pareto front diagram illustrating the concept of multiobjective optimization. The graph plots Objective B against Objective A. The blue curve represents the 'Pareto front of best possible materials,' while the stepped black line represents the 'Pareto front of best known materials.' [cite_start]A red arrow indicates that the self-driving laboratory, Ada, closes the gap between the suboptimal materials (gray area) and the theoretical limit [cite: 23, 44-52]."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Figure 1: Experimental setup. The laser pulse beam is tightly focused on the surface of the rotated Cu target, which leads to the generation of the X-ray. The sample is inserted inside the X-ray beam, and transmitted through the sample X-ray beam registered by the X-ray detector. In addition to the X-ray, the second harmonic (515 nm) is generated during laser-matter interaction. The second harmonic is collimated by the objective and transmitted to the spectrometer. The second harmonic is used as a feedback for the machine learning algorithm. Based on the signal, the Cu target location is changed by the motorized 5-axis stepper motor controlled by the driver."
        },
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Fig. 1. Data-driven microscopy workflow. (A) Pyramid of frustration: Trade-offs in acquisition parameters are visualised as a pyramid, highlighting the interdependence of signal-to-noise ratio (SNR), sample health, temporal resolution, spatial resolution, and the extend to the field of view, 3D volume and multiplexing (x, y, z, dimensions). Enhancing one parameter typically compromises at least one other; (B) Schematic of workflow: Acquisition control software: software-driven control of microscope hardware for image capture; Microscope hardware: imaging devices with programmatic interface: Custom reactive agent: Integration of a custom reactive agent that analyses live acquisition data, providing real-time feedback to the software to adapt the acquisition parameters; (C) Acquisition stages: Observation stage (Blue): The sample is continuously monitored using a simple imaging protocol, such as brightfield. This stage is non-invasive and preserves sample health; Reactive stage (Magenta): Upon detection of a target event (e.g., a cell entering a specific cell-cycle stage), the reactive agent initiates a fluorescence imaging protocol, enabling detailed observation of the event."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 1: The prior distribution, noiseless posterior distribution and noisy posterior distributions for a GP with covariance <f(x_i)f(x_j)> = M_{5/2}(|x_i - x_j|/l). For each distribution, we draw four random functions (colored lines). The black line represents the mean of each distribution, while the dark and light-shaded regions represent the 1sigma and 2sigma intervals, respectively."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 1: NN architectures. Both NN, the dynamics, and policy take input tensor concatenations of past actions and observations. They also share the same fully convolutional structure in the first three layers. At the output layer, the policy model includes the KL-filtering scheme (upper right corner), and the dynamics model output is multiplied with the WFS mask (lower right corner). For the GHOST, the input and output images are 24x24 pixels (set by the DM)."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 1: (a) Photograph of the KB-mirror system at the 34-ID-C beamline and (b) schematic of the four-bar bender design. Each mirror is equipped with four motors, as shown in Fig. 1b. Motors 1 and 2 control upstream and downstream bending forces of the four-bar bender, respectively, while motor 3 adjusts the incident angle, and motor 4 moves the mirror position in the direction normal to its surface."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Figure 1: Schematic of the adaptive scanning workflow with its three main components. (a) Experimental data acquisition in ptychography. At the scan position R_p of the scan sub-sequence R̃^P_t, the beam illuminates a sample, where the incident electron wave ψ^in_p(r − R_p) interacts with the transmission function t(r). The wave exiting the sample is propagated by a Fourier transform to the detector located in the far field and the intensity I_p = |Ψ^ex_p(k)|² is recorded as a diffraction pattern. (b) A reconstruction V generated from diffraction patterns of a scan sub-sequence is mapped to the compressed representation z by using an encoder network E_φ_e(V). (c) Schematic of the forward propagation process of the RNN model. The RNN consists of GRU units that use the hidden state H_t from the previous time step and the hybrid input information X_t to create a new hidden state H_(t+1). The hybrid input is the concatenation of the pre-processed information from the sub-sequence of scan positions R̃^P_t and the corresponding compressed representation of the partial reconstruction z_t. The output of the GRU cell is used to predict the positions of the next sub-sequence R̃^P_(t+1) and is also used as the input for the next GRU cell."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Figure 1: Autonomous materials discovery with the A-Lab. The schematic illustrates the full pipeline where air-stable unreported targets are identified using DFT-calculated convex hulls from the Materials Project and Google DeepMind[cite: 774]. [cite_start]Synthesis recipes are proposed via machine learning models trained on literature[cite: 775]. [cite_start]These recipes are executed by a robotic laboratory automating powder dosing (1), sample heating (2), and product characterization with XRD (3), with robotic arms handling all transfers[cite: 776, 777]. [cite_start]Phase purity is assessed by ML models and Rietveld refinement, with active learning proposing new recipes if the target yield is low[cite: 790, 791]."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 1: The evolution of automation in chemistry and materials science. This timeline illustrates the historical progression from manual experimentation to the modern era of self-driving laboratories (SDLs). The progression is divided into four stages: Manual (Pre-1990s), High-Throughput Experimentation (HTE) (1990s-2000s), Automated Synthesis & Characterization (2010s), and Self-Driving Laboratories (2020s-Present)[cite: 85, 86]. Key milestones are highlighted, such as the development of combinatorial chemistry, the introduction of robotic arms for sample handling, and the integration of AI for closed-loop optimization[cite: 87, 88]."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Figure 1: Key metrics for quantifying performance in SDLs. The metrics illustrated include degree of autonomy, operational lifetime, throughput, experimental precision, material usage, accessible parameter space, and optimization efficiency."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Fig. 1. Schematic illustration of the AI-guided robotic system for the inverse design of chiral functional films. (A) The workflow of the closed-loop research. (B) The layout of the robotic laboratory, including the mobile robot, liquid handling station, spin-coater, hotplate, and CD spectrometer."
        }
    ],
    "A 1.2": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Figure 2: The FAST workflow. (A) The cycle includes quasi-random initial measurements transferred to an edge device, initial sample estimation, computing candidate points, optimizing the path, and performing new measurements until a completion criterion is met[cite: 3274, 3275]. (B) Candidate computation involves examining the local neighborhood of unmeasured points to generate feature vectors, transforming them via RBF kernel, and using a neural network to predict Expected Reduction in Distortion (ERD) [cite: 3276-3288]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Figure 2: Examples of the three main pulsed regimes generated in the studied laser cavity. (d) Map of the average power of the output radiation; Autocorrelation function, optical spectrum and oscilloscope trace of (a) soliton molecule (b) partly coherent multi-pulses (c) single pulses. [cite: 5639-5641]"
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 2a: Schematic of the Ada self-driving laboratory. It shows the physical layout containing two robots: a 4-axis robot (N9) for film synthesis and characterization, and a larger 6-axis robot (UR5e) for sample transfer. [cite_start]The workspace includes an annealing oven, solution mixing area, dropcasting station, substrate storage, and an XRF microscope [cite: 109-114]."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Figure 2: Schematic representation of the algorithm. The agent receives information about environment (feedback and current coordinate) and reward. Based on the neural network (2 layers of 1024 neurons) it chooses one action: move left, move right or wait. The environment changes and the loop starts again."
        },
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Fig. 2. Overview of machine learning concepts for microscopy image analysis. (A) Schematic depicting that deep learning is a subset of machine learning, which is in turn a subset of artificial intelligence. (B) Schematic of major steps on training and using a machine learning model. First, data is collected, pre-processed and split into training, validation and test datasets. Models are trained on the training dataset and training is evaluated with the validation set to prevent overfitting. Once trained, a quality control of the model is done using an independent test dataset and if positive, the model can be used to generate predictions on new unseen data."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 2: An example of an iteration of a Bayesian optimization algorithm trying to maximize the negated Himmelblau function f(x_1, x_2) = -(x_1^2 + x_2 - 11)^2 - (x_1 + x_2^2 - 7)^2 whose true global optima are marked as white circles. Using existing data points and the assumption that the function is distributed as a GP, we can use Bayesian inference to compute a posterior consisting of a mean and error, upon which we can compute an acquisition function which informs us of the best points to sample. The black-edged diamonds superimposed on the acquisition function show the best eight points to sample, optimized in parallel and with the optimal routing represented by the red line."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 2: GHOST coronagraphic PSFs. (a) The PSF without any turbulence, and the DM set to be flat. (b) The PSF with simulated 1-stage systems residual phase screens played on SLM, and a flat DM. The speckle at around 1 o'clock is a ghost in the system."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 2: KB-mirror system at 28-ID-B beamline: upbound-reflective, vertically focusing bimorph mirror, with visible 18 electrode pairs for the local shaping actuators and the optical surface sandwiched between the electrodes (a), schematic for substrate dimensions and the piezo electrode channels (b); outward-reflecting, horizontally focusing bendable mirror (c) and detail of the flexure bender (d)."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Figure 2: Ptychographic reconstruction of a MoS₂ data set with a scanning procedure that follows (a) a sparse grid scan, (b) an adaptive scan and (c) the conventional grid scan. While only 250 diffraction patterns are used in (a) and (b), the full data set with 10,000 diffraction patterns is used in (c). The inset in (c) illustrates the illumination probe used for the reconstruction with an estimated size of 0.93Å. The pixel size is identical to the one used in the reconstruction and its magnitude is represented by the intensity, and the phase is represented by the HSV colorwheel. (d) FRC of the sparse grid scan and the adaptive scan averaged over 25 data sets and where the standard deviation is illustrated by the shaded area."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Figure 2: Outcomes from targeted syntheses of DFT-predicted materials. [cite_start]This chart summarizes the results for 58 new compounds plotted against their decomposition energies on a log scale[cite: 869]. [cite_start]Blue bars indicate the 41 successfully synthesized targets, while red bars indicate the 17 failures[cite: 870]. [cite_start]Diagonal lines mark targets optimized via active learning, while others were obtained using literature-based ML proposals[cite: 871, 872]. [cite_start]Inset pie charts display the fraction of successful targets (41/58) and successful recipes (130/355)[cite: 883]."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 2: Levels of autonomy in self-driving laboratories. This diagram defines a hierarchical classification system for laboratory autonomy, ranging from Level 0 to Level 5. Level 0 represents fully manual operations, while Level 1 introduces assisted automation for specific tasks[cite: 142, 143]. Level 2 involves batch automation, and Level 3 represents conditional autonomy where the system can execute complex workflows but requires human intervention for decision making[cite: 144, 145]. Level 4 is high autonomy, where the system operates largely independently with AI-driven decision making, and Level 5 is full autonomy, where the laboratory operates completely without human intervention, capable of self-repair and self-improvement[cite: 146, 147]."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Figure 2: Degrees of autonomy in SDLs. Illustration of the process workflows for (A) piecewise, where human users fully separate the experiment and computational system, (B) semi-closed-loop, where the algorithm and robotic components partially communicate, (C) closed-loop, where the human user has no influence in the goal seeking loop, and (D) self-motivated experimental systems, where the computational system dictates its own objectives."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Fig. 2. Performance of the AI-guided robotic system in the optimization of chiral films. (A) Evolution of the maximum g-factor as a function of the number of experiments. (B) Comparison of the maximum g-factor obtained by BO, random search, and human researchers. (C) Chemical structures of PDI-1, R-1, and S-1."
        }
    ],
    "A 1.3": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Figure 3: Numerical comparison of sampling methods. (A) Ground truth image. (B-D) Reconstructions at 10% scan coverage for Raster Grid (RG), Low-discrepancy random (LDR), and FAST methods. (G-I) The actual scan points corresponding to these reconstructions. (E, F) Evolution of Normalized Root Mean Square Error (NRMSE) and Structural Similarity metric (SSIM) as a function of scan coverage, showing FAST stabilizing at 27%[cite: 3339, 3340, 3352, 3353]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Figure 3: The variation of the rewards during training on the model of environment. [cite: 5681]"
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 2b: The autonomous experimental workflow steps. The process flows from mixing precursors -> dropcasting mixture -> imaging -> annealing -> performing XRF microscopy -> imaging -> measuring conductance -> calculating film conductivity -> planning the next experiment. [cite_start]Example images of the dropcasted precursor, XRF signal map, annealed sample, and conductance measurements are shown [cite: 115-119]."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Figure 3: Dependence of second harmonic intensity on X-ray photon flux at repetition frequencies of 2 MHz. The solid line shows square root dependence."
        },
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Fig. 3. Comparative analysis of machine learning algorithms in data-driven microscopy. Comparison of various machine learning (ML) algorithms employed in data-driven microscopy, delineating their respective advantages, limitations, and applications for image analysis tasks."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 3: (Upper left) The result of changing the positions of two coupled dimensions of the TES beamline. (Upper right) A quasi-random sample of 16 points from the ground truth. (Lower left) A non-latent GP fitted to the parameter space fitted to the sampled points. (Lower right) A latent GP fitted to the same points, which correctly infers the latent dimensions."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 3: PO4AO interface for RTC pipeline. COSMIC pipeline preprocesses the raw WFS data, projects it to DM-space with command matrix (using the modal basis matrices: S2M and V2M), then writes the \"delta volts\" to the shared memory buffer, and suspends the loop. Python interface (the green box) reads the shared memory buffer and passes the data to the PO4AO implementation. The PO4AO calculates the next command and saves the data (orange boxes), and the Python interface writes the command to shared memory, where COSMIC registers the command and passes it to the saturation management algorithm (SMA)/clipping stage."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 3: UML diagram of the software to drive the focusing system of 34-ID-C. The interface (34IDAbstractFocusingOptics) is implemented by both the hardware driver (right side of the diagram) and the digital twins (left side of the diagram, with different implementation corresponding to different simulation strategies)."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Figure 3: Ptychographic reconstructions of different MoS₂ data sets and with different scanning procedures. Reconstruction from 250 diffraction patterns of a data set that correspond to scan positions which follow (a–d) a sparse grid scanning sequence and (e–h) an adaptively predicted sequence. (i–l) Ground truth reconstruction of the full data set with 10,000 diffraction patterns shown with the scan positions used for the corresponding reconstructions (a–d) in green and (e–h) in red."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Figure 3: Active learning with pairwise reaction analysis. (a) [cite_start]The system analyzes failed attempts to determine occurred pairwise reactions[cite: 932]. (b) [cite_start]New precursors are suggested to substitute those involved in unfavorable reactions, avoiding redundant paths[cite: 933, 934]. (c) [cite_start]Successful precursor sets avoid unfavorable pairwise reactions[cite: 935]. (d) [cite_start]Free energy diagrams calculated from Materials Project data show the driving force at each reaction step[cite: 935, 936]. (e) [cite_start]A scatter plot depicts the efficiency of the active learning in identifying optimal paths or exhausting options relative to the search space size[cite: 937]."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 3: A system-level perspective of SDLs. This schematic illustrates the functional architecture of a self-driving laboratory, divided into the 'Brain' and the 'Body'. The 'Brain' (software) encompasses data management, AI/ML algorithms (e.g., Bayesian Optimization), and experimental planning modules[cite: 203, 204]. The 'Body' (hardware) consists of robotic platforms, synthesis modules, and characterization instruments[cite: 205]. The workflow forms a closed loop: the Brain plans experiments, the Body executes them, data is analyzed, and the loop repeats to optimize target properties[cite: 206, 207]."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Figure 3: Effect of noise on optimization efficiency. A Surface response plot of a two-dimensional michalewicz surrogate function, (B) median best response and (C) median mean squared error across ten replicates for a simulated optimization of a six-dimensional michalewicz surface with varying degrees of noise indicated by the legend. As the level of noise observed in the surrogate function is increased, the performance of the optimization algorithm decreases while the algorithm model's uncertainty increases. More precise experimental platforms, therefore, tend to generate higher performing self-driving laboratories. The optimization algorithm uses bagging regression with an exhaustive grid search hyperparameter tuned multi-layered perceptron and an upper confidence bounds decision policy. Noise is applied to the surrogate function by randomly sampling from a normal probability function with standard deviations of 0, 0.1, and 0.2 respectively and adding the sample to the surrogate output."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Fig. 3. Analysis of the experimental data. (A) Distribution of g-factors in the parameter space of PDI concentration and dopant ratio. (B) Distribution of g-factors in the parameter space of spin-coating speed and annealing temperature. The color scale represents the value of the g-factor."
        }
    ],
    "A 1.4": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Figure 4: Evolution of the FAST scan. (A, C, E) Reconstructions at 5%, 15%, and 20% coverage. (B, D, F) Corresponding actual measurement points. (G) Image obtained through a full-grid pointwise scan. (H) Points sampled specifically between 15% and 20% coverage[cite: 3419, 3420, 3439]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Figure 4: (a) Trajectories of the agent's movement, represented on a two-dimensional map of spaces of admissible filter values. The color of the map shows the values of the instant reward, which was calculated on the collected data at a current of 2.7 A. The black dotted lines show the agent trajectories obtained on the model of the environment. The colored lines show the trajectories obtained on the real laser system. (b) The value of the immediate reward depending on the step number of the agent for three trajectories. [cite: 5760-5763]"
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 3a: Input space maps for combustion synthesis conditions. Scatter plots show the relationship between fuel blend, total concentration, and annealing temperature against the fuel-to-oxidizer ratio. [cite_start]Colored points indicate experimental outcomes on the Pareto front, while gray points represent suboptimal materials [cite: 188-193]."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Figure 4: Results of the neural network performance in a sandbox. The black line represents the signal variation without the work of the neural network, the red line represents the signal variation with the working neural network, and the blue line represents the change in coordinates."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 4: A schematic of the TES (8-BM) beamline at NSLS-II. This representation shows the many optical components that make up modern beamlines, with each optical component having many degrees of freedom that must be optimized in concert in order to carry out experiments effectively."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 4: Learning curves for time delay experiments. The red lines correspond to the performance of PO4AO during each episode, and the blue lines are for the integrator. A single episode is 500 frames. The gray dashed line marks the end of the integrator warm-up for PO4AO. In all cases, the PO4AO outperforms the integrator all ready after the warm-up period. The training is done parallel to control, so the 10 episodes correspond to ~14 s in the figure."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 4: Screenshot of the Python code used to instantiate the focusing optics controller."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Figure 4: Ptychographic reconstruction of a DWCNT data set with a scanning procedure that follows (a) an adaptive scan, (b) a sparse grid scan, (c) an alternative low-dose scan and d) the conventional grid scan. 840 diffraction patterns have been used for the reconstructions in (a) and (b), 13,536 patterns were used for the reconstruction in (c) and the full data set consisting of 13,225 patterns was used for the reconstruction in (d). Half of the corresponding scan positions are illustrated on the right hand side of each reconstruction. Note that the alternative low-dose scan method used for generating (c) consists of a full grid scan at a very low dose and a consecutive scan using 311 scan positions that have been found to match the sample structure based on the result obtained from the previous scan. Only the latter scan positions are visualized in (c)."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Figure 4: Barriers to the synthesis of materials predicted to be stable. [cite_start]The 17 failed targets are categorized by specific synthesis complications[cite: 1001]. [cite_start]Barriers are divided into experimental (blue, 13 targets) and computational (green, 3 targets) categories[cite: 1003, 1004]. [cite_start]Four failure modes are illustrated schematically: slow reaction kinetics (showing driving force vs reaction coordinate), precursor volatility (phase diagram), product amorphization (free energy G vs temperature T), and computational limitations of DFT at 0 K (free energy G vs composition)[cite: 1005]."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 4: Data and AI agents in SDLs. This figure details the data flow and artificial intelligence components within an SDL. It highlights the role of Large Language Models (LLMs) and foundation models in parsing literature data to inform experimental priors[cite: 289, 290]. The diagram shows how active learning agents, specifically Bayesian Optimization, utilize acquisition functions to explore and exploit the chemical space[cite: 291, 292]. It also depicts the integration of simulation data (e.g., DFT calculations) to augment experimental datasets[cite: 293]."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Figure 4: Effect of surface complexity on optimization rate. Two-dimensional surface plots of the surrogate functions (A) Ackley, (B) Griewank, (C) Levy, and (D) Rastrigin and the median best response of ten optimization replicates across the four surrogates in (E) two-, (F) four-, and (G) six-dimensional parameter spaces. The optimization algorithm consists of gaussian processor regression with an upper confidence bounds decision policy."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Fig. 4. Characterization of the optimal chiral film. (A) CD and UV-vis absorption spectra. (B) AFM height image. (C) XRD pattern. (D) Proposed packing model of PDI molecules in the helical nanofibers."
        }
    ],
    "A 1.5": [
        {
            "paper_title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
            "source_filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
            "content": "Figure 5: Comparison of the per measured point center of mass (COM) of the diffraction patterns. (A, B) Inpainted CoMx and CoMy for the full-grid raster scan. (C, D) Inpainted CoMx and CoMy for the FAST scan at 20% coverage, demonstrating faithful reproduction of film curvature information[cite: 3456, 3477, 3478]."
        },
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Figure 5: (a) The trajectories of the agent, presented on a two-dimensional map of the spaces of admissible filter values, which were obtained at different values of the pump current. Blue line - agent trajectory for 2.7 A environment, orange line agent trajectory for 2.1 A environment, green line agent trajectory for 1.7 A environment, doted line agent trajectory for the model of environment. The color of the map shows the values of the immediate reward, which was calculated on the collected data at a pump diode current of 2.1 A. (b) The value of the immediate reward depending on the step number of the agent. [cite: 5818-5820]"
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 3b: Output space graph showing the trade-off between annealing temperature and conductivity. [cite_start]The plot displays empirical Pareto fronts from four separate optimization campaigns (colored lines/points), demonstrating the clear trade-off where higher conductivity requires higher annealing temperatures [cite: 194-196]."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Figure 5: Working regime of neural network. The black line shows the intensity observed by spectrometer, red line shows stepper motor coordinate. (a) \"Slow\" regime, when no information about intensity level was given. (b) \"Fast\" regime, when the approximate maximum intensity was transmitted to the neural network. (c) The heat map of the intensity distribution for all x and y coordinates. The dashed line indicates the path that the network used to find optimal location pair (x,y)."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 5: Four different beam configurations on the NSLS-II TES (8-BM) beamline, where the upper left-hand panel shows the initial beam and the lower right represents the optimal alignment. In this alignment test, we adjust the translation and rotation of each of the horizontal and vertical Kirkpatrick-Baez mirrors and the pitch and vertical translation of a toroidal mirror, for a total of six degrees of freedom to maximize the flux density of the beam."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 5: PSFs on different additional control delays. The top row is for the integrator control, and the bottom is for PO4AO. The PO4AO and its hyper-parameters are exactly the same for all time delays the time delay is learned from the interaction."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 5: Example of calibrated response function: upward bender motor of the horizontally focusing mirror at 34-ID-C."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Figure 5: Fine-tuning of a policy with RL that (a) has not been initialized and (b) has been initialized via supervised learning. In the latter case, the training starts with a sequence that matches a sparse grid scan sequence. Positions A indicate the scan positions of the first sub-sequence R̃^P_0 that is provided to the RNN as part of the initial input. Positions B and C are the scan positions of all predicted sub-sequences at iteration 0 and 10,000, respectively. The trajectories they form during the optimization process are indicated by dashed blue lines."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Extended Data Figure 1: A-Lab hardware setup. [cite_start]A detailed photographic overview of the physical stations in the A-Lab[cite: 1289]. [cite_start]Panel (a) shows the precursor preparation station, including loading areas, powder dispensing, and mixing [cite: 1266-1273]. [cite_start]Panel (b) shows the heating station with robot arms and box furnaces [cite: 1274-1278]. [cite_start]Panel (c) shows the product handling and characterization station, featuring the X-ray diffractometer, ball dispenser, and shaker [cite: 1279-1288]."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 5: The current landscape of SDLs. A map categorizing existing self-driving laboratories based on their application domain and degree of autonomy. The domains include organic synthesis, inorganic materials, thin films, and nanomaterials[cite: 350, 351]. Specific examples of SDLs are plotted, such as 'Ada' for thin films and the 'A-Lab' for inorganic powders, showing their relative positions in terms of throughput and autonomous capability[cite: 352, 353]. The figure emphasizes the diversity of materials currently being accelerated by SDL technologies[cite: 354]."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Figure 5: Analysis of SDL Performance in Literature. A The system throughput as a function of demonstrated unassisted lifetime, B the number of trials required to reach the optimum value as a function of the total material cost per experiment, and C the dimensionality of the parameter space as a function of the number of trials required to reach the optimum for both liquid handler and microfluidics based automated systems. Note that publications that do not report the listed values are not included in the figure."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Fig. 5. General applicability of the AI-guided robotic system. (A) Chemical structures of other PDI derivatives and chiral dopants tested. (B) Summary of the optimized g-factors for different material combinations."
        }
    ],
    "Laser system": [
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "As an experimental source of the dissipative solitons to test the proposed technique we used figure-of-eight mode-locked fiber laser. This is a flexible platform to tune spectral-temporal properties of the dissipative solitons by adjustable amplification and a saturable absorption inside the fiber loops. The fiber laser cavity consists of two fiber loops, unidirectional (main) and bidirectional (NALM) ones, connected to each other through a 40/60 coupler Fig. 1. NALM loop comprise 5-m long amplifying section of double-clad Yb-doped fiber with absorption of 3.9 dB m-1 at 987 nm. The main loop also includes a 40/60 output coupler and a high-power Faraday isolator that ensures unidirectional propagation. Active fiber is pumped through fiber beam combiner by multimode laser diode at 978 nm. All stretches of fiber and fiber elements inside the cavity maintain polarisation. We implemented a tunable spectral filter allowing to tune simultaneously the central wavelength and spectral bandwidth in the range of 1030–1070 nm and 2.4–4.2 nm, respectively."
        }
    ],
    "Deep Q-learning algorithm": [
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "We formulated the problem in terms of RL by describing the laser tuning process as a sequence of changes of the filter's width and central wavelength by certain values. The central length varied between 1030 nm and 1070 nm, the width - from 2.5 nm to 4.2 nm, the variation step was 0.1 nm for both parameters. Thus, the agent's action space consisted of four possible actions: ±0.1nm for width and ±0.1nm for central length. When the agent attempted to go outside the range of the acceptable values, then the action was not performed and the agent remained in the same position. The state space of the laser was described using two spectral filter parameters and five output radiation characteristics: power and width of the spectrum, the noise and duration at half maximum of the autocorrelation function, the amplitude of pulses. In the reinforcement learning, training of an agent consists in searching for an optimal policy by evaluating the results of the agent's interaction with the environment. We used the DDQN algorithm (which we describe in detail further in the article) in which the policy is determined by computing the value of the action state function (Q-function). At each step, the agent estimates the value of the Q-function for each possible action and chooses the action with the maximal value. As the state space in our problem was continuous, we used a deep neural network to approximate the value of the Q-function. The training process consisted of direct interaction of the agent with the environment and recalculation of the values of the Q-function until the agent learned the optimal policy. Commonly, there is an interest in finding stable pulsed regime with the highest energy provided by the fiber laser. However, energy of a pulse has a threshold value, above which mode-locked regime switches to unstable or partially mode-locked regime. Therefore, we apply here the following reward: R = Paverage / Pnoise where Paverage is the pulse energy taken from the oscilloscope trace, Pnoise is the characteristic of the pulse noise, which is obtained from the ACF data. Pnoise was calculated as a normalized difference between a height of ACF trace and a height of envelope of ACF trace. To derive the height of ACF envelope, we applied a low-pass 3-order Butterworth filter with 0.01 (π rad/sample) cut-off frequency to ACF trace."
        }
    ],
    "A 1.6": [
        {
            "paper_title": "Deep reinforcement learning for self-tuning laser source of dissipative solitons",
            "source_filename": "Kuprikov et al. - 2022 - Deep reinforcement learning for self-tuning laser source of dissipative solitons.pdf",
            "content": "Figure 6: The architecture of double deep Q-learning network with experience replay buffer. Purple arrows show the agent's interaction with the environment. The dotted line separates the time steps. Green lines show what is saved in the experience replay buffer and at what stages. Blue arrows describe the agent training process. [cite: 5892-5895]"
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 4: Quantification of algorithm benefit. (a) Normalized hypervolume vs. samples for qEHVI (with and without noise) compared to random search. (b) Acceleration factor of qEHVI relative to random search. (c) [cite_start]Enhancement factor of qEHVI relative to random search [cite: 267-271]."
        },
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Figure 6: Comparison of operating regimes of the laser-plasma source with and without the reinforcement learning-based automatic target position adjustment. (a) Dynamics of the X-ray signal variation in full scale. (b) Enlarged view of the graph in (a). The dotted lines show the linear trend due to laser ablation."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 6: Four different beam configurations on the NSLS-II ISS (8-ID) beamline during automated alignment, where the upper left-hand panel shows the initial beam and the lower right represents the optimal alignment. In this alignment test, we adjust the translation of a central crystal and the translation and pitch of two ancillary crystals for a total of five degrees of freedom to maximize the flux density of the total beam on the area detector."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 6: The contrast with different time delays. The blue lines correspond to the azimuthal average of integrator images, and the red lines are the same for PO4AO. The line style indicates the length of the time delay. The solid black line is for flat DM, and the dashed black line is for flat DM (open loop) with first-stage residuals played on SLM. Note that around 11 lambda/D is the correction area of the DM-492, and around 18 lambda/D is the correction area of the numerically simulated first-stage DM. PO4AO provides better contrast inside the second-stage control radius for all time delays."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 6: Comparison of simulated beam profiles (red line) with measured data collected at the 34-ID-C beamline (blue dotted line): at focus - horizontal (a) and vertical (b) profiles, out of focus (bender motors position +5 µm) - horizontal (c) and vertical (d) profiles."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Figure 6: (a) Learning curves for the first 10,000 iterations of RL with multiple agents and without credit assignment or with credit assignment, illustrated in orange and blue, respectively. (b) A Voronoi diagram is used to assign a unique reward to each scan position of the predicted sequence. The scan positions are shown as red dots, where the first 50 positions are distributed on the right side within the dark blue area. For visualization purpose, the ground truth reconstruction is included in the diagram."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Extended Data Figure 2: Robotic installations for sample transfer in the A-Lab. [cite_start]Close-up images of the grippers on the UR5e robotic arms used for specific tasks: (a) sample preparation, (b) loading/unloading crucible racks, and (c) sample retrieval and characterization[cite: 1296]. [cite_start]Panel (d) shows the linear rail extending the robot's working envelope, and (e) shows the carousel for organizing samples [cite: 1297-1299]."
        },
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 7: Challenges and future directions. This diagram summarizes the primary hurdles facing the widespread adoption of SDLs and the roadmap for the future. Key challenges identified include lack of standardization in hardware and software (interoperability), data scarcity and quality issues, and the high cost of infrastructure[cite: 412, 413]. Future directions point towards the development of 'Robotic Chemists' (humanoid robots), cloud-based laboratories, and the creation of universal foundation models for chemistry that can generalize across different experimental setups[cite: 414, 415]."
        }
    ],
    "Preamble": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": ""
        }
    ],
    "A self-driving laboratory advances the Pareto front for material properties": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": ""
        }
    ],
    "Quantification of algorithm performance": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "We used computer\nsimulations to quantify the benefit of the qEHVI algorithm\nrelative to random search (an open-loop sampling technique that\ndoes not use feedback from the experiment to determine which\nexperiment to do next). These simulations were performed by\nrunning both the random and qEHVI sampling techniques on a\nresponse surface fit to the experimental data (see “Methods”\nsection). Scenarios with and without experimental noise were\nsimulated by adding synthetic noise to the response surface as\nappropriate (see “Models of the experimental response surface\nand noise” in “Methods” section).The hypervolume (i.e., the area\nunder the Pareto front) was used to measure the progress of\noptimization (Fig. 4a). We used acceleration factor (Fig. 4b) and\nenhancement factor (Fig. 4c) to compare our closed-loop sampling using qEHVI to open-loop sampling using random search;\nsee “Methods” section59. In a noise-free scenario, qEHVI required\nless than 100 samples to outperform 10,000 random samples\n(Fig. 4a). The performance of the qEHVI algorithm degraded in\nthe presence of simulated experimental noise (see “Methods”\nsection), but still exceeded the performance of random search.\nThis performance decrease due to noise emphasizes the importance of minimizing experimental noise when developing a selfdriving experiment. Benchmarks comparing other closed-loop\nand open-loop sampling techniques yielded similar results (see\nSupplementary Fig. 11). These findings highlight how self-driving\nlaboratories can effectively search large materials design spaces\nwithout requiring extremely high throughput."
        }
    ],
    "Translation of discovery to a scalable manufacturing process": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "The practical application of combustion synthesis would require\nthe deposition of uniform films over large areas that are inaccessible to drop casting. On this basis, we set out to combine\npalladium combustion synthesis with ultrasonic spray coating60.\nWe sprayed precursors directly onto a preheated glass substrate60\n(Fig. 5a, see “Methods” section). The precursors decomposed in\nless than five minutes to yield reflective, conductive palladium\nfilms (Fig. 5b). An XRF map of the films (Fig. 5c) showed\nimproved homogeneity relative to the drop-cast films (Fig. 2b).\nWe performed additional spray coating experiments to verify\nthat the trends observed in the autonomous optimization\ntranslate to spray coating (i.e., that conductive palladium films\ncan be obtained below 200 °C and that the film conductivity\nincreases with temperature). Specifically, we spray coated\npalladium films using three recipes from the autonomously\nidentified Pareto front, with temperatures of 191, 200, and 226 °C\n(Fig. 6 and Supplementary Table 2). Triplicate samples were spray\ncoated using each recipe. All three recipes yielded films\napproximately 50–60 nm thick, as measured by XRF microscopy\n(see “Methods” section). All the films were relatively uniform,\nwith spatial variations in the film thickness less than 5% of the\nmean within an 8 mm × 20 mm region at the center of each\nsample (see “Methods” section and Supplementary Table 3).\nSpatial variations in the conductivity within the same region were\nmeasured using the robot and were less than 18% of the mean for\nall samples (see “Methods” section and Supplementary Table 3).\nThe film conductivity of the lowest temperature recipe\n(T = 191 °C) was 1.1 × 105 S m−1, which is approximately 1% of the bulk conductivity of palladium61. The film conductivity can\nincrease by more than an order of magnitude when the spray\ncoating temperature is increased by 35 °C. The highest temperature recipe tested (T = 226 °C) yielded palladium films with a\nconductivity of 2.0 × 106 S m−1, which is comparable to the\nconductivities of sputtered palladium films reported in the\nliterature (2.0–5.8 × 106 S m−1; Fig. 6)62–64. These findings create\nnew opportunities to deposit palladium films without vacuum\nonto large-area substrates, including an expanded range of\ntemperature-sensitive polymers (e.g., Nafion41, polyethersulfone42,\nand heat-stabilized polyethylene naphthalate42). One application\nof this deposition process could be the fabrication of large,\nsupported palladium membranes for more cost-effective electrocatalytic palladium membrane reactors65."
        }
    ],
    "References": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "1. Tabor, D. P. et al. Accelerating the discovery of materials for clean energy in\nthe era of smart automation. Nat. Rev. Mater. 3, 5–20 (2018).\n2. Stein, H. S. & Gregoire, J. M. Progress and prospects for accelerating materials\nscience with automated and autonomous workflows. Chem. Sci. 10, 9640–9649\n(2019).\n3. Häse, F., Roch, L. M. & Aspuru-Guzik, A. Next-generation experimentation\nwith self-driving laboratories. Trends Chem. 1, 282–291 (2019).\n4. MacLeod, B. P., Parlane, F. G. L., Brown, A. K., Hein, J. E. & Berlinguette, C. P.\nFlexible automation accelerates materials discovery. Nat. Mater. https://\ndoi.org/10.1038/s41563-021-01156-3 (2021).\n5. Ament, S. et al. Autonomous materials synthesis via hierarchical active\nlearning of nonequilibrium phase diagrams. Science Advances 7, eabg4930\n(2021).\n6. Bash, D. et al. Multi‐fidelity high‐throughput optimization of electrical\nconductivity in P3HT‐CNT composites. Adv. Funct. Mater. 2102606 (2021).\n7. Nikolaev, P. et al. Autonomy in materials research: a case study in carbon\nnanotube growth. npj Comput. Mater. 2, 16031 (2016).\n8. MacLeod, B. P. et al. Self-driving laboratory for accelerated discovery of thinfilm materials. Sci. Adv. 6, eaaz8867 (2020).\n9. Langner, S. et al. Beyond ternary OPV: high-throughput experimentation and\nself-driving laboratories optimize multicomponent systems. Adv. Mater. 32,\ne1907801 (2020).\n10. Li, J. et al. Autonomous discovery of optically active chiral inorganic\nperovskite nanocrystals through an intelligent cloud lab. Nat. Commun. 11,\n2046 (2020).\n11. Gongora, A. E. et al. A Bayesian experimental autonomous researcher for\nmechanical design. Sci. Adv. 6, eaaz1708 (2020).\n12. Burger, B. et al. A mobile robotic chemist. Nature 583, 237–241 (2020).\n13. Wang, L., Karadaghi, L. R., Brutchey, R. L. & Malmstadt, N. Self-optimizing\nparallel millifluidic reactor for scaling nanoparticle synthesis. Chem. Commun.\n56, 3745–3748 (2020). 14. Shimizu, R., Kobayashi, S., Watanabe, Y., Ando, Y. & Hitosugi, T.\nAutonomous materials synthesis by machine learning and robotics. APL\nMater. 8, 111110 (2020).\n15. Dave, A. et al. Autonomous discovery of battery electrolytes with robotic\nexperimentation and machine learning. Cell Rep. Phys. Sci. 1, 100264 (2020).\n16. Deneault, J. R. et al. Toward autonomous additive manufacturing: Bayesian\noptimization on a 3D printer. MRS Bull. https://doi.org/10.1557/s43577-021-\n00051-1 (2021).\n17. Hall, B. L. et al. Autonomous optimisation of a nanoparticle catalysed\nreduction reaction in continuous flow. Chem. Commun. https://doi.org/\n10.1039/d1cc00859e (2021).\n18. Krishnadasan, S., Brown, R. J. C., deMello, A. J. & deMello, J. C. Intelligent routes\nto the controlled synthesis of nanoparticles. Lab Chip 7, 1434–1441 (2007).\n19. Moore, J. S. & Jensen, K. F. Automated multitrajectory method for reaction\noptimization in a microfluidic system using online IR analysis. Org. Process\nRes. Dev. 16, 1409–1415 (2012).\n20. Walker, B. E., Bannock, J. H., Nightingale, A. M. & deMello, J. C. Tuning reaction\nproducts by constrained optimisation. React. Chem. Eng. 2, 785–798 (2017).\n21. Salley, D. et al. A nanomaterials discovery robot for the Darwinian evolution\nof shape programmable gold nanoparticles. Nat. Commun. 11, 2771 (2020).\n22. Epps, R. W. et al. Artificial chemist: an autonomous quantum dot synthesis\nbot. Adv. Mater. 32, e2001626 (2020).\n23. Christensen, M. et al. Data-science driven autonomous process optimization.\nCommun. Chem. 4, 1–12 (2021).\n24. Mekki-Berrada, F. et al. Two-step machine learning enables optimized\nnanoparticle synthesis. npj Comput. Mater. 7, 1–10 (2021).\n25. Abdel-Latif, K. et al. Self‐driven multistep quantum dot synthesis enabled by\nautonomous robotic experimentation in flow. Adv. Intell. Syst. 3, 2000245 (2021).\n26. Grizou, J., Points, L. J., Sharma, A. & Cronin, L. A curious formulation robot\nenables the discovery of a novel protocell behavior. Sci. Adv. 6, eaay4237 (2020).\n27. Schweidtmann, A. M. et al. Machine learning meets continuous flow\nchemistry: automated optimization towards the Pareto front of multiple\nobjectives. Chem. Eng. J. 352, 277–282 (2018).\n28. Cao, L. et al. Optimization of formulations using robotic experiments driven\nby machine learning DoE. Cell Rep. Phys. Sci. 2, 100295 (2021).\n29. Maaliou, O. & McCoy, B. J. Optimization of thermal energy storage in packed\ncolumns. Sol. Energy 34, 35–41 (1985).\n30. Ahmadi, M. H., Ahmadi, M. A., Bayat, R., Ashouri, M. & Feidt, M. Thermoeconomic optimization of Stirling heat pump by using non-dominated sorting\ngenetic algorithm. Energy Convers. Manag. 91, 315–322 (2015).\n31. Li, Z., Pradeep, K. G., Deng, Y., Raabe, D. & Tasan, C. C. Metastable highentropy dual-phase alloys overcome the strength-ductility trade-off. Nature\n534, 227–230 (2016).\n32. Zhang, L. et al. Correlated metals as transparent conductors. Nat. Mater. 15,\n204–210 (2016).\n33. Park, H. B., Kamcev, J., Robeson, L. M., Elimelech, M. & Freeman, B. D.\nMaximizing the right stuff: The trade-off between membrane permeability and\nselectivity. Science 356, eaab0530 (2017).\n34. Oviedo, F. et al. Bridging the gap between photovoltaics R&D and\nmanufacturing with data-driven optimization. Preprint at https://arxiv.org/\n2004.13599v1 (2020).\n35. Liu, L. et al. Making ultrastrong steel tough by grain-boundary delamination.\nScience 368, 1347–1352 (2020).\n36. Ramirez, I., Causa’, M., Zhong, Y., Banerji, N. & Riede, M. Key tradeoffs\nlimiting the performance of organic photovoltaics. Adv. Energy Mater. 8,\n1703551 (2018).\n37. Kirkey, A., Luber, E. J., Cao, B., Olsen, B. C. & Buriak, J. M. Optimization of\nthe bulk heterojunction of all-small-molecule organic photovoltaics using\ndesign of experiment and machine learning approaches. ACS Appl. Mater.\nInterfaces 12, 54596–54607 (2020).\n38. Ren, S. et al. Molecular electrocatalysts can mediate fast, selective CO2\nreduction in a flow cell. Science 365, 367–369 (2019).\n39. Baumeler, T. et al. Minimizing the trade-off between photocurrent and\nphotovoltage in triple-cation mixed-halide perovskite solar cells. J. Phys.\nChem. Lett. 11, 10188–10195 (2020).\n40. Voskanyan, A. A., Li, C.-Y. V. & Chan, K.-Y. Catalytic palladium film\ndeposited by scalable low-temperature aqueous combustion. ACS Appl. Mater.\nInterfaces 9, 33298–33307 (2017).\n41. Mauritz, K. A. & Moore, R. B. State of understanding of nafion. Chem. Rev.\n104, 4535–4585 (2004).\n42. MacDonald, W. A. et al. Latest advances in substrates for flexible electronics. J.\nSoc. Inf. Disp. 15, 1075 (2007).\n43. Kim, M.-G., Kanatzidis, M. G., Facchetti, A. & Marks, T. J. Low-temperature\nfabrication of high-performance metal oxide thin-film electronics via\ncombustion processing. Nat. Mater. 10, 382–388 (2011).\n44. Hennek, J. W., Kim, M.-G., Kanatzidis, M. G., Facchetti, A. & Marks, T. J.\nExploratory combustion synthesis: amorphous indium yttrium oxide for thinfilm transistors. J. Am. Chem. Soc. 134, 9593–9596 (2012).\n45. Perelaer, J. et al. Printed electronics: the challenges involved in printing\ndevices, interconnects, and contacts based on inorganic materials. J. Mater.\nChem. 20, 8446–8453 (2010).\n46. Li, D., Lai, W.-Y., Zhang, Y.-Z. & Huang, W. Printable transparent conductive\nfilms for flexible electronics. Adv. Mater. 30, 1704738 (2018).\n47. Cochran, E. A. et al. Role of combustion chemistry in low-temperature\ndeposition of metal oxide thin films from solution. Chem. Mater. 29,\n9480–9488 (2017).\n48. Wang, B. et al. Marked cofuel tuning of combustion synthesis pathways for\nmetal oxide semiconductor films. Adv. Electron. Mater. 5, 1900540 (2019).\n49. Plassmeyer, P. N., Mitchson, G., Woods, K. N., Johnson, D. C. & Page, C. J.\nImpact of relative humidity during spin-deposition of metal oxide thin films\nfrom aqueous solution precursors. Chem. Mater. 29, 2921–2926 (2017).\n50. Kumar, A., Wolf, E. E. & Mukasyan, A. S. Solution combustion synthesis of\nmetal nanopowders: copper and copper/nickel alloys. AIChE J. 57, 3473–3479\n(2011).\n51. Manukyan, K. V. et al. Solution combustion synthesis of nano-crystalline\nmetallic materials: mechanistic studies. J. Phys. Chem. C 117, 24417–24427\n(2013).\n52. Mitzi, D. Solution Processing of Inorganic Materials (Wiley, 2008).\n53. Cochran, E. A., Woods, K. N., Johnson, D. W., Page, C. J. & Boettcher, S. W.\nUnique chemistries of metal-nitrate precursors to form metal-oxide thin films\nfrom solution: materials for electronic and energy applications. J. Mater.\nChem. A 7, 24124–24149 (2019).\n54. Pujar, P., Gandla, S., Gupta, D., Kim, S. & Kim, M. Trends in low‐temperature\ncombustion derived thin films for solution‐processed electronics. Adv.\nElectron. Mater. 6, 2000464 (2020).\n55. Daulton, S., Balandat, M. & Bakshy, E. Differentiable Expected Hypervolume\nImprovement for Parallel Multi-Objective Bayesian Optimization. Advances in\nNeural Information Processing Systems 33 (eds. Larochelle, H. et al.)\n9851–9864 (Curran Associates, Inc., 2020).\n56. Knowles, J. ParEGO: a hybrid algorithm with on-line landscape\napproximation for expensive multiobjective optimization problems. IEEE\nTrans. Evol. Comput. 10, 50–66 (2006).\n57. Paria, B., Kandasamy, K. & Póczos, B. A Flexible Framework for MultiObjective Bayesian Optimization using Random Scalarizations. In Proceedings\nof The 35th Uncertainty in Artificial Intelligence Conference (eds. Adams, R.\nP. & Gogate, V.) 115 766–776 (PMLR, 2020).\n58. Daulton, S., Balandat, M. & Bakshy, E. Parallel Bayesian Optimization of\nMultiple Noisy Objectives with Expected Hypervolume Improvement.\nAdvances in Neural Information Processing Systems 34 (eds. Ranzato, M. et al.)\n(Curran Associates, Inc., 2021).\n59. Rohr, B. et al. Benchmarking the acceleration of materials discovery by\nsequential learning. Chem. Sci. 11, 2696–2706 (2020).\n60. Yu, X. et al. Spray-combustion synthesis: efficient solution route to highperformance oxide transistors. Proc. Natl Acad. Sci. USA 112, 3217–3222\n(2015).\n61. Matula, R. A. Electrical resistivity of copper, gold, palladium, and silver. J.\nPhys. Chem. Ref. Data 8, 1147–1298 (1979).\n62. Shi, Y. S. Electrical resistivity of RF sputtered Pd films. Phys. Lett. A 319,\n555–559 (2003).\n63. Hloch, H. & Wissmann, P. The electrical resistivity of thin pd films grown on\nSi(111). Phys. Status Solidi A 145, 521–526 (1994).\n64. Anton, R., Häupl, K., Rudolf, P. & Wißmann, P. Electrical and structural\nproperties of thin palladium films. Z. f.ür. Naturforsch. A 41, 665–670 (1986).\n65. Delima, R. S., Sherbo, R. S., Dvorak, D. J., Kurimoto, A. & Berlinguette, C. P.\nSupported palladium membrane reactor architecture for electrocatalytic\nhydrogenation. J. Mater. Chem. A 7, 26586–26595 (2019).\n66. Bernhardsson, E. & Freider, E. L. https://github.com/spotify/luigi.\n67. Pedregosa, F. et al. Scikit-learn: machine learning in Python. J. Mach. Learn.\nRes. 12, 2825–2830 (2011).\n68. Balandat, M. et al. BoTorch: a framework for efficient Monte-Carlo Bayesian\noptimization. Advances in Neural Information Processing Systems 33 (eds.\nLarochelle, H. et al.) 21524–21538 (Curran Associates, Inc., 2020).\n69. Bakshy, E. et al. Advances in Neural Information Processing Systems vol. 31\n(The MIT Press, 2018).\n70. Owen, A. B. Scrambling Sobol’ and Niederreiter–Xing Points. J. Complex. 14,\n466–489 (1998)."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "1. Rodenburg, J. M. Ptychography and related diffractive imaging methods. Adv. Imaging Electron Phys. 150, 87–184 (2008).\n2. Humphry, M., Kraus, B., Hurst, A., Maiden, A. & Rodenburg, J. Ptychographic electron microscopy using high-angle dark-field scattering for sub-nanometre resolution imaging. Nat. Commun. 3, 1–7 (2012).\n3. Lozano, J. G., Martinez, G. T., Jin, L., Nellist, P. D. & Bruce, P. G. Low-dose aberration-free imaging of Li-rich cathode materials at various states of charge using electron ptychography. Nano Lett. 18, 6850–6855 (2018).\n4. Jiang, Y. et al. Electron ptychography of 2D materials to deep sub-ångström resolution. Nature 559, 343–349 (2018).\n5. Zhou, L. et al. Low-dose phase retrieval of biological specimens using cryo-electron ptychography. Nat. Commun. 11, 1–9 (2020).\n6. McMullan, G., Faruqi, A. & Henderson, R. Direct electron detectors. Methods Enzymol. 579, 1–17 (2016).\n7. Tate, M. W. et al. High dynamic range pixel array detector for scanning transmission electron microscopy. Microsc. Microanal. 22, 237–249 (2016).\n8. Thibault, P. et al. High-resolution scanning x-ray diffraction microscopy. Science 321, 379–382 (2008).\n9. Maiden, A. M. & Rodenburg, J. M. An improved ptychographical phase retrieval algorithm for diffractive imaging. Ultramicroscopy 109, 1256–1262 (2009).\n10. Van den Broek, W. & Koch, C. T. Method for retrieval of the three-dimensional object potential by inversion of dynamical electron scattering. Phys. Rev. Lett. 109, 245502 (2012).\n11. Maiden, A. M., Humphry, M. J. & Rodenburg, J. Ptychographic transmission microscopy in three dimensions using a multi-slice approach. JOSA A 29, 1606–1614 (2012).\n12. Van den Broek, W. & Koch, C. T. General framework for quantitative three-dimensional reconstruction from arbitrary detection geometries in TEM. Phys. Rev. B 87, 184108 (2013).\n13. Tsai, E. H., Usov, I., Diaz, A., Menzel, A. & Guizar-Sicairos, M. X-ray ptychography with extended depth of field. Opt. Express 24, 29089–29108 (2016).\n14. Jiang, Y. et al. Breaking the Rayleigh limit in thick samples with multi-slice ptychography. Microsc. Microanal. 24, 192–193 (2018).\n15. Pelz, P. M., Qiu, W. X., Bücker, R., Kassier, G. & Miller, D. R. J. Low-dose cryo electron ptychography via non-convex Bayesian optimization. Sci. Rep. 7, 9883 (2017).\n16. Song, J. et al. Atomic resolution defocused electron ptychography at low dose with a fast, direct electron detector. Sci. Rep. 9, 1–8 (2019).\n17. Schloz, M. et al. Overcoming information reduced data and experimentally uncertain parameters in ptychography with regularized optimization. Opt. Express 28, 28306–28323 (2020).\n18. Chen, Z. et al. Mixed-state electron ptychography enables sub-angstrom resolution imaging with picometer precision at low dose. Nat. Commun. 11, 1–10 (2020).\n19. Huang, X. et al. Optimization of overlap uniformness for ptychography. Opt. Express 22, 12634–12644 (2014).\n20. Levine, S., Finn, C., Darrell, T. & Abbeel, P. End-to-end training of deep visuomotor policies. J. Mach. Learn. Res. 17, 1334–1373 (2016).\n21. Andrychowicz, O. M. et al. Learning dexterous in-hand manipulation. Int. J. Robot. Res. 39, 3–20 (2020).\n22. Sharma, S., Kiros, R. & Salakhutdinov, R. Action recognition using visual attention. arXiv preprint arXiv:1511.04119 (2015).\n23. Ba, J., Mnih, V. & Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv preprint arXiv:1412.7755 (2014).\n24. Mnih, V., Heess, N., Graves, A. et al. Recurrent models of visual attention. In Advances in Neural Information Processing Systems 2204–2212 (2014).\n25. Betterton, J.-R., Ratner, D., Webb, S. & Kochenderfer, M. Reinforcement learning for adaptive illumination with x-rays. In 2020 IEEE International Conference on Robotics and Automation (ICRA) 328–334 (IEEE, 2020).\n26. Vasudevan, R. K. et al. Autonomous experiments in scanning probe microscopy and spectroscopy: Choosing where to explore polarization dynamics in ferroelectrics. arXiv preprint arXiv:2011.13050 (2020).\n27. Dahmen, T. et al. Feature adaptive sampling for scanning electron microscopy. Sci. Rep. 6, 1–11 (2016).\n28. Schloz, M., Müller, J., Pekin, T. C., Van den Broek, W. & Koch, C. T. Deep reinforcement learning for data-driven adaptive scanning in ptychography. Zenodo https://doi.org/10.5281/zenodo.7865575 (2023).\n29. Van Heel, M., Keegstra, W., Schutter, W. & Van Bruggen, E. Arthropod hemocyanin structures studied by image analysis. Life Chem. Rep. Suppl 1, 5 (1982).\n30. Wang, Z., Bovik, A. C., Sheikh, H. R. & Simoncelli, E. P. Image quality assessment: From error visibility to structural similarity. IEEE Trans. Image Process. 13, 600–612 (2004).\n31. Pelz, P. M., Johnson, I., Ophus, C., Ercius, P. & Scott, M. C. Real-time interactive 4D-stem phase-contrast imaging from electron event representation data: Less computation with the right representation. IEEE Signal Process. Mag. 39, 25–31 (2021).\n32. Madsen, J. et al. Ab initio electrostatic potentials for 4D-stem ptychographic reconstruction. Microsc. Microanal. 28, 392–393 (2022).\n33. Huang, P. Y. et al. Direct imaging of a two-dimensional silica glass on graphene. Nano Lett. 12, 1081–1086 (2012).\n34. Kalinin, S. V. et al. Automated and autonomous experiment in electron and scanning probe microscopy. arXiv preprint arXiv:2103.12165 (2021).\n35. Kalinin, S. V. et al. Machine learning in scanning transmission electron microscopy. Nat. Rev. Methods Primers 2, 1–28 (2022).\n36. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors. Nature 323, 533–536 (1986).\n37. Elman, J. L. Finding structure in time. Cogn. Sci. 14, 179–211 (1990).\n38. Werbos, P. J. Generalization of backpropagation with application to a recurrent gas market model. Neural Netw. 1, 339–356 (1988).\n39. Chung, J., Gulcehre, C., Cho, K. & Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n40. Masci, J., Meier, U., Cireşan, D. & Schmidhuber, J. Stacked convolutional auto-encoders for hierarchical feature extraction. In International Conference on Artificial Neural Networks 52–59 (Springer, 2011).\n41. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992).\n42. Liu, Y., Zhang, K., Basar, T. & Yin, W. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. In NeurIPS (2020).\n43. Sutton, R. S., McAllester, D. A., Singh, S. P. & Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 1057–1063 (2000).\n44. Wu, C. et al. Variance reduction for policy gradient with action-dependent factorized baselines. arXiv preprint arXiv:1803.07246 (2018).\n45. Feriani, A. & Hossain, E. Single and multi-agent deep reinforcement learning for AI-enabled wireless networks: A tutorial. IEEE Commun. Surv. Tutor. 23, 1226–1252 (2021).\n46. Okabe, A. Spatial tessellations. In International Encyclopedia of Geography: People, the Earth, Environment and Technology: People, the Earth, Environment and Technology 1–11 (2016).\n47. Madsen, J. & Susi, T. The abTEM code: Transmission electron microscopy from first principles. Open Res. Eur. 1, 13015. https://doi.org/10.12688/openreseurope.13015.1 (2021).\n48. Rong, G. & Tan, T.-S. Jump flooding in GPU with applications to Voronoi diagram and distance transform. In Proceedings of the 2006 Symposium on Interactive 3D Graphics and Games 109–116 (2006).\n49. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems Vol. 32 (eds Wallach, H. et al.) 8024–8035 (Curran Associates, Inc., 2019).\n50. Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n51. https://gitlab.com/Schlozma/adaptive-scanning-in-ptychography."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "1. Abolhasani, M. & Kumacheva, E. The rise of self-driving labs in chemical and materials sciences. Nat. Synth. 2, 483–492 (2023).\n2. Beal, J. & Rogers, M. Levels of autonomy in synthetic biology engineering. Mol. Syst. Biol. 16, e10019 (2020).\n3. Martin, H. G. et al. Perspectives for self-driving labs in synthetic biology. Curr. Opin. Biotechnol. 79, 102881 (2023).\n4. Volk, A. A. et al. AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning. Nat. Commun. 14, 1–16 (2023).\n5. S. Surfanovic & D. Bingham, Virtual Library of Simulation Experiments: Test Functions and Datasets. https://www.sfu.ca/~ssurjano/about.html. (2023).\n6. Y. Watanabe, T. Okamoto & E. Aiyoshi, Nauka, https://doi.org/10.1541/IEEJEISS.126.1559.\n7. Griewank, A. O. Generalized descent for global optimization. J. Optim. Theory Appl. 34, 11–39 (1981).\n8. D. H. Ackley, A Connectionist Machine for Genetic Hillclimbing, Springer US, 1987.\n9. Pedregosa, F. et al. J. Mach. Learn. Res. 12, 2825–2830 (2011).\n10. Harris, C. R. et al. Array programming with NumPy. Nature 585, 357–362 (2020).\n11. Epps, R. W. & Abolhasani, M. Modern nanoscience: Convergence of AI, robotics, and colloidal synthesis. Appl. Phys. Rev. 8, 041316 (2021).\n12. L. S. Shapley, A Value N-Person Games, https://doi.org/10.7249/P0295.\n13. Krishnadasan, S., Brown, R. J. C. C., deMello, A. J. & DeMello, J. C. Intelligent routes to the controlled synthesis of nanoparticles. Lab Chip 7, 1434–1441 (2007).\n14. Wigley, P. B. et al. Fast machine-learning online optimization of ultra-cold-atom experiments. Sci. Rep. 6, 1–6 (2016).\n15. Bezinge, L., Maceiczyk, R. M., Lignos, I., Kovalenko, M. V. & deMello, A. J. Pick a Color MARIA: adaptive sampling enables the rapid identification of complex perovskite nanocrystal compositions with defined emission characteristics. ACS Appl. Mater. Interfaces 10, 18869–18878 (2018).\n16. Epps, R. W. et al. Artificial Chemist: An Autonomous Quantum Dot Synthesis Bot. Adv. Mater. 32, 2001626 (2020).\n17. Salley, D., Keenan, G., Grizou, J., Sharma, A., Martín, S. & Cronin, L. A nanomaterials discovery robot for the Darwinian evolution of shape programmable gold nanoparticles. Nat. Commun. 11, 2771 (2020).\n18. Li, J. J. J. et al. U1 snRNP regulates cancer cell migration and invasion in vitro. Nat. Commun. 11, 1–10 (2020).\n19. Mekki-Berrada, F. et al. npj Comput. Mater. 7, 1–10 (2020).\n20. Abdel-Latif, K., Epps, R. W., Bateni, F., Han, S., Reyes, K. G. & Abolhasani, M. Self‐Driven Multistep Quantum Dot Synthesis Enabled by Autonomous Robotic Experimentation in Flow. Adv. Intell. Syst. 3, 2000245 (2021).\n21. Ohkubo, I. et al. Mater. Today Phys. 16, 100296. (2021)\n22. Jiang, Y. et al. An artificial intelligence enabled chemical synthesis robot for exploration and optimization of nanomaterials. Sci. Adv. 8, eabo2626 (2022).\n23. Bateni, F.et al. Autonomous Nanocrystal Doping by Self‐Driving Fluidic Micro‐Processors. Adv. Intell. Syst. 4, 2200017 (2022).\n24. Kosuri, S. et al. Machine-Assisted Discovery of Chondroitinase ABC Complexes toward Sustained Neural Regeneration. Adv. Healthc. Mater. 11, 2102101 (2022).\n25. Tamasi, M. J. et al. Machine learning on a robotic platform for the design of polymer-protein hybrids. Adv. Mater. 34, 2201809 (2022).\n26. Knox, S. T., Parkinson, S. J., Wilding, C. Y. P., Bourne, R. A. & Warren, N. J. Autonomous polymer synthesis delivered by multi-objective closed-loop optimisation. Polym. Chem. 13, 1576–1585 (2022).\n27. Wakabayashi, Y. K., Otsuka, T., Krockenberger, Y., Sawada, H., Taniyasu, Y. & Yamamoto, H. Machine-learning-assisted thin-film growth: Bayesian optimization in molecular beam epitaxy of SrRuO3 thin films. APL Mater. 7, 101114 (2019).\n28. Li, C. et al. A cluster of palmitoylated cysteines are essential for aggregation of cysteine-string protein mutants that cause neuronal ceroid lipofuscinosis. Sci. Rep. 7, 1–10 (2017).\n29. Bateni, F. et al. Smart Dope: A Self-Driving Fluidic Lab for Accelerated Development of Doped Perovskite Quantum Dots. Adv. Energy Mater. 14, 2302303 (2024).\n30. Sadeghi, S. et al. Autonomous nanomanufacturing of lead-free metal halide perovskite nanocrystals using a self-driving fluidic lab. Nanoscale 16, 580–591 (2024)."
        }
    ],
    "Acknowledgements": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "The authors are grateful to Natural Resources Canada’s Energy Innovation Program\n(EIP2-MAT-001) for financial support. The authors are grateful to the Canadian Natural\nScience and Engineering Research Council (RGPIN-2018-06748), Canadian Foundation\nfor Innovation (229288), Canadian Institute for Advanced Research (BSE-BERL-162173),\nand Canada Research Chairs for financial support. B.P.M., F.G.L.P., T.D.M. and C.P.B.\nacknowledge support from the SBQMI’s Quantum Electronic Science and Technology Initiative, the Canada First Research Excellence Fund, and the Quantum Materials and\nFuture Technologies Program. We would like to acknowledge the many open-source\nsoftware communities without whose efforts this project would not have been possible.\nPlease see the supplementary information for further details."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "M.S., T.C.P. and W.V.d.B. acknowledge financial support from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) Grant No. BR 5095/2-1. M.S., J.Mü. and C.T.K acknowledge support from the DFG—Project-ID 182087777—SFB 951. M.S. and C.T.K. acknowledge financial support from the DFG—Project-ID 414984028—SFB 1404. We thank Prof. Sang Ho Oh and Dr. Jinsol Seo (Korea Institute of Energy Technology, Naju, Korea), as well as Dr. Bumsu Park (CEMES, Toulouse, France) for providing the MoS₂ sample. We acknowledge funding from the European Research Council (ERC) under the European Union's Horizon 2020 Research and Innovation Programme via Grant agreement No. 756277-ATMEN (J.Ma. and T.S). M.S. would like to thank Jayesh K. Gupta (Microsoft, Stanford, USA) for his helpful suggestion about the description of the POSG formalism."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "M.A. gratefully acknowledge the financial support from the Dreyfus Program for Machine Learning in the Chemical Sciences and Engineering (Award # ML-21-064), University of North Carolina Research Opportunities Initiative (UNC-ROI) program, and National Science Foundation (Awards #1940959 and 2208406)."
        }
    ],
    "Author contributions": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "C.P.B. conceived and supervised the project. B.P.M., K.E.D. and F.G.L.P. designed and\nperformed the autonomous optimization experiments. M.B.R., K.O., C.W., K.E.D., O.P.,\nM.S.E., B.P.M. and F.G.L.P. developed the robotic hardware. M.S.E., O.P. and K.E.D.\ndeveloped and configured the robotic control software. F.G.L.P. and T.H.H. developed\nthe data analysis software with input from N.T., K.E.D. and B.P.M., F.G.L.P., M.M. and\nB.P.M. performed and analyzed the simulations. M.S.E. and B.P.M. configured the EVHI\noptimization algorithm and interfaced it with the self-driving laboratory. K.O., H.N.C.\nand C.C.R. developed the spray coater hardware and software. C.C.R. performed the\nspray coating experiments. N.T. performed additional data analysis. D.J.D. performed\nadditional experiments. All authors participated in the writing of the manuscript."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "M.S. conceived the idea of the adaptive scanning workflow. J.Mü. carried out the ptychography experiment. M.S. implemented all the neural network applications and modified the ptychography reconstruction algorithm. M.S. wrote up the manuscript. J.Ma. and T.S. developed the atom finding method used for the alternative low-dose procedure. T.C.P., W.V.d.B., T.S. and C.K. commented on the manuscript. W.V.d.B. and C.K. supervised the project. The manuscript reflects the contributions of all authors."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "M.A. and A.A.V. conceived the project. A.A.V. designed the benchmarking algorithm. M.A. acquired funding and directed the project. A.A.V. and M.A. drafted and edited the manuscript."
        }
    ],
    "Competing interests": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "The authors declare no competing interests."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "The authors declare no competing interests."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "The authors declare no competing interests."
        }
    ],
    "Additional information": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41467-022-28580-6."
        },
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Supplementary Information The online version contains supplementary material available at https://doi.org/10.1038/s41598-023-35740-1.\n\nCorrespondence and requests for materials should be addressed to M.S.\n\nReprints and permissions information is available at www.nature.com/reprints.\n\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41467-024-45569-5.\n\nCorrespondence and requests for materials should be addressed to Milad Abolhasani.\n\nPeer review information Nature Communications thanks the anonymous reviewer(s) for their contribution to the peer review of this work.\n\nReprints and permissions information is available at http://www.nature.com/reprints\n\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
        }
    ],
    "A 1.7": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 5: Low-temperature spray combustion synthesis apparatus and results. (a) Photo of the ultrasonic spray-coating nozzle and hotplate fixture. (b) Photograph of a resulting palladium film on a glass substrate. (c) [cite_start]XRF homogeneity map showing the palladium signal distribution across the film [cite: 282-287]."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 7: A schematic of beamline 5.3.1 at the Advanced Light Source. The beamline has four degrees of freedom (toroidal mirror pitch and bend, and monochromator angle and height) and four constraints (four-jaw slits)."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 7: Learning curve for the low flux experiment. The blue line is the cumulative reward after each episode for the integrator, and the red line is for PO4AO. The dashed green line is the reward after each episode when the turbulence was not played and the loop opened."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 7: Measured (a) and simulated (b) beam images at the 28-ID-B beamline."
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": " Extended Data Figure 3: Communication protocols connecting each module in the A-Lab. [cite_start]A block diagram illustrating the local area network (LAN) connecting all hardware modules (furnaces, robots, XRD machine, balances) to a central control computer via RS-485, HTTP, and Socket connections[cite: 1325, 1326]. [cite_start]The control computer manages the workflow, while Arduino-controlled devices handle specific tasks like capping and dispensing[cite: 1306, 1315]."
        }
    ],
    "A 1.8": [
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "source_filename": "MacLeod et al. - 2022 - A self-driving laboratory advances the Pareto front for material properties.pdf",
            "content": "Figure 6: Conductivity comparison bar chart. It compares the conductivity of spray-coated palladium films (Recipe 1 at 191°C, Recipe 2 at 200°C, Recipe 3 at 226°C) identified by Ada against literature values for sputtered films and bulk palladium. [cite_start]The chart shows spray combustion films achieving conductivities comparable to sputtered films [cite: 331-334]."
        },
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 8: Four different beam configurations on the Advanced Light Source Beamline 5.3.1 during automated alignment. In total, the photon transport has four active degrees of freedom: the focusing mirror pitch and tangential bend, and the channel-cut crystal angle and height. The upper left-hand panel shows the initial manually aligned beam and the lower right the final beam after automated alignment. The upper right and lower left panels show intermediate points collected in the automated alignment process."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 8: PSFs during the low flux experiment. The left image is for the integrator, and the right is for the PO4AO."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 8: MOBO auto-alignment for the 34-ID-C digital twin.(a) and (b) show the ID x and y beam profiles, respectively, obtained as data for the manually optimized reference structure (M), the initial structure (I), and the Nash solution (NS) structure. (c) The scatter plot for the optimization objectives (FWHM and PL) with the colors indicating the trial number. (d-i) show the actual 2D profiles (not used for the optimization) of the beam at the diffractometer position: (d) reference, (e) initial, (f) NS solution, and (g-i) other Pareto optimal beam profiles."
        }
    ],
    "Materials and Methods": [
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Experimental Setup: In the experiments, a femtosecond ytterbium fiber laser ANTAUS-10W-40u/250K with a wavelength of 1030 nm and an average power of up to 20 W was used. The pulse repetition rate was 2.0 MHz, with a maximum pulse energy of 10 µJ and a laser pulse duration of approximately 280 fs. The laser radiation was focused onto the end of a rotating and vertically cyclically displaced copper cylinder using a microscopic objective PAL-20-NIR-HR-LC00 with a focal length of 10 mm. A target with a polished side surface was mounted on a motor shaft taken from a hard disk. The vibrations of the rotating surface on the shaft did not exceed 2 µm. This displacement algorithm allowed a single target to be operated for at least 5 h—during this time interval the dispersion of the X-ray intensity did not exceed 15%. To prevent deposition of ablated particles on the focusing optics, a compressed air blowing system was assembled in the exposure area. X-ray radiation was measured using a single-channel scintillation detector SCSD-4 placed 13 cm away from the target. The focus position relative to the target was controlled based on the intensity of the X-ray radiation. The second harmonic signal was focused into the HR USB4000 fiber spectrometer. The rotating target was mounted on a motorized 5-axis table controlled by a WiFi2Duet stepper motor driver. Signal collection was fully automated using LabVIEW. Neural Network Architecture: The developed setup is based on the DQN learning algorithm. Deep Q-Network (DQN) is a popular algorithm used in reinforcement learning, which combines the Q-learning algorithm with deep neural networks to train an agent with optimal action strategies in a given environment. The DQN algorithm was implemented based on two linear layers, each consisting of 1024 neurons. The training was performed on an Nvidia RTX 3070 graphics card. The training parameters were set as follows: memory size of 100,000 elements, batch size of 1024 elements, learning rate of 10^-4, Adam optimizer, MSE loss error criterion, and error decay rate of 5x10^-4. Construction of the Reward Function: The reward function was calculated as follows: -3 if the neural network suggests leaving the movement area; -1*(I_{t-1}-I_t)+(I_t-I_max)/I_max if the feedback amplitude decreases; (I_{t-1}-I_t)+(I_t-I_max)/I_max if the feedback amplitude increases; 0.5 if the pause occurs within the range of 0.9 I_max-I_max; -(I_t-I_max)/I_max if the pause occurs within the range of 0.9 I_max-I_max (to avoid stopping outside the optimum)."
        },
        {
            "paper_title": "Inverse design of chiral functional films by a robotic AI-guided system",
            "source_filename": "Xie et al. - 2023 - Inverse design of chiral functional films by a robotic AI-guided system.pdf",
            "content": "Materials: PDI-1 was synthesized according to literature procedures. R-1 and S-1 were purchased from Sigma-Aldrich. All solvents were of analytical grade and used as received. Robotic System: The mobile robot (MiR100) was equipped with a UR5 robotic arm (Universal Robots) and a gripper (Robotiq). The liquid handling station (Opentrons OT-2) was used for solution preparation. The spin-coater (Laurell) and hotplate (Thermo Fisher) were integrated into the workflow for film fabrication. Characterization: CD spectra were recorded on a JASCO J-1500 spectrometer. UV-vis absorption spectra were measured on a PerkinElmer Lambda 750 spectrophotometer. AFM images were obtained using a Bruker Dimension Icon microscope. XRD measurements were performed on a Rigaku SmartLab diffractometer. Algorithm: The BO algorithm was implemented using the BoTorch library in Python. Gaussian process regression was used as the surrogate model, and the expected improvement (EI) acquisition function was used to select the next experiments."
        }
    ],
    "Results and Discussion": [
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "Feedback Signal Selection: From a practical standpoint, the key parameters of a laser-plasma X-ray source are the X-ray flux, source size, stability of the X-ray signal, and ease of use. In our setup, the continuous movement of the target ensures that its time of usage is practically unlimited. However, the X-ray yield linearly decreases over time due to the ablation of the target surface by femtosecond laser pulses, and beats of the rotating target introduce oscillations in the X-ray signal. The square root dependence of the second harmonic (SH) on X-ray flux provides an opportunity to utilize the SH as feedback for reinforcement learning. Checking the Operation and Stability of the Neural Network in the Sandbox: Initially, the neural network was tested without being connected to external sensors and control elements. The conducted performance evaluation demonstrated that the model maintains stability regardless of the initial positions and even when subjected to linear drift speeds up to four times higher than the training speeds. Coupling the Laser Pulse into a Fiber Using Reinforcement Machine Learning: The next modeling task involved coupling light into the fiber. For this purpose, the neural network was modified to perform movements in two coordinates. Figure 5 illustrates the results of the neural network's operation in two different regimes: one where the maximum amplitude was not initially specified, and one where it was. The best convergence and stability of the neural network are achieved when the feedback signal is initially different from zero. Stabilising the Intensity of the X-ray Source: When optimizing X-ray pulse generation, the main objectives were compensating for target oscillations and addressing the linear drift caused by target ablation. The conducted validation demonstrates a significant reduction in X-ray signal oscillations by 2-3 times and effective compensation of the linear trend."
        }
    ],
    "Conclusions": [
        {
            "paper_title": "Self-Adjusting Optical Systems Based on Reinforcement Learning",
            "source_filename": "Mareev et al. - 2023 - Self-Adjusting Optical Systems Based on Reinforcement Learning.pdf",
            "content": "In conclusion, the optimization of laser-plasma X-ray sources using reinforcement learning techniques has shown promising results in improving stability and performance. Through the adjustment of the laser focus position, the X-ray signal output can be enhanced and stabilized. The incorporation of reinforcement learning algorithms allows adaptive optimization, compensating for target oscillations, linear drift, and other effects that may impact X-ray signal generation. The validation process demonstrated the effectiveness of reinforcement learning in reducing X-ray signal oscillations and compensating for linear trends during laser ablation. By carefully selecting learning parameters and ensuring a sufficient number of training iterations, stability and robustness can be achieved. Further research and experimentation in this field hold great potential for advancing the performance and reliability of laser-plasma X-ray sources, benefiting various applications such as material analysis, medical imaging, and industrial processes."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "This paper reports on an AI-driven system to achieve auto-alignment of nanofocusing KB-mirror systems within next-generation synchrotron radiation beamlines. Through the intricate development and comprehensive study of an ultra-realistic digital twin representing two distinct beamlines, we have systematically explored and refined the control system, verifying its applicability in real-world scenarios. Our successful experimental demonstration at a X-ray beamline using real focusing KB mirrors highlights the system's capability to create a focused beam and mold beam structures into specific shapes. Marking one of the pioneering steps towards complete automation of future beamline operation and optimization, this work aligns seamlessly with our previous studies that employed ML-based control systems for multi-variable adaptive mirror control. Together, these advancements forecast a significant breakthrough in beamline automation, unlocking scientific research avenues previously out of reach. This study not only confirms the feasibility of the new method but also establishes a robust foundation for anticipated future enhancements. We expect diligent refinement and continued testing, particularly in cutting-edge research domains such as Bayesian optimization and reinforcement learning, will unveil even more capabilities. These developments will serve as crucial elements in the toolkit of modern technological innovation, with potential impacts that could reshape our understanding of optics and automation. As a worth mentioning example, this work represents a significant step towards more complex applications, such as autonomous mirror-based zoom optics systems that can dynamically change focal spot sizes for in-situ experiments, such as electrochemical dissolution and growth, vital in fields like electrocatalysis, synthesis, and corrosion. Such dynamic control, adapting to sample sizes changing on a minute scale, demands optimization of many parameters—an almost impossible task manually. The algorithm developed here not only guides the optics to an optimal setup but maintains it, offering potential gains in accuracy and reliability for ML-driven controllers of Adaptive Optics (AOs) in correcting residual aberrations. This research paves the way for new capabilities in coherent nanoprobe beamlines, enabling experiments that track sub-nanometer structural evolution for various applications."
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "It is critical to the development of future SDLs that studies include clear and precise efforts to quantify the capabilities of the presented platform. Without more deliberate and thorough evaluation of SDLs, the field will lack the necessary information for guiding future research. However, due to the inherently different challenges posed by each experimental space, there is a significant difficulty in comparing performance between systems by features such as optimization rate. Additionally, there is not a clear indicator to identify a fully optimized experimental space in high experimental cost problems. Instead, it is more effective to apply the criteria laid out in this perspective and include quantified data regarding the performance of the platform, software, and combined system. By doing so, the knowledge gap in the existing SDL literature will be better filled, and researchers can pursue quantifiably promising research directions."
        },
        {
            "paper_title": "Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning",
            "source_filename": "Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf",
            "content": "This study proposes an innovative liquid lens microscope system that achieves rapid and precise autofocus by utilizing deep reinforcement learning[cite: 1921]. Firstly, a liquid lens driven by the electrical dielectric wetting principle was fabricated, offering the advantages of small volume and fast response speed, which can effectively improve the structural compactness and zoom speed of microscopes when integrated[cite: 1922]. Secondly, an end-to-end autofocus is achieved by training a deep reinforcement learning model, further enhancing the focusing speed[cite: 1923]. Concurrently, a reward function tailored for the autofocus task was designed, enabling the model to focus more rapidly and autonomously[cite: 1924]. Furthermore, several action group design methods were introduced, which effectively enhance the speed and accuracy of autofocus by adjusting key parameters[cite: 1925]. In the experiments, an average of 3.15 steps was required to achieve autofocus, representing a 79% and 60.63% improvement in speed compared to traditional search algorithms[cite: 1926]. Additionally, a novel method for random sampling from multiple \"state\" dataset lists was proposed to address the limitation of model sensitivity to only the trained data[cite: 1927]. By increasing the number of state datasets, the model's ability to extract common features was significantly enhanced, enabling reliable autofocus across different samples and fields of view[cite: 1928]. With the expansion of the state datasets to 50, the model achieved a 97.2% success rate, with a root mean square error (RMSE) of 2.85 x 10^-3 V for the predicted voltage on the test set[cite: 1929]. This result demonstrates that the trained agent developed a robust autofocus strategy that is not dependent on the training data, thereby improving its generalization capability[cite: 1930]. The proposed liquid lens microscope system utilizing DRLAF significantly simplifies the structural complexity, enhances system compactness, reduces operational difficulty, and increases focusing speed[cite: 1931]."
        }
    ],
    "Data-Driven Microscopy": [
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Data-driven microscopes can analyse imaging data in real-time and execute predefined actions upon specific triggers. These reactive systems feature feedback loops between quantitative image analysis and microscope control, which allows them to tailor data acquisition to objects or phenomena of interest. Specifically for event-driven approaches, this trigger can be based on detecting the occurrence of a specific event. By implementing the prediction of states of interest even smart or intelligent microscopy approaches can be realised. A recent work showcasing event-triggered protocol switching is by Oscar André et al. (2). They, for example, performed dual-scale imaging of host-pathogen interactions using a co-culture model. The system first continuously scanned multiple fields of view of the sample at low magnification to monitor interactions between fluorescently labelled human cells and bacteria. An integrated algorithm analysed each frame to detect interaction events based on proximity analysis. Upon detecting a target number of cell-bacteria interactions, the system automatically switched to a higher numerical aperture objective and acquisition speed and imaged the identified interactions. This allowed to capture the cellular actin remodelling induced by the infection at high temporal-spatial resolution. This dual-scale approach balances population-level behavioural monitoring and targeted high-resolution data collection in a highly efficient and high-content manner. In super-resolution microscopy, data-driven strategies help mitigate inherent trade-offs between resolution, speed, field of view and phototoxicity during live imaging. A system by Jonatan Alvelid et al. (3) combines fast widefield surveillance with precisely targeted nanoscopy imaging. For instance, cultured neurons expressing genetically encoded calcium indicators were continuously monitored with widefield imaging to detect neuronal activity. Real-time analysis of calcium dynamics allowed the detection of spike events and localisation of regions of interest. Upon spike detection, the system rapidly positioned and activated Stimulated Emission Depletion (STED) nanoscopy illumination at identified sites to visualise synaptic vesicle dynamics. By limiting high-intensity light exposure spatially and temporally only when critical events occurred, this selective super-resolution imaging approach reduced cumulative photon dose by over 75% compared to continuous STED acquisition. Beyond adjusting microscope hardware, data-driven systems can coordinate external experimental devices by integrating microfluidics control software. An automated live-to-fixed cell imaging platform called NanoJ-Fluidics, developed by Pedro Almada et al. (4), performs buffer exchange directly on the microscope stage. The system uses simple epifluorescence image analysis to detect cell rounding at the onset of mitosis. Upon rounding detection, NanoJ-Fluidics triggers fixation, permeabilization, and fluorescent labelling through sequential perfusion, preparing the cells for subsequent super-resolution imaging. These examples showcase the reliance on traditional image analysis techniques to identify events of interest, which typically involve signal colocalization, intensity, or shape thresholds. However, the integration of machine learning-based image analysis can elevate reactive data-driven microscopy to a new level by enabling the detection of subtle and complex features that would otherwise go unnoticed."
        }
    ],
    "Machine learning for automated microscopy image analysis": [
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Recent advances in machine learning, particularly in deep learning neural networks, have revolutionised automated image analysis for microscopy (5, 6). By training on a sufficient amount of data, machine learning models can achieve or surpass human performance in complex image processing tasks such as cell identification, structure segmentation, motion tracking, and signal or resolution enhancement. Different models excel in various aspects crucial for enhancing microscopy imaging experiments. In this section, we will introduce fundamental machine learning concepts and highlight learning strategies well-suited for microscopy imaging tasks. Machine learning involves algorithms that learn patterns from data to make predictions without explicit programming. It falls under the umbrella of artificial intelligence, aiming to imitate intelligent behaviour. Through a training process, the algorithms tune the parameters of a specific image processing model to perform one particular task. Thus, machine learning practice requires training data, validation data and test data. The latter two dataset are used to evaluate the performance of the model during and after the training, respectively. Upon a positive quality evaluation result, the model can be used in new unseen data to make the predictions. In supervised learning, the model is trained on matched input and output examples, like images and labels, to infer general mapping functions. Unsupervised learning finds intrinsic structures within unlabelled data through techniques like clustering. A relatively simple but powerful machine learning model is the support vector machine (SVM) (7). SVMs excel at classification tasks such as identifying cell types in images. SVMs plot each image as a point in a multidimensional feature space and tries to find the optimal dividing hyperplane between classes. New images are classified based on which side of the hyperplane their features fall on. SVMs have good generalisation ability provided that the features extracted from the classes are descriptive enough as to characterise them. In microscopy, SVMs are often used for initial proof-of-concept experiments to classify images into binary categories like mitotic or non-mitotic cells. Their simplicity makes SVMs convenient for implementing basic feedback loops, for instance, triggering high-resolution imaging when a specific cell type is detected. Deep learning models including convolutional neural networks (CNNs) are state-of-the-art for complex image processing tasks. CNNs are made up of artificial neurons trained to recognise patterns in image data. One of the most influential CNN architectures is the U-Net (8), which was first introduced in 2015. U-Nets have encoder layers that capture hierarchical contextual information and down sample the data, and decoder layers which rebuild the information back into a detailed map using information from the encoder path passed through the skip connections. Compared to SVMs, U-Nets can handle raw images by automatically extracting a rich feature representation, and therefore, it performs better on datasets with increased complexity. In microscopy, U-Nets excel at segmentation tasks like identifying and delineating different cell types, nuclei or components in the image. Their ability to recognise complex structures based on contextual understanding of images makes U-Nets well-suited for implementing data-driven microscopy feedback loops. For example, U-Nets could be used to alter illumination or magnification when specific cellular structures are detected. An additional powerful class of machine learning approaches gaining traction in microscopy are generative adversarial networks (GANs) (9). GANs contain paired generator and discriminator networks trained in an adversarial manner. The generator creates synthetic images to mimic real data, while the discriminator classifies images as real or fake. Competing drives the generator to produce increasingly realistic outputs. In microscopy, GANs are applied for data augmentation, image enhancement, modality translation, and simulation. For instance, GANs can create diverse training data, convert brightfield to fluorescence-like images, or simulate images under different conditions. A major benefit of GANs is that it can be unsupervised and thus no labelled or paired datasets are needed to train them. For smart microscopes, GANs could enable pre-processing loops to improve image quality before analysis and provide an estimate of variability or confidence in the generated results to prioritise tasks. They also hold promise for predicting nanoscale information to guide super-resolution imaging. Machine learning provides data-driven microscopy with flexibility and empowers faster and more adaptative imaging workflows. Trained models extract relevant information from images that is then used to optimise data collection by adapting microscope parameters accordingly in real-time. This transformative potential has been demonstrated across diverse imaging modalities, as highlighted in the next section."
        }
    ],
    "Applications of machine learning powered reactive microscopy": [
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Artificial intelligence is driving the development of intelligent microscopes that can sense, analyse, and adapt in real-time. Recent innovations have demonstrated reactive imaging systems across various modalities, ranging from widefield to super-resolution microscopy. In this section, we will discuss the key applications of machine-learning-powered reactive microscopy, highlighting the potential of these systems to revolutionise optical imaging. MicroPilot (10), a software that provides a framework for data-driven microscopy, is one of the pioneering works in the field. The system is based on LabView and C for image analysis and can be implemented in various commercial systems. It is also compatible with Micro-Manager (11), an open-source tool widely used to control microscopes. The MicroPilot study provides different examples of how cells can be monitored in low-resolution mode, and images can be analysed using a SVM trained to classify different mitotic stages. A complex experiment is triggered once a cell in a desired stage is detected. After completion, the imaging returns to scanning mode until the next detection. To demonstrate its capacity, an experiment was conducted to study the potential role of a specific protein in the condensation of mitotic chromosomes. The experiment monitored 3T3 cells and it triggered Fluorescence Recovery After Photobleaching (FRAP) acquisitions upon identification of each prophase cell. Half of the nucleus was selectively photobleached, and the signal recovery was monitored. It is worth noting that this set of experiments was completed in just four unattended nights, generating results equivalent to what would have taken a full month for an expert user. Building on this concept, MicroMator (12) was developed to offer an open-source toolkit for reactive microscopy using Python and Micro-Manager. It includes pre-trained U-Net models to segment yeast and bacteria cells. Researchers applied MicroMator to selectively manipulate targeted cells during live imaging. One noteworthy example involves an optogenetically-driven recombination experiment in yeast. In this experiment, genetically modified yeast cells are selected for exposure to light, triggering recombination and the subsequent expression of a protein that arrests growth together with a fluorescent protein for monitoring purposes. To generate islets of recombined yeast, MicroMator's algorithm individually selects yeast cells at a minimum distance apart, tracks them and repeatedly triggers light exposure on them, increasing the chances of recombination and, thus, the amount of relevant information in the acquired data. In addition to enhancing data information density, the image quality can be dynamically optimised based on the sample properties. One example of this is the learned adaptive multiphoton illumination (LAMI) (13) approach, which uses a physics-informed machine learning method to estimate the optimal excitation power to maintain a good SNR across depth in multiphoton microscopy. This becomes particularly relevant for non-flat samples with varying scattering regions. Given the surface characteristics of the sample, LAMI selectively adjusts the excitation power where needed, effectively expanding the imaging volume by at least an order of magnitude, while minimising the potential for photodamage effects. The effectiveness is also demonstrated by observing immune cell responses to vaccination in a mouse lymph node with live intravital multiphoton imaging. Furthermore, LAMI significantly reduces computation time by incorporating a machine learning-based method instead of a purely physics-based approach. The computation time is reduced from approximately one second per focal time point to less than one millisecond. In a study conducted by Suliana Manley and her team, they aimed to image mitochondria division using Structured Illumination Microscopy while minimising photodamage effects (14). To achieve this, they trained a U-Net to detect spontaneous mitochondria divisions in dual-colour images labelling mitochondria and the mitochondria-shaping dynamin-related protein 1. The model was integrated into the imaging workflow to trigger interchangeably the acquisition mode from a slow imaging rate, suitable for live-cell observation, to a faster imaging rate, enabling the collection of higher time-resolved data of mitochondrial fission. Interestingly, the similarity in the morphological characteristics and protein accumulation at fission sites allowed the network to be repurposed for detecting fission events in the bacteria C. crescentus without additional training. The research team quantitatively assessed and compared photobleaching decay among the slow, fast, and event-driven acquisitions. When compared to the fast mode, they observed a significant reduction in photobleaching using the event driven acquisition mode, thereby extending the duration of imaging experiments. As expected, this reduction is not as big as with the slow acquisition mode, but it comes with the benefit of capturing the event with higher temporal resolution and thus the measurement of an average smaller constriction diameters that would have been otherwise missed. Lastly, the work of Flavie Lavoie-Cardinal's research group focuses on capturing the remodelling process of dendritic F-actin, transitioning from periodic rings to fibres, within living neurons with STED imaging (15). They monitor cells with confocal microscopy based on which synthetic STED images are generated, considerably reducing photodamage effects. For this, they employ a task-assisted generative adversarial network (TA-GAN). TA-GAN's training is strengthen by also considering the error of actin ring and fibres segmentation in the synthetic images. During acquisition, the system estimates the uncertainty of the model predicting synthetic images to decide whether to initiate a real STED image acquisition and fine-tune the generative model if needed. This allows to track actin remodelling in stimulated neurons at high accuracy and resolution. Their results illustrate that this strategy can potentially reduce the overall light exposure by a significant margin, up to 70% and importantly, they manage to acquire biologically relevant live super-resolution time lapse images for 15 minutes. By integrating machine learning into the microscopy workflow, researchers have showcased techniques to enhance data quality and quantity while minimising phototoxicity. The applications highlighted in this section demonstrate the transformative potential of machine learning-powered microscopes across diverse imaging modalities and biological questions. As machine learning methods and computational power continue advancing, we can expect even more breakthroughs in intelligent microscopy, bringing us closer to the goal of fully automated, optimised imaging platforms that accelerate biological discovery."
        }
    ],
    "Conclusions and outlook": [
        {
            "paper_title": "The Rise of Data-Driven Microscopy powered by Machine Learning",
            "source_filename": "Morgado et al. - 2024 - The rise of data‐driven microscopy powered by machine learning.pdf",
            "content": "Data-driven microscopy has demonstrated impressive capabilities in optimising illumination, modality switching, acquisition rates, and event-triggered imaging. These approaches improve image acquisition's efficiency and information content, enabling studying dynamic biological processes across different scales. Intelligent microscopes offer new experimental possibilities, from observing rare neuronal activity at the nanoscale resolution to studying immune cell dynamics in tissues. However, realising the full potential of data-driven microscopy requires addressing technical and practical challenges. One major limitation is the need for robust and accurate machine learning models, especially when dealing with small microscopy datasets. Expanding open-source repositories of annotated images and simulations can facilitate the development and validation of new algorithms. Additionally, incorporating unsupervised and self-supervised techniques shows promise in overcoming the scarcity of labelled data. Another critical aspect is the design of microscope hardware optimised for data-driven imaging. Retrofitting analysis and control modules into traditional systems is common, but purpose-built instrumentation that integrates software, optics, detectors, and automation is essential. For example, spatial light modulators can enable rapid adaptable illumination for optimal signal-to-noise ratio across different samples. On the detection side, high-speed, low-noise cameras or point-scanning systems tailored for live imaging can enhance acquisition speeds. In order to increase the use of data-driven microscopy software, it needs to be made more user-friendly and accessible. This can be achieved by creating simplified interfaces for designing and executing reactive imaging experiments, allowing non-experts to take advantage of these advanced methods. Expanding open-source platforms like Micro-Manager will encourage community contributions and drive innovation. Additionally, package managers, such as BioImage Model Zoo (16), ZeroCostDL4Mic (17), and DL4MicEverywhere (18), that facilitate the sharing and installation of pre-trained models can help overcome barriers in deploying machine learning solutions. As data-driven microscopy moves beyond proof-of-concept studies, ensuring the robustness and reproducibility of autonomous microscopes becomes crucial. Maintaining image quality control and detecting failures during unsupervised operation for extended duration is challenging. Detailed performance benchmarking across laboratories using standardised samples can help identify best practices. While this approach can be a great asset in minimising user bias, a selection bias in decision making can still arise. Here, extensive validation of machine learning predictions and adaptive decisions is required to build trust in intelligent systems. Data-driven microscopy represents a new era for optical imaging, overcoming inherent limitations through real-time feedback and automation. Intelligent microscopes have the potential to transform bioimaging by opening up new experimental possibilities. Pioneering applications demonstrate the ability to capture dynamics, rare events, and nanoscale architecture by optimising acquisition on-the-fly. While challenges in robustness, accessibility, and validation remain, the future looks promising for microscopes that can sense, analyse, and adapt autonomously. We envision data-driven platforms becoming ubiquitous tools that empower researchers to image smarter, not just faster. The next generation of automated intelligent microscopes will provide unprecedented spatiotemporal views into biological processes across scales, fuelling fundamental discoveries."
        }
    ],
    "Bayesian optimization": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Consider an expensive-to-sample black-box function f(x) with d-dimensional inputs x. In finding the right input x to achieve the maximal value of f(x), it is untenable to utilize optimization methods that rely on lots of function samples. We can address this by treating the function as a stochastic process (which describes a distribution over all possible realizations of the function) and using Bayesian inference to construct a posterior distribution p(f), i.e. describing how likely it is that every possible function f is the true function. If we sample the function at points x = {x_1, x_2, ..., x_n} and observe values y = {f(x_1), f(x_2), ..., f(x_n)}, then we can use Bayesian inference to write our posterior belief about f given that we observe x and y as p(f|x, y) = [p(y|f, x)p(f)] / p(y|x), where p(y|f, x) is the likelihood, p(f) is the prior, and p(y|x) is the marginal likelihood. Each iteration of Bayesian optimization then consists of three steps: (i) Estimate the posterior p(f|y, x) from some historical observations (x, y). (ii) Use the posterior to find the most desirable point x* within some predefined bounds. (iii) Sample that point and add it to our historical observations."
        }
    ],
    "Gaussian process models": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "A GP is a stochastic process where every collection of variables y has a multivariate normal distribution; for notational simplicity and without loss of generality, we assume throughout this paper that all of our processes are zero mean. The GP is described entirely by the covariance matrix Sigma describing the observations y. A GP model consists of assigning a covariance matrix to a set of sample data y at inputs x and computing the posterior mean and posterior variance at every other input. In practice, the covariance of the process is not known a priori and is approximated by constructing and fitting a kernel. To construct our kernel, we take the hyperparameters which maximize the marginal likelihood. Once we have our kernel K(x_i, x_j, theta) and optimized hyperparameters theta*, we can use GP regression to construct posteriors. It may be the case that our observations are noisy, i.e. that observing the function at points x will yield y = f(x) + epsilon where epsilon is a random noise term. If we assume that epsilon is homoskedastic and Gaussian, then we can account for the noise by adding a constant noise variance sigma^2 to the diagonal of the kernel K."
        }
    ],
    "Acquisition functions": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "The acquisition function A(x) is a model of a given objective over possible inputs which, given a posterior p(f|x, y), quantifies the desirability of sampling a given input x. For each iteration of the optimization, we optimize the acquisition function over the inputs. Acquisition functions can be either analytic or non-analytic. Analytic acquisition functions are directly computable from the posterior; as the posterior for a GP is determined entirely by the mean mu and variance sigma, they may be expressed as A(x) = f(mu(x), sigma(x)). The simplest example is the expected mean, where on every iteration the algorithm will sample the point with the largest expected mean. A less risk-averse example is the expected improvement, which is our expectation for how much the cumulative maximum f* will increase if we were to sample x. Some useful acquisition functions cannot be computed directly from the mean and variance of the posterior. Acquisition functions that involve sampling from the posterior to estimate some ensemble are more flexible and often more robust. One example of this is in selecting multiple points, as in when we want to find the best n points to sample given some analytic acquisition function A(x). We address this interdependence with a Monte Carlo acquisition function, where we might evaluate the acquisition of some collection of points by sampling from the posterior and taking an ensemble average of the result."
        }
    ],
    "Beamline-specific considerations": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "In this section, we look at beamline-specific considerations that improve the practical application of Bayesian optimization to the automated alignment problem. We consider the common optimization problem of maximizing the beam power density. Input parameters for beamlines can be highly coupled. In fitting GPs to beamline data, we adopt a kernel of the form k(x_i, x_j, theta) = f(|D exp S(x_i - x_j)|), where f(r) is some radial function, D is a diagonal matrix, and S is a skew-symmetric matrix. The application of Bayesian optimization relies on reliable diagnostic feedback, which is often not a realistic assumption for real-life scenarios. Undesirable behavior in the diagnostics can occur both sporadically or systematically. We use the classification method outlined by Milios et al. (2018) which fits a Dirichlet distribution to the data from which we can generate class probabilities. Using this probability, we can weight any objective-based acquisition function to prefer inputs that lead to valid outputs. Bayesian optimization is particularly useful when sampling the objective function f(x) is expensive. This is strictly true for some beamlines where computing a diagnostic is expensive. Many beamlines, though, have no latency in the diagnostics and are only expensive to sample because they are expensive to move around. Another challenge to machine learning-based optimization is hysteresis, which manifests at beamlines when the actual position of some input varies from the desired input. Even though we combine estimates of the different beam attributes into a scalar fitness to be maximized, it is still beneficial to construct and train three separate models for the flux, horizontal spread and vertical spread, a method typically referred to as composite optimization."
        }
    ],
    "Implementation": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Our beamline alignment tools are implemented in the Blop Python package, relying on the BoTorch Python package. In Blop we develop a customized kernel which fits to latent beamline dimensions and weight common acquisition functions by the probabilistic constraint. We also use BoTorch for model fitting and acquisition function optimization. The algorithm is used in terms of an agent, which we instantiate with motors and diagnostic equipment. We can 'tell' the agent about the values of pre-defined objectives and 'ask' it for new points to sample. The agent wraps the steps of Bayesian optimization into a single customizable routine. We have designed Blop with Bluesky in mind, as it can use Bluesky to automatically take data, analyze it and optimize the inputs with the same feedback and control systems used for beamline experiments."
        }
    ],
    "Experiments": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Alignment of a Kirkpatrick-Baez mirror system on the TES beamline: We optimize for the flux density on the sample by allowing each K-B mirror and the toroidal mirror to pitch and translate into and out of the beam for a total of six degrees of freedom. Alignment of a Johann spectrometer on the ISS beamline: Maximizing the flux on the area detector maximizes the resolution of the spectrometer and so we seek to colocate the reflections of the crystals onto the same point. We use three crystals to focus the beam onto a two-dimensional area detector. Photon transport optimization on the Advanced Light Source beamline 5.3.1: The photon transport system comprises a first focusing mirror, a monochromator and a few apertures. Using the described automated alignment, we were able to maximize the power density on the sample in under 5 min, with a final beam size of 1 mm x 0.3 mm (horizontal x vertical, FWHM). Alignment of an electron beam at the Accelerator Test Facility: We modulate three bending quadrupole electromagnets and a solenoid to manipulate the shape of the beam, for a total of four degrees of freedom. Simulated alignment of the TES beamline: We use digital twins of beamlines using the Sirepo-Bluesky back end, allowing us to optimize the beam with the same Bluesky-based code used to align real beamlines. The eight-dimensional optimization of the simulated TES beamline shows the benefit of using both latent inputs and composite outputs."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Here, we present the results of several experiments performed with PO4AO on GHOST to explore its performance for high-level conditions relevant to operational on-sky AO. Specifically, we explore the impact of temporal delay, robustness for low S/N, ability to cope with misregistration, and the effect of history length on performance. In all experiments, the PO4AO is compared against an integrator whose gain is adjusted to minimize WFS residuals. The GHOST laboratory AO system has been built to evaluate new AO control techniques. It utilizes a simple single-source on-axis setup equipped with a pyramid WFS and a Boston micromachines deformable mirror (DM-492). A programmable spatial light modulator (SLM) introduces turbulence with high spatial resolution. To simulate a faster second-stage system using GHOST, we generated residual phase screens numerically for the lab setup. This exact set of residual turbulence phase-screens is then replayed by the SLM for all our GHOST experiments. In the time delay experiment, we ran PO4AO on three different temporal delays (0, 1, and 2 additional frames). PO4AO outperforms the integrator after the warm-up of 20 episodes, providing around a factor of three improvement in reconstructed wavefront variance for all delays. In the low flux experiment, we set the light source such that the flux was around 6000 camera counts/frame (S/N ~ 2). PO4AO considerably reduces the photon flux inside the control radius compared to the integrator. In the vibration reduction experiment, we studied the effect of the number of past telemetry frames. The PO4AO correction performance improves with the number of history frames considered. We observed a vibration spike at 16 Hz; PO4AO with 128 history frames dampens the spike. In the misregistration experiment, we introduced various degrees of misregistration by shifting the DM off-axis. PO4AO is able to obtain stability and performance with dynamic misregistration while the integrator gets unstable. Finally, we discuss the latency introduced by the method. The total delay budget accounts for pipeline latency and the Python implementation delay, including CNN forward pass, data handling, and state updates."
        }
    ],
    "Further development and discussion": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "We have applied the same automated alignment tools to several different facilities and have shown that the same Python package can effectively align a range of beamlines. Further refinement of these automated alignment tools will involve applying them to more beamlines at more facilities, with different flavors of optimization problems. How practical automated alignment can be necessitates an intuitive graphical user interface, from which the configuration of the optimizer is easy to understand. Further development also includes the implementation of new features and better performance in the software. The enabling of Pareto efficient optimization would give the beamline scientist more control over the beam quality. We also plan to allow for a decentralized agent, which can run on a high-performance computing server and communicate with the control system using a streaming system like Kafka and feed back to the experiment control using Bluesky-Queueserver. Fly scanning, the strategy of sampling while moving parameters, presents the potential to speed up beamline alignment. We also note that the largest obstacle to applying automated alignment to existing beamlines is the difficulty in constructing robust feedbacks, as many beam diagnostics have non-negligible backgrounds or malfunctioning pixels. This suggests the benefit of more sophisticated diagnostic methods, using machine learning techniques like image segmentation."
        }
    ],
    "A 1.9": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 9: Four different electron beam configurations at the Brookhaven National Laboratory ATF at different stages of automated alignment, where the upper left-hand panel shows the starting beam and the lower right the optimal beam. In this alignment test, we tune the current of four quadrupole electromagnets to maximize the objective in equation (23)."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 9: Learning curves for different history lengths. We plot the cumulative reward after each episode for all history lengths after the warm-up phase. All history lengths have the same warm-up phase (not included in the plot). The longer the time length is, the better the performance."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 9: MOBO auto-alignment for the 28-ID-B digital twin. (a) The manually tuned (M) focused structure. (b) The initial structure obtained by randomly shifting the parameters. (c) The Nash solution (NS) structure selected after the optimization. (d-e) Other structures in the Pareto front. Subplots (b-e) are similar to the corresponding subplots in Fig. 8, only with the addition of the peak intensity count (pl) information in the text boxes. (f-h) 2D projections of the 3D scatter plot of the optimization objectives with the colors indicating the trial number."
        }
    ],
    "A 1.10": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 10: The eight-dimensional optimization of the simulated TES beamline, where the degrees of freedom comprise the toroidal and Kirkpatrick-Baez mirrors. The colors show different varieties of Bayesian optimization algorithms both with and without latent inputs and composite outputs, with both the cumulative maximum of all individual runs (thin lines) and the median cumulative maximum (thick line). Each variety starts out with a quasi-random sampling of 32 points (shaded light blue) and then performs a Bayesian optimization loop with the expected improvement acquisition function. The benefit of using both latent inputs and composite outputs is shown, as we can achieve a better optimum more robustly and more quickly."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 10: Temporal PSD of mode #1 (tip) on different history lengths."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 10: MOBO auto-alignment experimental validation at the 28-IDB beamline. (A) The initial structure, (B) the Nash solution, and (C-E) the 2D projections of the 3D scatter plot of the optimization objectives, with the colors indicating the trial number and the circled points indicating the structures in the Pareto front."
        }
    ],
    "A 1.11": [
        {
            "paper_title": "A general Bayesian algorithm for the autonomous alignment of beamlines",
            "source_filename": "Morris et al. - 2024 - A general Bayesian algorithm for the autonomous alignment of beamlines.pdf",
            "content": "Figure 11: The four-dimensional optimization of just the K-B mirrors, whose motors are each misaligned by up to 0.05 mm. After an initial quasi-random sample of 16 points (shaded light blue), the agent is able almost instantly to return to the optimal alignment."
        },
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "Fig. 11: The misregistration experiment. Here, we plot the cumulative loss over an episode during the misregistration experiment. The blue line is the well-tuned integrator calibrated with centered DM, and the green line is the well-tuned integrator calibrated with DM 120 microns off-axis. The red line is the PO4AO calibrated and pre-trained with centered DM. The dashed black lines indicate the moment when DM was manually shifted. Since the shifting was done manually, the gray areas around the line indicate the uncertainty of the exact moment."
        },
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Fig. 11: Alignment to a reference structure. (a) The random initial structure, (b) the Nash solution, and (c) the remaining structure in the Pareto front. (d) The change in the optimization objective through the optimization trials."
        }
    ],
    "Related Work": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "PO4AO addresses the predictive control and reconstruction in the XAO control loop as a single reinforcement learning (RL) problem; hence, PO4AO is related to many aspects of XAO control, such as predictive control, optimal gain compensation, misregistration identification, reconstruction algorithms, and vibration canceling. Remarkable progress has been achieved with various approaches to tackle the XAO control problem. These methods include the Kalman filter-based linear controllers, sometimes combined with machine learning for system identification. These methods rely on linear models for wavefront sensing and temporal evolution to obtain a state estimation of the system. Other methods focus on correcting temporal error and vary from spatio-temporal linear filters to filters operating on single modes, such as Fourier or Zernike modes. The predictive filters are obtained either from modeling or utilizing data analysis/machine learning. Some methods have also been tested on-sky. Moreover, machine learning methods utilizing neural networks (NNs) for predictive control have been studied, where NNs show a lot potential, especially for AO systems with high number of degrees of freedom (DoF), and in noisy conditions. Lately, NNs, and the more modern deep NNs, have also been used for the wavefront reconstruction step. The results indicate that NN reconstruction is less sensitive to non-linearity and increases the operational range of the pyramid WFS. Finally, different NN-based RL approaches have been studied during the last years. PO4AO differs from other RL methods in AO literature by using so-called model-based RL instead of model-free RL (for a discussion on the difference between these methods, see Ref. 11). For interested readers, Fowler and Landman provide a more thorough review of machine learning methods for wavefront control and phase prediction."
        }
    ],
    "Classical Adaptive Optics Control and Baseline Controller": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "An AO system is commonly controlled with a linear integrator controller, referred to as the integrator. We consider it our reference method against the PO4AO as it is still widely used in AO. Integrator control in AO usually relies on the so-called interaction matrix mapping DM commands to WFS measurements. Once the interaction matrix is estimated, the inverse problem, i.e., reconstruction v given Delta w, needs to be considered. As D is generally not invertible, some regularization approach is needed. Here, we restrict ourselves to linear methods described by a reconstruction matrix C mapping WFS measurements to DM commands. As our regularization method, we project D to a smaller dimensional subspace spanned by the Karhunen-Loève (KL) modal basis. Each KL mode in the basis has a representation in terms of actuator voltages. This relation is fully determined by a transformation matrix mapping DM actuator voltages to m first modal coefficients. The regularized reconstruction matrix is now defined by the Moore-Penrose pseudo-inverse. The number of modes m defines stability at the cost of resolution; smaller m results in lower noise amplification while producing a reconstruction with fewer modal basis functions (less detailed reconstruction). An optimal m balances the error produced by these two effects. We use the leaky integrator as a baseline AO controller to which PO4AO is compared. At a given time step, the WFS measures the residual wavefront. The leaky integrator then obtains the new control voltages. The DM saturation can cause a build-up of modes outside the control radius. Hence, introducing a leakage typically chosen near one, e.g., 0.99, in the DM commands commonly used to remove those unseen modes and increase robustness."
        }
    ],
    "Adaptive Optics as a Markov Decision Process": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "In this paper, we model AO control loop as a Markov decision process (MDP), which is the de facto mathematical framework for sequential decision problems in RL. In AO control, instead of the full state of the system (exact DM shape, the full atmosphere profile, etc.), we only observe WFS data, that represents only a partial information of the whole system. These kinds of processes are referred to as partially observed MDPs in the RL literature, and, in theory, optimal decisions (control) should consider all the past measurements observed (from the beginning of operation). However, we expand the state space to include a history of WFS measurement and DM control voltages to guarantee approximately Markovian statistics and treat the process as an ordinary MDP, where optimal decisions can be taken directly from the previous state formulation (in our case a concatenation of past measurement and actions). We identify an action as the applied differential voltage. We now define the state s_t of the MDP as a sequence of observations and actions. We assume that there exist Markovian transition dynamics where the only new element of the next state is the next observation. This transition model contains information on the time delay, misregistration and non-linearity errors, and atmospheric turbulence. Further, in the following formulation of the control algorithm, the initial reconstruction matrix serves as preprocessing to observation and does not connect measurement to action directly. As the reward function of the MDP, we consider the negative Euclidian norm of the residual wavefront observed through the WFS with a regularization term that favors small actions. The addition of the regularization term effectively regularizes the control algorithm, making it less prone to saturation and oscillation, especially early in the training procedure."
        }
    ],
    "Model-Based Policy Optimization": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "The key idea of PO4AO is to learn a non-linear control law that maps past telemetry to new DM commands from data collected from the AO loop and maximizes the reward. In RL terminology, this control law is referred to as the policy and will be formulated as a mapping from the current state to the next action. Hence, the policy combines the reconstruction and control steps in AO. In this work, the policy is constructed as a NN, and its parameters are derived indirectly via model-based policy optimization. More precisely, the method collects data to learn a dynamics model that is also represented by an NN and can be used to predict the subsequent state given the current state and an action. The dynamics model is then used to optimize the policy. Both NNs are fully convolutional NN (CNN) with three layers. The method first runs the so-called warm-up phase, where an initial data set is collected by injecting random control signals into the control system, followed by training the involved NN models. The warm-up phase aims to ensure rough estimates of policy and dynamic NN and, consequently, stabilize the training procedure in the beginning. After the warm-up phase, the method iterates three phases: (1) Running the policy, (2) Improving the dynamics model, and (3) Improving the policy. Phases 2 and 3 are run in parallel to phase 1. The dynamics model is trained via a supervised learning objective, minimizing the squared difference between true next states and predictions. The policy is optimized to maximize the expected reward within a planning horizon H, utilizing the dynamics model to simulate the future."
        }
    ],
    "PO4AO Implementation and Hyperparameters": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "PO4AO has a lot of free adjustable parameters (see Table 1). We arrange hyperparameters under four different subcategories: Reinforcement Learning Parameters, Training Parameters, Markov Decision Process Parameters, and Replay Buffers. Reinforcement Learning Parameters set the frequency on which the policy NN is updated. The episode length is the number of frames in an episode; a single training procedure is run during the episode. The warm-up length determines how many episodes are run in the warm-up phase. The initial and minimum warm-up noise set the range for the noise added during warm-up, which is reduced linearly. The loss function penalty parameter alpha defines the amount of regularization in the reward function. Training Parameters set the number of gradient steps in dynamics and policy optimization. After the warm-up phase, the loop is suspended, and the first training procedure is run. After the first training iteration, the loop is closed with policy, and the parallel training procedure is started. The mini batch size is the number of data points used to calculate a single gradient step. MDP Parameters specify the MDP formulation. The number of history frames decides the number of past measurements in the MDP formulation. The planning horizon sets the future time window considered by the PO4AO. Replay Buffers include the warm-up and replay buffer. The warm-up buffer saves the data recorded during the warm-up phase. The newest data is added to the replay buffer, which keeps the latest replay buffer size episodes in memory. The mini-batch sampled during the training is sampled from the warm-up buffer with the probability set by the train warm-up percentage and otherwise from the replay buffer."
        }
    ],
    "Conclusion and Discussion": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "To conclude, this paper demonstrates that PO4AO is a robust controller for a second-stage AO system in a lab simulation. RL is shown to mitigate several critical error terms in XAO control, such as misregistration, photon noise, and temporal error. Moreover, running PO4AO is a turnkey operation as the hyperparameters are tuned only when the method is implemented, and the method adapts automatically to changing conditions like noise level, misregistration, and wind profile. Extensive experiments on GHOST confirm that PO4AO can adapt to and mitigate these error terms on real hardware. In addition, we showed that PO4AO could also mitigate vibrations if it considers enough past telemetry frames. However, like most deep RL methods, PO4AO is somewhat sensitive to the choice of hyperparameters. Tuning the parameters can take time, but the method performs robustly under all conditions once a good combination of hyperparameters is found. The method did not improve the system's stability to more degrees of freedom. We observed that the integrator was stable to ~350 KL modes, and PO4AO did not enable us to control more KL modes robustly. Additionally, we open-sourced the Python implementation of PO4AO, which is well-commented and can be easily adapted to numerical simulations or real hardware. The paper also discusses the hyperparameters of PO4AO and how they affect the method. The open-source implementation introduces an additional ~700 µs to the pipeline latency, making it suitable for systems running lower than 1000 Hz. Various avenues exist to optimize the method further, such as transitioning to lower-level programming languages, using NVIDIA TensorRT, optimizing memory handling, and streamlining the pipeline."
        }
    ],
    "Appendix A: Implementation Details": [
        {
            "paper_title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
            "source_filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
            "content": "This section discusses open-source Python implementation. The code consists of three Python files: po4ao.py, po4ao_models.py, po4ao_util.py, and po4ao_config.py. The po4ao.py script includes the main function that starts the warm-up phase and training procedure and closes the loop. It includes the step-function that interfaces the Python code to the RTC pipeline. The file po4ao_models.py contains the NN models: the policy and the dynamics. The file po4ao_util.py includes utility functions, such as the implementation for replay buffers and shared memory optimizers. The configuration file po4ao_config.py includes all adjustable hyperparameters of the method. The parameters include integrator settings, NN model architecture parameters, and control delay parameters."
        }
    ],
    "Focusing optical systems": [
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "This section describes the two focusing optical systems employed to create and test the automatic AI-driven controller. The 34-ID-C beamline focusing system was first fully characterized to develop an accurate digital twin to study and identify the optimal strategy for AI implementation. We assembled a similar system at the 28-ID-B beamline for experimental validation and comprehensively characterized its digital twin. This allowed us to simulate the experiments, assess potential limitations, identify possible issues, and predict outcomes.\n\n34-ID-C Beamline: The 34-ID beamline at the APS hosts two experimental stations: the Laue diffraction microscopy instrument (34-ID-E) and the Coherent Diffraction Imaging instrument (34-ID-C). The characteristics of the undulator source and the beamline layout can be found in the Supplemental Document, Section S1.1. The monochromator was tuned to an energy of 10 keV using the 3rd harmonic of the undulator. Beamline 34-ID-C uses a pair of bendable Kirkpatrick-Baez (KB) mirrors as the final focusing optics shown in Fig. 1a. A set of slits is employed upstream of the KB mirrors to select the coherent fraction of the monochromatic beam. The typical spot size is ~500 x 500 nm² at the sample position, ~120 mm downstream of the end of the horizontally focusing mirror of the KB pair. A layer of Au nanoparticles (100-200 nm) deposited on a Si substrate is placed at the sample position to determine the focal spot size. The diffraction signal of the 111-family planes of the Au nanoparticle is captured by an area detector and integrated. Motors on the sample holder allow scanning in the transverse plane to provide the horizontal and vertical beam profiles.\n\n28-ID-B Beamline: The 28-ID-B beamline, the Instrumentation Development, Evaluation, & Analysis Beamline (IDEA) at the APS was designed to characterize the performance of state-of-the-art X-ray optics and devices planned for the APS Upgrade (APS-U) project. Additionally, this beamline serves as a testing ground to validate, optimize, and refine new optical concepts and control strategies proposed for APS-U beamlines. The source parameters and beamline layout is reported in the Supplemental Document, Section S1.2. The monochromator was tuned to an energy of 20 keV using the 5th harmonic of the undulator. In experimental hutch B, we assembled a KB mirror focusing system consisting of an outboard-reflecting, horizontally focusing bendable mirror using an in-house flexure bender design and an upward-reflecting PZT (lead zirconate titanate) bimorph mirror manufactured by JTEC Corporation (Osaka, Japan) for vertical focusing. The bender mirror is Pt coated on a silicon substrate, shaped in a trapezoidal form with dimensions measuring 300 mm in length, 36.28 mm at the wide end, 19.32 mm at the narrow end, and 12 mm in thickness. Bending moments are administered to both ends of the mirror through a flexure mechanism, with a piezo linear actuator on each side providing the force. To guarantee bending reproducibility, the positions of the bending arms are continually monitored by two capacitive sensors. The bimorph mirror is Pt coated with an active area of 150 mm (length) x 8 mm (width) on a silicon substrate with a dimension of 160 mm (length) x 50 mm (width) x 10 mm (thickness). Two piezoelectric strips, with 18 separate electrodes on each stripe, are glued on the top surface of the mirror, sandwiching the active area. Each pair of electrodes, at the same position along the strips, forms one actuator capable of modifying the local surface shape, thus forming a total of 18 local surface actuator channels (ch1-ch18). Two long piezo strips are glued to the bottom surface of the mirror, forming a single global bender actuator (ch20) capable of bending the entire optical surface. A grounded channel (ch19) is on the backside of all piezo stripes. In this study, we only utilized the global bender, keeping the surface actuators in a rest position by setting their channel voltages to 500 V. Both mirrors were operated at a grazing angle of 3 mrad. A scintillator-based X-ray detector was placed at the focal plane of the KB mirror, providing 2D images of the transverse profile of the beam. The focal spot was set at ~2.3 m from the vertical mirror center (~3.1 m from the horizontal one), with a typical size of around 10 x 2 µm²."
        }
    ],
    "Software driver for the optical systems and their digital twins": [
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "As a simulation engine, we adopted the kernel libraries of OASYS, to create the ultra-realistic digital twin of the two beamlines. The architecture has a command interface consistent across the real hardware drivers and the simulated digital twin. Fig. 3 shows the class diagram of the software for the focusing system of the 34-ID-C beamline, a schematic that was also adapted for the 28-ID-B beamline. The interface (34IDAbstractFocusingOptics) is implemented by both the hardware driver (right side of the diagram) and the digital twins (left side of the diagram, with different implementation corresponding to different simulation strategies). Both the simulated digital twins and the real hardware controller (implemented using EPICS) share a common interface for controlling the motors. From a software engineering perspective, the architecture follows a Facade design pattern that allows one to choose the specific implementation. Fig. 4 displays a portion of the Python code used to instantiate the focusing optics controller. Once the controller instance is obtained—with initial parameters varying between the digital twin and real beamline—it is used by the AI-driven controller without regard for the chosen implementation. The complete code is available in the public online repository. Within the OASYS suite, we used the ray-tracing program ShadowOui to design and prepare the software framework. The source radiation was simulated using the Shadow/SRW hybrid undulator simulator, which generates rays based on the spatial and angular radiation distribution calculated by the wave optics program SRW at a given energy. Then the radiation propagation simulation was carried out using several advanced features: i) The Hybrid method was employed to compute diffraction effects when the beam is clipped by an aperture or mirror length. It can also simulate the effect of figure errors in optical elements when diffraction occurs. ii) The metrology data for all mirrors involved were integrated using the DABAM online metrology database tools. The two-dimensional mesh of the measured surface error profile was added to the surface of each mirror. iii) Analytical models were implemented to generate surface profiles created by the different bender designs at the two beamlines, following the formulas detailed in references. To create a surface profile that closely approximates the ideal shape—such as an elliptical cylinder shape defined by geometrical parameters like source-to-mirror distance (p), mirror-to-focus distance (q), and the grazing angle (theta)—a non-linear least-square fitting process is employed. This process determines the bending forces necessary to match the ideal profile. Subsequently, the bender-induced surface error, representing the discrepancy between the achievable shape through bending and the ideal shape, is incorporated as an additional surface error profile. In addition, we needed to represent the behavior of the bendable mirrors under operating conditions, taking into account that the upstream and downstream bender motors can be activated separately, influencing only a portion of the beam (one-half each if the beam is properly aligned and centered on the mirror surface). To simulate this behavior, we shaped the mirror's upstream half based on the equation corresponding to the upstream motor setup and the downstream half according to the downstream motor setup. These analytical equations, defining the surfaces and corresponding focal positions, are generally related to the force or momentum applied to the mirror, with the focal position typically inversely proportional to the applied force. However, real controllers often operate on different units, such as voltages or distances, depending on the type of motors (e.g., piezometric or stepper motors) that drive the bending mechanism. To reconcile this discrepancy, we calibrated all involved mirrors by measuring the obtained focal position as a function of the motor's setup. The focal position was determined by analyzing the local curvature of the wavefront measured with a coded-mask-based single-shot wavefront sensor. Assuming a linear correlation between force and motor position, we calculated the response function according to (1/q = p0x + p1), where q is the focal position, x is the motor position value (direct position or applied voltage), and p0 and p1 are linear fitting coefficients. By linearly interpolating the corresponding plot (an example shown in Fig. 5), this calibration allowed us to correlate the focal position, and thus the surface profile, with the bender motor positions. Details on the calibration of all the motors can be found in Supplemental Document, Section S2. In the case of the 34-ID-C beamline, it is worth underlining that the simulation of the beam profile at the sample position not only involved the treatment of the mirror surface shape but also included the application of the Hybrid algorithm to account for the diffraction effects from the coherence slits and mirror size. Fig. 6 illustrates the comparison between the measured signal and the simulated one, with identical motor setup, in both at-focus and out-of-focus conditions. All the combined features provided by OASYS lead to an excellent agreement between the simulation and measurements, ensuring that any conclusions drawn from studies on simulated data will be valid and applicable to the real beamline."
        }
    ],
    "AI-driven auto-alignment": [
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "Bayesian optimization (BO) is a model-based optimization method that uses an iterative approach to solve black-box optimization problems with possibly noisy inputs and outputs. BO methods rely on Bayes' theorem to create a statistical \"surrogate\" model that incorporates the previous observations and prior knowledge to decide the next query of the system using a Bayesian interpretation of the statistics. The BO process consists of two primary components: i) a surrogate model that approximates the system response to inputs, updated after each query, and ii) an acquisition function that suggests the next query point based on the surrogate model. For the surrogate model, Gaussian processes (GPs) are a popular choice, providing not only function output but also uncertainty at a query point. A GP is an infinite-dimensional extension to the familiar multivariate normal distribution. Whereas a normal distribution is characterized by its mean and the variance (or the uncertainty) about the mean, a GP is similarly described by a mean function and a covariance kernel. The mean function gives the expected value of the surrogate function at each observation point, while the covariance kernel describes the uncertainty about the model at that point. The GP surrogate model can be expressed as f ~ GP[v(x), k(xi, xj)], where x is the input parameter, v is the mean function, and k is the covariance kernel that encapsulates the joint variability of the possible objective function values for a pair of inputs xi, xj. The mean and covariance functions are chosen in advance to encode prior knowledge about the system under consideration. Commonly, the mean is chosen as a constant function, while the covariance is selected such that points xi, xj that are closer in the input space have a higher positive value. This selection encodes the belief that the function values of nearby points should be closer than those farther apart. Moreover, the covariance kernel encodes assumptions regarding the smoothness and length scale of the objective function. In this work, we employ GPs with a constant mean function and utilize the scaled Matern kernel for the covariance. The acquisition function is typically defined such that its maximum corresponds to a potential optimal point for the objective(s), whether because of large predicted objective values, significant prediction uncertainty, or a combination of both. A popular choice for the acquisition function is the Expected Improvement (EI) function which accounts for both the likelihood that a new query point will improve the objective and the magnitude of the expected improvement. For noise-free observations and a maximization objective, the EI acquisition function takes the form of an integral. This mathematical framework, encompassing the Gaussian process surrogate model and the Expected Improvement acquisition function, provides sufficient machinery for optimizing a typical black-box problem with a single objective.\n\nMulti-objective optimization with Gaussian processes: Beam alignment can be understood and expressed from single-objective and multi-objective perspectives. If the desired beam profile can be accurately described, we can articulate the alignment goal as minimizing a single distance metric between the pixelated test (unaligned) structure and the desired beam structure. Various measures like the Kullback-Leibler (KL) divergence, the Euclidean norm, or the Wasserstein distance (also known as the earth mover's distance) may serve as minimization objectives to capture the difference between the test and desired profiles. However, practical applications reveal weaknesses in these metrics. KL divergence and Euclidean norm are ineffective when there is no overlap (for a large enough misalignment) between the test and target structures, while Wasserstein distance may be computationally prohibitive. Alternately, beam alignment can be viewed as a careful orchestration of multiple priorities, such as the location, shape, and intensity spread of the beam. This perspective requires tuning these factors to desired values while recognizing that optimizing one may compromise others. Although it is possible to assign a priori weights to these priorities to create a single optimization objective, such optimal weighing can be challenging to determine and susceptible to changes in experimental conditions. A more robust approach is identifying a solution for each possible tradeoff between priorities and making a posteriori choice on the desired solution. This full set of solutions forms the \"Pareto front\" (PF) for the multi-objective optimization problem, and a Bayesian approach to map this solution set is known as the Multi-Objective Bayesian Optimization (MOBO) approach. To formalize the multi-objective optimization method, we begin by assuming an optimization problem with the M optimization objectives g(x), where each objective is a maximization objective. A solution g(x*) is considered to dominate another solution g(x') if g(x*) >= g(x') for all m and g(x*) > g(x') for at least one m. Each dominated solution is deemed inferior and undesirable. Conversely, there exist many solutions that do not dominate each other. Then the surface connecting the set of non-dominated solutions is actually an estimate of the true Pareto front. Furthermore, we can use a hypervolume reference point inferior to all the non-dominated solutions to define a hypervolume dominated by (or under) the estimated PF, so that the true PF is the set of points with the maximal dominated hypervolume. This gives us one MOBO approach: an iterative procedure that, at every iteration, identifies a query point that increases the dominated hypervolume, effectively increasing the quality of the estimated PF. In this work, the MOBO surrogate model f is composed of M individual GPs, with each GP a surrogate model for the individual objective g(x). Analogous to the expected improvement (EI) function in the single-objective cases, the Expected Hypervolume Improvement (EHVI) function is a popular acquisition function for the MOBO settings. The EHVI function accounts for both the probability that a new query point will improve the dominated hypervolume and the magnitude of the expected improvement. This work uses the Noisy Expected Hypervolume Improvement (NEHVI) acquisition function that extends EHVI to settings with noisy observations and constraints on any observables in the experiment.\n\nChoice of the optimization objectives: In pursuing the goal of auto-alignment, we need to consider both the position of the beam maximum and the intensity distribution around this maximum. To capture these aspects, we define three optimization objectives related to the full width at half-maximum (FWHM) of the beam, the location of the peak (PL), and the negative logarithm of the peak intensity (NLPI), given by equations relating FWHM_H, FWHM_V, P_H, P_V, and P_int.\n\nMOBO implementation details: We first describe some a priori considerations for the MOBO design. Search space: The search space (or search boundary) for each parameter is determined by experimental considerations (such as the physically viable range for a motor motion) and prior knowledge regarding the parameter's impact on the beam profile. In this work, we use the digital twin and prior calibrations to define physically viable search ranges that should be adequate for the misalignment levels. Although narrowing the search space might lead to faster optimization, we opt not to restrict these overly but to keep them fixed throughout the experiments. Hypervolume reference point: A reference point is provided to specify the lower bound for the optimization objectives, which allows for calculating the sampling hypervolume relative to this reference point, and all the sampled points are expected to improve upon the reference objective value. Although choosing a reference point can significantly impact the sampling, determining the optimal reference point a priori can be challenging. Consequently, we use clearly suboptimal objective values to define the reference points in the subsequent experiments. Constraints: Optionally, equality or inequality constraints can be applied to any observable, such as the integrated intensity. These constraints can filter the sampled points, thereby not only lowering the computational cost per iteration but also guiding the sampling toward more desirable regions. Initialization and termination: A few observations must be acquired in a space-filling manner before initiating the optimization routine. This helps the GPs create useful surrogate models for the full search space, reducing the possibility of the optimization being stuck in a small subset of the search space. We typically use 10 iterations (or trials) of Sobol space-filling sampling on the entire parameter space (unless otherwise stated) for optimization initialization. As for the termination criterion, we simply set a fixed budget for the number of observations and terminate when the budget is expended. Besides the a priori considerations, we must also define a criterion for a posteriori selection of a single beam profile from the multiple candidates comprising the PF estimate. Here, we employ the Nash bargaining solution that dominates the largest number of sampled non-Pareto structures, a simple criterion that does not require prior information about the experiment. However, it is essential to note that selecting a single optimal structure is a challenging research problem. A decision-maker might want to examine the full set of PF optimal structures to make a final choice. The optimization implementation uses custom code that leverages the OASYS API for the beam profile and objective calculation and the Optuna library as the user-facing interface for GP-based optimization and post-processing. For this work, the Optuna library uses the BoTorch library as the backend for GP-based modeling and calculations. Specifically, the multi-objective modeling relies on the BoTorch ModelListGP class, encapsulating individual SingleTaskGP models for each objective function. The default settings for the mean and covariance kernels are as set in the SingleTaskGP model in BoTorch.\n\nDevelopment of the AI-driven controller on 34-ID-C digital twin: We first developed and tested the MOBO autofocusing routine on the focusing optics setup in the 34-ID-C digital twin. In the dual-KB-mirror setup (described in Section 1.1), each mirror is tuned using four motors to adjust the upstream and downstream bending forces, the incident angle (or pitch), and the position (or translation) of the mirror. This results in a total of eight optimization parameters. The collected data consists of 1D vertical and horizontal beam profiles obtained using the diffractometer scan. Given the complexity of defining a single peak intensity value for the data, the optimization was conducted using only the FWHM and PL objectives. For the 34-ID-C numerical experiments, we started with a manually tuned structure obtained using 1e6 source photons and no background (Figs. 8a, 8b, 8d). A random shift was applied to the mirror motors to obtain the initial misaligned structure for the optimization (Figs. 8a, 8b, 8e). The MOBO routine was configured with the objective values of the initial structure as the hypervolume reference point and with a constraint of >=6e3 photons on the integral photon count to ensure that the beam did not drift out of bounds. We started with 10 steps of Sobol sampling followed by 140 steps of the GP-based acquisition and sampling for the full parameter set. The results of the numerical experiment are shown in Fig. 8, with subplots (8a, 8b) displaying the collected data for the reference optimized structure, the initial misaligned structure, and the Nash solution, while subplots (8d-8i) show the 2D beam profiles at the diffractometer position. Note that these 2D profiles were not available and not used for the optimization during a beamline experiment but are provided here for reference and analysis. The results indicate that the MOBO routine produces a highly compact structure comparable to the reference structure. The motor parameter values for the reference, random, and PF structures are listed in Table 1. Differences in such parameter values, between the reference and optimized structures, are consistent with the expected dynamic and behavior of the optical system.\n\nTest of the AI-driven controller on 28-ID-B digital twin: After refining the MOBO routine through the 34-ID-C digital twin experiment, we adapted the procedure for application to the 28-ID-B beamline. Since the data collected in this setting are 2D images, we can define the NLPI objective. We determined using the digital twin that the use of full set of objectives (FWHM, PL, NLPI) is most effective for the beam optimization. We initiated the numerical experiment with 5e4 source photons, excluding background noise. Again, a manually tuned structure was used as a starting point (Fig. 8a [sic - probably 9a]), followed by a random shift to the parameters to obtain the initial misaligned structure (Fig. 8b [sic - probably 9b]). For this experiment, the objective value of the initial structure served as the hypervolume reference. Additionally, a constraint was imposed to maintain the integral photon count at >=1e3 photons, which helped guide the sampling away from the out-of-bounds region. The optimization procedure was launched with 10 steps of Sobol sampling followed by 140 steps of GP-based sampling. The optimization results are illustrated in Fig. 9, with the NS beam structure (Fig. 9c) selected as a posteriori, the remaining PF structures (Figs. 9d-9e), and scatter plots (Figs. 9f-9h) showing the spread and the history of the sampled objective values. The structures in the Pareto front form an envelope in the lower-left region (the desirable region) of the scatter plots, with each structure being an optimal solution based on different tradeoffs among the objectives. The optimal parameter values, the initial settings, and the values associated with the PF structures are compiled in Table 2. A noteworthy observation is that the candidate solution in Fig. 9d contains a long tail in the horizontal direction but has a low FWHM value. This uncovers the limitation of using the FWHM metric: it does not adequately account for non-Gaussian structures with long tails. As a consequence, the NS structure has lower FWHM than the reference structure, but is not as compact and also has a lower peak intensity count."
        }
    ],
    "The AI-driven controller in operating conditions": [
        {
            "paper_title": "AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems",
            "source_filename": "Rebuffi et al. - 2023 - AutoFocus AI-driven alignment of nanofocusing X-ray mirror systems.pdf",
            "content": "After the numerical validation through the digital twin, we tested the auto-alignment procedure in a real-world setting at the 28-ID-B beamline. We conducted two key experiments: i) the alignment of an unfocused structure into a compact, focused structure, and ii) the alignment towards a specific predefined structure.\n\nObtaining a compact, focused structure: To evaluate this fundamental auto-alignment routine, we started from a significantly misaligned structure, as shown in Fig. 10a. Since this was a proof-of-concept exercise conducted in a real, noisy environment, we used looser criteria for the hypervolume reference point, setting them to the suboptimal values of FWHM_HR=40 µm, PL_HR=50 µm, NLPI_HR=infinity, and did not apply any constraints on the integral photon counts. We again performed 10 steps of Sobol sampling across the full parameter space to initialize the MOBO calculation, followed by 100 optimization steps involving all parameters. The experiment was conducted using the same search space as in the numerical experiment with the digital twin. The results shown in Fig. 10 confirm that the MOBO procedure succeeded in producing a compact structure. However, the aberration and background in the experimental data led to the acquisition of structures with a range of FWHM values but similar peak locations and intensities. The PF structures also showed the same trend (see Supplemental Document, Fig. S3). Furthermore, Figs. 10c-10e reveal that the first few optimization iterations were sufficient to discover the central region with favorable candidate structures; the subsequent iterations attempted to find the optimal parameter settings within this favorable region. Detailed descriptions of the candidate PF solutions and the parameter values for the PF solutions can be found in the Supplemental Document, Section S4.\n\nObtaining a structure with specific desired properties: In practical applications, we often desire a structure with a specific shape or level of defocus rather than the most compact focused structure. To emulate this type of alignment challenge, we used the arbitrarily chosen horizontal and vertical FWHM values, FWHM_H,R=18 µm, FWHM_V,R=7 µm respectively, as the reference or desired beam properties. We then modified the FWHM objective in Eq. 6. We used the starting structure shown in Fig. 11a and performed a two-step optimization routine with 10 steps of Sobol and 40 steps of GP-based sampling for translation and pitch parameter subset, followed by 100 sampling steps for the entire parameter set. We note that the efficacy of this two-step method over a one-step optimization for this experimental setup remains undetermined. We used the same hypervolume reference point (excluding the NLPI term) from Section 4.3 for the modified objectives. The results in Fig. 11 show that the MOBO procedure produced a structure with properties close to the reference values.\n\nChallenges and Considerations: Among the optimization objectives we employed, peak location and peak intensity consistently perform well under all scenarios. However, using FWHM as an optimization objective presents challenges, as it does not consider the presence of long tails or other non-Gaussian features in the beam structure. Moreover, defining the precise shape and location of the beam can become difficult in the presence of background interference and noise. A partial solution could be to use more generic measures for the beam width, such as the root mean square (rms) of the beam distribution, for the optimization of non-Gaussian beam profiles, as it happens for highly coherent beams, that have diffraction fringes and/or diffused scattering features/halos around the focused peak. More accurate denoising and beam characterization criteria would be hardware-specific and require careful consideration for each particular experiment setting. A different challenge arises when aiming to align to more complex beam profiles that cannot be simply characterized by the width and peak location. In such cases, we would need to consider one of two approaches: either use a more sophisticated metric like the Wasserstein distance to characterize the structural differences between two beam profiles, or apply numerical inversion techniques or neural network approaches for the precise beam alignment. Specifically for the latter approach, it can still be beneficial to perform a fast and rough alignment using the procedure described in this work before fine-tuning it with another technique. Furthermore, we believe it is feasible to enhance the robustness and efficiency of the proposed auto-alignment approach through further research in several directions. Firstly, careful consideration and potential dynamic tuning of constraints, the hypervolume reference point, and even the search space could significantly improve the efficiency of the auto-alignment routine. Secondly, if the overall goal of the experiment is to maintain a stable optical system through multiple runs of the autofocusing routine, then reusing the information acquired in one auto-alignment run could accelerate future auto-alignments on the same optical system. Alternatively, in a more sophisticated approach, we could exploit the idea of multi-fidelity optimization to create and dynamically update a complex GP-based (or NN-based) model of the optical system, then exploit this extra surrogate for accelerated optimization. Future work might also include designing a reinforcement learning procedure that utilizes the GP-based surrogate model for data-efficient real-time beam stabilization and control. In addition, directly measuring the focal spot using a 2D detector, especially when the spot size is below 100 nm, is exceptionally challenging. Presently, no detector system can provide the necessary spatial resolution for this task. Potential measurement methods include fluorescence edge scans, ptychography, and wavefront sensing. Among these, wavefront sensing emerges as the only single-shot option. In this approach, the wavefront downstream of the focal plane is captured and then backpropagated to pinpoint the focal plane and determine its associated size and position. Intensive efforts are in progress to refine and improve the resolution of this method."
        }
    ],
    "Conclusion": [
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "We have presented a method for electron ptychography that reduces the electron dose through adaptive scanning. Sub-sequences of scan positions are predicted by the model within milliseconds, allowing an acquisition rate that theoretically exceeds rates currently achieved in 4D-STEM experiments. The method would therefore have the potential to be applied in real time at the microscope if the used ptychography reconstruction algorithm could generate images sufficiently fast. Future work does therefore require either an improvement of iterative ptychography algorithms in terms of processing speed or the integration of direct ptychography reconstruction methods, such as single-sideband (SSB) ptychography, in the adaptive scanning workflow. We show an improved resolution and reconstruction quality when using an adaptive scanning approach on experimentally acquired monolayer MoS₂ data sets in comparison to another dose reduction scanning approach. These improvements show that adaptive scanning for ptychography is a useful technique to lower the dose needed for the analysis of sensitive samples. It can be provided with simulated or generic experimental training data to increase its applicability to a variety of different or less periodic material structures. We have demonstrated the generalizability of our method by applying it to simulated DWCNT data sets and showing that it outperforms other low-dose alternatives. In addition, the proposed workflow can be taken as a blueprint for a broad range of scanning microscopy methods and thus paves the way for future research in machine learning supported, automated and autonomous microscopy."
        }
    ],
    "Funding": [
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Open Access funding enabled and organized by Projekt DEAL."
        }
    ],
    "Table 1": [
        {
            "paper_title": "Deep reinforcement learning for data-driven adaptive scanning in ptychography",
            "source_filename": "Schloz et al. - 2023 - Deep reinforcement learning for data-driven adaptive scanning in ptychography.pdf",
            "content": "Table 1: Performance of adaptive scanning for various experimental settings that differ in the number of scan positions and the total number of sub-sequences. For each setting, the oversampling ratio N_k/N_u, which is calculated following Ref. 17, and the electron dose is given.\n\n# Pos. | N_k/N_u | Dose (e⁻/Å⁻²) | # Sub-seq. | Q_SSIM\n250 | 8.21 | 1.34E5 | 2 | 9.89 ± 6.80%\n250 | 8.21 | 1.34E5 | 3 | 12.38 ± 10.72%\n250 | 8.21 | 1.34E5 | 4 | 15.71 ± 7.60%\n250 | 8.21 | 1.34E5 | 5 | 15.84 ± 7.23%\n335 | 10.83 | 1.79e5 | 5 | 10.03 ± 8.17%\n420 | 13.43 | 2.25e5 | 5 | 8.60 ± 5.30%\n500 | 15.86 | 2.68e5 | 5 | 8.08 ± 7.60%"
        },
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "Table 1: Overview of the suggested performance metrics in SDLs with a summary for each metric, a list of reported studies that achieved a high degree of performance for each metric, and the subsequently reported metrics of the listed studies.\n\nMetric | Summary | Exemplary Studies | Reported Metrics\n\nDegree of Autonomy | Classification of the extent with which human intervention is required for regular operation. The metric can be piecewise, semi-closed-loop, or closed-loop. | 4,13–16,18,20,23,26,29,30 | Closed-loop\n\nOperational Lifetime | The total time that a platform can conduct experiments. The metric should be reported in four forms: demonstrated unassisted lifetime, demonstrated assisted lifetime, theoretical unassisted lifetime, and theoretical assisted lifetime. Additional research efforts should be made to evaluate the maximum lifetimes outside of case study optimizations. | 4 | 700 samples (demonstrated, unassisted)\n\nThroughput | The rate that the platform can conduct experiments. The metric should be reported in both demonstrated and theoretical throughput which includes both sample preparation and measurement. Additional research efforts should be made to evaluate the maximum throughput outside of case study optimizations. | 4,15 | 30 to 33 samples per hr (demonstrated)\n\nExperimental Precision | A quantitative value representing the reproducibility of an experimental platform. Precision estimates should be made using unbiased sequential experiments in conditions similar to those found during optimization. Sequential replication of a test condition can introduce bias. | 4,16,23 | Alternating random\n\nMaterial Usage | The total quantity of materials used per experiment. The metric should be broken down into total active quantity during experimentation, total used per experiment, total hazardous material used per experiment, and total high value material used per experiment. The values should be reported in either volume or mass, where appropriate. Additional effort should be taken to include the material usage for auxiliary steps, such as reactor cleaning or preconditioning. | 4,13,24,25 | 0.06 to 0.2 mL per sample\n\nAccessible Parameter Space | Qualitative and quantitative description of accessible parameter space for a system along with the attainable measurement techniques. The reporting should be sub-divided into demonstrated and theoretical range. | 9 | 1.6 × 10^11\n\nOptimization Efficiency | Quantitative analysis of the performance of a full system and its experiment selection algorithm. The most effective performance metric is direct algorithm benchmarking with replicates. The existing method can be compared with random sampling along with state-of-the-art selection algorithms. In the absence of sufficient data generation, simulated benchmarking can be applied. Where appropriate, linear regressions and explainable artificial intelligence techniques should be applied to any models used along with the required data set size to reach predictability. | 14–16,25 | Grid-search, SNOBFIT, CMA-ES, Nelder-Meade, and Human benchmarking"
        }
    ],
    "Autonomous materials-discovery platform": [
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "The materials-discovery pipeline followed by the A-Lab is schematically\nshown in Fig. 1. All target materials considered in this work are new\nto the lab, that is, not present in the training data for the algorithms\nit uses to propose synthesis recipes, and 52 of the 58 targets have no previous synthesis reports, to the best of our knowledge (Methods).\nThe experiments reported in this study represent the first attempts by\nthe A-Lab to synthesize any of these targets. Each target is predicted to\nbe on or very near (<10 meV per atom) the convex hull formed by stable\nphases taken from the Materials Project14 and cross-referenced with an\nanalogous database from Google DeepMind. Because the A-Lab handles\nsamples in open air, we only considered targets that are predicted not\nto react with O2, CO2 and H2O (Methods).\nFor each compound proposed to the A-Lab, up to five initial synthesis\nrecipes are generated by a ML model that has learned to assess target\n‘similarity’ through natural-language processing of a large database\nof syntheses extracted from the literature15, mimicking the approach\nof a human to base an initial synthesis attempt on analogy to known\nrelated materials. A synthesis temperature is then proposed by a second ML model trained on heating data from the literature16 (Methods).\nIf these literature-inspired recipes fail to produce >50% yield for their\ndesired targets, the A-Lab continues to experiment using Autonomous\nReaction Route Optimization with Solid-State Synthesis (ARROWS3\n),\nan active-learning algorithm that integrates ab initio computed reaction energies with observed synthesis outcomes to predict solid-state\nreaction pathways17. Experiments are performed under the guidance\nof this algorithm until the target is obtained as the majority phase or\nall synthesis recipes available to the A-Lab are exhausted.\nThe A-Lab carries out experiments using three integrated stations for\nsample preparation, heating and characterization, with robotic arms\ntransferring samples and labware between them (Fig. 1 and Extended\nData Figs. 1 and 2). The first station dispenses and mixes precursor\npowders before transferring them into alumina crucibles. A robotic arm\nfrom the second station loads these crucibles into one of four available\nbox furnaces to be heated (Methods). After allowing the samples to\ncool, another robotic arm transfers them to the third station, where they\nare ground into a fine powder and measured by XRD. The operations of\nthe lab are controlled through an application programming interface,\nwhich enables on-the-fly job submission from human researchers or\ndecision-making agents (Extended Data Fig. 3).\nThe phase and weight fractions of the synthesis products are\nextracted from their XRD patterns by probabilistic ML models trained\non experimental structures from the Inorganic Crystal Structure Database (ICSD) following the methodology outlined in previous work18,19.\nBecause the target materials considered in this work have no experimental reports, their diffraction patterns are simulated from computed\nstructures available in the Materials Project and corrected to reduce\ndensity functional theory (DFT) errors (Supplementary Note 1). For\neach sample, the phases identified by ML are confirmed with automated\nRietveld refinement (Methods and Supplementary Note 2) and the\nresulting weight fractions are reported to the management server of\nthe A-Lab to inform subsequent experimental iterations, if necessary,\nin search of an optimal recipe with high target yield."
        }
    ],
    "Experimental synthesis outcomes": [
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "Using the described workflow, the A-Lab synthesized 41 of the 58 target\ncompounds over 17 days of continuous experimentation, representing\na 71% success rate. We show in the next section that this success rate\ncould be improved to 74% with only minor modifications to the lab’s\ndecision-making algorithm, and further to 78% if the computational\ntechniques were also improved. The high success rate demonstrates\nthat comprehensive ab initio calculations can be used to effectively\nidentify new, stable and synthesizable materials. The outcome for all\n58 compounds is plotted in Fig. 2 against their decomposition energies\n(on a log scale), a common thermodynamic metric that describes the\ndriving force to form a compound from its neighbours on the phase diagram20 (Supplementary Fig. 2). A negative (positive) decomposition\nenergy indicates that a material is stable (metastable) at 0 K. Of the\ntargets considered in this work, 50 are predicted to be stable, whereas\nthe remaining eight are metastable but lie near the convex hull. Over\nthe range of decomposition energies considered, we do not observe a\nclear correlation between decomposition energy and whether a material was successfully synthesized.\nIn total, 35 of the 41 materials synthesized by the A-Lab were obtained\nusing recipes proposed by ML models trained on synthesis data from\nthe literature (Supplementary Note 3). These literature-inspired recipes\nwere more likely to succeed when the reference materials are highly\nsimilar to our targets (Supplementary Fig. 3), confirming that target\n‘similarity’ is a useful metric to select effective precursors21. At the\nsame time, precursor selection remains a highly nontrivial task, even\nfor thermodynamically stable materials. Despite 71% of targets eventually being obtained, only 37% of the 355 synthesis recipes tested by\nthe A-Lab produced their targets. This finding echoes previous work\nthat has established the strong influence of precursor selection on\nthe synthesis path, ultimately deciding whether it forms the target or\nbecomes trapped in a metastable state22–25.\nThe active-learning cycle of the A-Lab17 identified synthesis routes\nwith improved yield for nine targets, of which six had zero yield from\nthe initial literature-inspired recipes. Targets optimized with active\nlearning are indicated by the bars containing diagonal lines in Fig. 2.\nIn this framework, improved synthesis routes are designed using two\nhypotheses: (1) solid-state reactions tend to occur between two phases\nat a time (that is, pairwise)26–28 and (2) intermediate phases that leave\nonly a small driving force to form the target material should be avoided,\nas they often require long reaction time and high temperature22,23,29.\nThe A-Lab continuously builds a database of pairwise reactions\nobserved in its experiments—88 unique pairwise reactions (Supplementary Table 2) were identified from the synthesis experiments performed\nin this work. This database allows the products of some recipes to be\ninferred, precluding their testing; a recipe that yields an observed set\nof intermediates (already present in the lab’s database) need not be\npursued at higher temperatures, as the remaining reaction pathway is\nalready known (Fig. 3a,b). This can reduce the search space of possible\nsynthesis recipes by up to 80% when many precursor sets react to form\nthe same intermediates (Fig. 3e and Supplementary Notes 4 and 5). Furthermore, knowledge of reaction pathways can be used to give priority\nto intermediates with a large driving force to form the target, computed\nusing formation energies available in the Materials Project (Fig. 3c,d).\nFor example, the synthesis of CaFe2P2O9 was optimized by avoiding\nthe formation of FePO4 and Ca3(PO4)2, which have a small driving force\n(8 meV per atom) to form the target. This led to the identification of an\nalternative synthesis route that forms CaFe3P3O13 as an intermediate,\nfrom which there remains a much larger driving force (77 meV per atom)\nto react with CaO and form CaFe2P2O9, causing an approximately 70%\nincrease in the yield of the target (Supplementary Note 6)."
        }
    ],
    "Barriers to synthesis": [
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "Seventeen of the 58 targets evaluated by the A-Lab were not obtained\neven after its active-learning cycle. We identify slow reaction kinetics,\nprecursor volatility, amorphization and computational inaccuracy as\nfour broad categories of ‘failure modes’ that prevented the synthesis\nof these targets. The prevalence of each failure mode is shown in Fig. 4,\naccompanied by their affected targets. Sluggish reaction kinetics hindered 11 of the 17 failed targets, each\ncontaining reaction steps with low driving forces (<50 meV per atom;\nSupplementary Fig. 4). In principle, these targets can be made accessible\nby using a higher synthesis temperature, longer heating time, improved\nprecursor mixing or intermittent regrinding—standard procedures that\nare at present outside the domain of the A-Lab’s active-learning algorithm. As such, we manually reground the original synthesis products\ngenerated by the A-Lab and heated them to higher temperatures, which\nled to the successful formation of two further targets, Y3Ga3In2O12 and\nMg3NiO4, bringing our total success rate to 74% (Supplementary Note 7).\nOne could also use more reactive precursors to provide a greater driving\nforce to form the target, although our experiments were constrained\nto air-stable binary precursors that sometimes restricted the A-Lab’s\nchoice of synthesis routes to those forming highly stable intermediates. System modifications to enable multistep heating, intermediate\nregrinding and expanded precursor selection should improve the ability\nof the lab to adapt and overcome failed synthesis attempts.\nPrecursor volatility disrupted all synthesis experiments targeting\nCaCr2P2O9, causing a change in the net stoichiometry of its samples\n(Supplementary Note 8). This can be attributed to the use of ammonium phosphate precursors, NH4H2PO4 and (NH4)2HPO4, which proceed\nthrough a series of decomposition reactions and ultimately evaporate\nabove 450 °C (ref. 30). Still, recipes based on these precursors can succeed if the ammonium phosphate reacts with another precursor before\nits evaporation temperature, effectively locking the phosphate ions in\nthe solid state. For example, volatility does not seem to be an issue for\nthe Mn-containing phosphates targeted in this work, as each Mn oxides\nprecursor reacts with the ammonium phosphates at low temperature\n(<500 °C) to form Mn2(PO4)3 as an intermediate. This precursor behaviour can, in principle, be learned when sufficient pairwise reaction data\nhave been collected, after which the A-Lab may favour the selection of\nprecursors that trap in phosphate ions at low temperature and therefore\npreclude unwanted volatility.\nMelting of samples at high temperature inhibited the crystallization of one target, Mo(PO3)5, whose synthesis attempts produced\namorphous samples (Supplementary Fig. 5). Although the use of a\nmolten flux can sometimes improve reaction kinetics31, the formation\nof an amorphous state that is low in energy may reduce the driving\nforce for crystallization. Indeed, using the workflow outlined in ref. 32,\nwe identified amorphous configurations of Mo(PO3)5 with energies as\nlow as 61 meV per atom above the crystalline ground state, a finding\nthat is consistent with the widely reported glass-forming ability of\nphosphate-rich compounds33,34.\nSome failure modes result from inaccuracies in the computed stability of the target and therefore cannot be addressed by modifications\nto the experimental procedures. Fundamental-electronic-structure\nchallenges are probably affecting La5Mn5O16, as all the attempts to\nsynthesize this phase instead yielded LaMnO3, which DFT unexpectedly predicts to be highly unstable (120 meV per atom above the hull),\neven though it is widely reported in the literature to be experimentally\naccessible35. If the energy of LaMnO3 were lowered, consistent with\nits experimental stability, La5Mn5O16 would be destabilized (above\nthe hull). Errors in the computed energy of LaMnO3 may arise from\nits strong Jahn–Teller activity36, compositional off-stoichiometry37\nor the presence of f-states in La—all of which present challenges to\nconventional DFT. Problems with YbMoO4 were found to be because\nof a poor pseudopotential choice in the Materials Project that destabilizes the well-known oxide, Yb2O3, and it is likely that, in more accurate calculations, YbMoO4 is not stable. A similar lanthanide-related\nelectronic-structure problem may also be responsible for the failure\nto synthesize BaGdCrFeO6. These examples demonstrate the ability of\nthe A-Lab to provide important feedback to high-throughput computed\ndatasets. With improved calculations that exclude the computationally problematic compounds in this work, our total success rate would\nincrease to 78% (43/55 targets)."
        }
    ],
    "Outlook": [
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "In 17 days of closed-loop operation, the A-Lab performed 355 experiments and successfully realized 41 of 58 novel inorganic crystalline solids with diverse structures and chemistries. This unexpectedly high\nsuccess rate (71%) for the synthesis of computationally predicted compounds was achieved by integrating robotics with: (1) DFT-computed\ndata to survey the energetic landscape of precursors, reaction intermediates and final products; (2) heuristic suggestions for synthesis procedures obtained from ML models trained on text-mined synthesis data;\n(3) ML interpretation of experimental data; and (4) an active-learning\nalgorithm that improves on failed synthesis procedures. The study\nalso revealed several opportunities to enhance the lab’s active-learning\nalgorithm by addressing failures caused by slow reaction kinetics,\nwhich would enable an improved success rate of 74% with in-line\nsolutions.\nOur paper demonstrates that autonomous research agents can markedly accelerate the pace of materials research. Researchers initialized\nthe A-Lab by proposing 58 target materials, which were successfully\nrealized at a rate of >2 new materials per day with minimal human intervention. Such rapid discovery points to a vast landscape of opportunities in materials synthesis and development. Although this work\nfocused on a limited subset of all possible synthesis targets, many new\ncandidates await evaluation. As the breadth of ab initio computations\ncontinues to grow, so will this list of novel materials.\nAdvances in simulations, ML and robotics have intersected to enable\n‘expert systems’ that show autonomy as an emergent quality by the\nsum of its automated components. The A-Lab demonstrates this by\ncombining modern theory-driven and data-driven ML techniques\nwith a modular workflow that can discover novel materials with minimal human input. Lessons learned from continuing experiments can\ninform both the system itself and the greater community through\nsystematic data generation and collection. The systematic nature\nof the A-Lab provides a unique opportunity to answer fundamental\nquestions about the factors that govern the synthesizability of novel\nmaterials, serving as an experimental oracle to validate predictions\nmade on the basis of data-rich resources such as the Materials Project.\nIn future iterations of the platform, such an oracle may be expanded\nto investigate factors beyond synthesizability, including microstructure and device performance. Although our current success rate for\nthe synthesis of novel compounds is high, the remaining discrepancies between current predictions and their experimental outcomes\nis a crucial signal required to improve our understanding of materials\nsynthesis. 18. Szymanski, N. J., Bartel, C. J., Zeng, Y., Tu, Q. & Ceder, G. Probabilistic deep learning\napproach to automate the interpretation of multi-phase diffraction spectra. Chem. Mater.\n33, 4204–4215 (2021).\n19. Szymanski, N. J. et al. Adaptively driven X-ray diffraction guided by machine learning for\nautonomous phase identification. NPJ Comput. Mater. 9, 31 (2023).\n20. Bartel, C. J. Review of computational approaches to predict the thermodynamic stability\nof inorganic solids. J. Mater. Sci. 57, 10475–10498 (2022).\n21. He, T. et al. Similarity of precursors in solid-state synthesis as text-mined from scientific\nliterature. Chem. Mater. 32, 7861–7873 (2020).\n22. Miura, A. et al. Selective metathesis synthesis of MgCr2S4 by control of thermodynamic\ndriving forces. Mater. Horiz. 7, 1310–1316 (2020).\n23. Bianchini, M. et al. The interplay between thermodynamics and kinetics in the solid-state\nsynthesis of layered oxides. Nat. Mater. 19, 1088–1095 (2020).\n24. Aykol, M., Montoya, J. H. & Hummelshøj, J. Rational solid-state synthesis routes for\ninorganic materials. J. Am. Chem. Soc. 143, 9244–9259 (2021).\n25. Martinolich, A. J. & Neilson, J. R. Toward reaction-by-design: achieving kinetic control of\nsolid state chemistry with metathesis. Chem. Mater. 29, 479–489 (2017).\n26. Miura, A. et al. Observing and modeling the sequential pairwise reactions that drive\nsolid-state ceramic synthesis. Adv. Mater. 33, 2100312 (2021).\n27. Cordova, D. L. M. & Johnson, D. C. Synthesis of metastable inorganic solids with extended\nstructures. ChemPhysChem 21, 1345–1368 (2020).\n28. Malkowski, T. F. et al. Role of pairwise reactions on the synthesis of Li0.3La0.57TiO3 and the\nresulting structure–property correlations. Inorg. Chem. 60, 14831–14843 (2021).\n29. Todd, P. K. et al. Selectivity in yttrium manganese oxide synthesis via local chemical\npotentials in hyperdimensional phase space. J. Am. Chem. Soc. 143, 15185–15194\n(2021).\n30. Pardo, A., Romero, J. & Ortiz, E. High-temperature behaviour of ammonium dihydrogen\nphosphate. J. Phys. Conf. Ser. 935, 012050 (2017).\n31. Gupta, S. K. & Mao, Y. Recent developments on molten salt synthesis of inorganic\nnanomaterials: a review. J. Phys. Chem. C 125, 6508–6533 (2021).\n32. Aykol, M., Dwaraknath, S. S., Sun, W. & Persson, K. A. Thermodynamic limit for synthesis\nof metastable inorganic materials. Sci. Adv. 4, eaaq014 (2018).\n33. Bridge, B. & Patel, N. D. The elastic constants and structure of the vitreous system Mo-P-O.\nJ. Mater. Sci. 21, 1186–1205 (1986).\n34. Muñoz, F. & Sánchez-Muñoz, L. The glass-forming ability explained from local structural\ndifferences by NMR between glasses and crystals in alkali metaphosphates. J. Non-Cryst.\nSolids 503–504, 94–97 (2019).\n35. Norby, P., Krogh Andersen, I. G., Andersen, E. K. & Andersen, N. H. The crystal structure of\nlanthanum manganate(iii), LaMnO3, at room temperature and at 1273 K under N2. J. Solid\nState Chem. 119, 191–196 (1995).\n36. Kim, Y.-J., Park, H.-S. & Yang, C.-H. Raman imaging of ferroelastically configurable Jahn–\nTeller domains in LaMnO3. NPJ Quantum Mater. 6, 62 (2021).\n37. Alonso, J. A. et al. Non-stoichiometry, structural defects and properties of LaMnO3+δ with\nhigh δ values (0.11≤δ≤0.29). J. Mater. Chem. 7, 2139–2144 (1997)."
        }
    ],
    "Online content": [
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "source_filename": "Szymanski et al. - 2023 - An autonomous laboratory for the accelerated synthesis of novel materials.json",
            "content": "1. Jain, A., Shin, Y. & Persson, K. A. Computational predictions of energy materials using\ndensity functional theory. Nat. Rev. Mater. 1, 15004 (2016).\n2. Sun, J. et al. Accurate first-principles structures and energies of diversely bonded\nsystems from an efficient density functional. Nat. Chem. 8, 831–836 (2016).\n3. Nikolaev, P. et al. Autonomy in materials research: a case study in carbon nanotube\ngrowth. NPJ Comput. Mater. 2, 16031 (2016).\n4. Chang, J. et al. Efficient closed-loop maximization of carbon nanotube growth rate using\nBayesian optimization. Sci. Rep. 10, 9040 (2020).\n5. MacLeod, B. P. et al. Self-driving laboratory for accelerated discovery of thin-film\nmaterials. Sci. Adv. 6, eaaz8867 (2020).\n6. Burger, B. et al. A mobile robotic chemist. Nature 583, 237–241 (2020).\n7. Ludwig, A. Discovery of new materials using combinatorial synthesis and high-throughput\ncharacterization of thin-film materials libraries combined with computational methods.\nNPJ Comput. Mater. 5, 70 (2019).\n8. Ren, Z. et al. Embedding physics domain knowledge into a Bayesian network enables\nlayer-by-layer process innovation for photovoltaics. NPJ Comput. Mater. 6, 9 (2020).\n9. Sun, S. et al. A data fusion approach to optimize compositional stability of halide\nperovskites. Matter 4, 1305–1322 (2021).\n10. Li, J. et al. Synthesis of many different types of organic small molecules using one\nautomated process. Science 347, 1221–1226 (2015).\n11. Kitson, P. J. et al. Digitization of multistep organic synthesis in reactionware for on-demand\npharmaceuticals. Science 359, 314–319 (2018).\n12. Coley, C. W. et al. A robotic platform for flow synthesis of organic compounds informed\nby AI planning. Science 365, eaax1566 (2019).\n13. Manzano, J. S. et al. An autonomous portable platform for universal chemical synthesis.\nNat. Chem. 14, 1311–1318 (2022).\n14. Jain, A. et al. Commentary: The Materials Project: a materials genome approach to\naccelerating materials innovation. APL Mater. 1, 011002 (2013).\n15. He, T. et al. Inorganic synthesis recommendation by machine learning materials similarity\nfrom scientific literature. Sci. Adv. 9, eadg8180 (2023).\n16. Huo, H. et al. Machine-learning rationalization and prediction of solid-state synthesis\nconditions. Chem. Mater. 34, 7323–7336 (2022).\n17. Szymanski, N. J., Nevatia, P., Bartel, C. J., Zeng, Y. & Ceder, G. Autonomous and dynamic\nprecursor selection for solid-state materials synthesis. Nat. Commun. https://doi.org/\n10.1038/s41467-023-42329-9 (2023)."
        }
    ],
    "1. Introduction": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "In the face of pressing global challenges such as climate change, energy sustainability, and current or emerging healthcare crises, we must seek efficient solutions in the context of a growing global population and increasing resource demands. The accelerated development of materials, technology, and scientific understanding emerges as a potential avenue for tackling these challenges. Traditional research methods, often characterized by gradual progress with limited efficiency, may prove insufficient for the urgency these challenges demand. The integration of laboratory automation and data-driven decision making can potentially facilitate a more rapid and efficient exploration of solutions, while offering multiple advantages over traditional scientific discovery. Notably, automated experimentations can perform experiments faster and with higher precision, while data-driven search algorithms can quickly and efficiently explore experimental space based on feedback from available data (“closed-loop” experimentation). Additionally, issues such as reproducibility challenges and the underrepresentation of negative results in the scientific literature have been identified. At the same time, automation encourages the further digitization of research. The utilization of automated systems enables more precise documentation of experimental protocols, enhancing repeatability and reproducibility, while digitization facilitates data recording and sharing, with particular emphasis on the significance of negative or null results, contributing to a more comprehensive and accurate portrayal of scientific endeavors. High quality large datasets made possible by autonomous experimentation would aid in the development of artificial intelligence (AI) for materials science and chemistry, creating better machine learning (ML) and deep learning (DL) models, and enhancing the decision-making capabilities of data-driven algorithms."
        }
    ],
    "2. Infrastructure": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "SDLs encompass three fundamental components: (i) automated laboratory devices proficient in executing complex chemical operations, (ii) software packages designed to seamlessly handle laboratory operations and the resulting data, and (iii) an experimental planner capable of processing acquired data and guiding subsequent laboratory procedures. In this section, we provide an overview of the essential constituents of SDLs and discuss ongoing efforts to harmonize their integration and control."
        }
    ],
    "2.1. Hardware": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Chemical experiments require different types of operations including chemical handling, reaction execution, post-reaction processing/purification, formulation, device fabrication, and chemical property measurements. For certain steps, taskspecific automated hardware systems have existed for decades, and many have been commercialized as standard laboratory instruments. Most of these systems have not been designed for fully automated workflows, but rather for interfacing with a human researcher. Prominent examples stem from the field of analytical chemistry, where automated solutions, often covering multi-step analytical workflows (e.g., chromatography-mass spectrometry from an autosampler), are routinely available in chemistry laboratories. The integration of such platforms with further automated solutions to enable SDLs presents a major challenge, and can be approached through different strategies. Fixed purpose-built automated systems couple multiple platforms in a static fashion, whereas partially automated workflows, requiring a human-in-the-loop, allow for platforms that can be adapted and repurposed for different experiments.39 Development in general-purpose robotic systems that can perform basic chemistry tasks and interface with the modules have allowed for completely automated yet modular SDLs.40 Moreover, open hardware for lab automation has been proposed to lower the financial barrier to building a SDL.41 In this subsection, we will review the various automated hardware modules, and provide a brief discussion on the development of general-purpose chemistry robotics platforms. (Figure 2) A summary of the distinctions between the hardware types are shown in Table 1"
        }
    ],
    "2.1.1. Specialized Hardware": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The automation of laboratory operations has come a long way since the highthroughput catalyst screening campaigns performed by Bosch and Mittasch using continuous-flow platforms. As of today, chemistry and materials discovery laboratories host a range ofautomated solutions for routine tasks. At the same time, there are still many operations that are routinely performed by human researchers in traditional laboratories, due to the need for operational flexibility and adaptive decision-making.45 Finding automated solutions for these workflows requires interdisciplinary efforts within chemistry and engineering, prompting a re-conceptualization of central laboratory processes. At the core of most laboratory routines lies a set of fundamental operations that are essential across various types of experiments. These include, most importantly, the handling and transfer of materials (most often as liquids or solids), or the precise control of vessel or reactor conditions like temperature, atmosphere, and pressure. Whereas the latter have largely benefited from technological advances outside of chemistry, the challenge of automated reagent handling remains specific to the chemical laboratory. The most straightforward and widely applied solution to automated liquid dispensing is the use of syringe pumps or peristaltic pumps. Commercial solutions to these technologies are widespread, and have facilitated the transfer and dispensing liquids in numerous SDLs. The automation of positivedisplacement pipettes (PDPs) has also emerged as an alternative for robotic liquid dispensing, particularly in the context of biological experimentation. Gantry-based systems using PDPs (e.g. SPT LabTech Mosquito, or the OpenTrons OT systems), allow for substantial throughput increases of parallelized experiments in multi-well plates. Remarkably, the capabilities of PDPs to dispense microliter quantities have enabled the miniaturization of many experiments, particularly for biological assays. The downsides of PDP usage include a limited measurement range, along with challenges in handling e.g., highly viscous liquids and slurries. In contrast to liquid dispensing, dispensing powders or other types of solids presents a more significant challenge for laboratory automation. While automated liquid dispensing benefits from precise robotic volumetric displacement, automated solid dispensing requires real-time measurements of the dispensed quantities, making it more rare and costly. As a consequence, automated laboratories often resort to working with stock solutions of solid reagents when possible. In any SDL, these basic reagent handling operations are coupled to more problem-specific modules, including reaction execution (in environment-controlled reactors), separation and purification, or device fabrication. Given the large diversity between these modules, they will be discussed in the respective sections of this review. Eventually, the necessary characterization feedback to “close the loop” is provided by the diverse library of analytical instrumentation, which are already used in a (semi-)automated fashion in traditional laboratories, but require dedicated integration into SDL workflows. One popular solution is the static combination of individual modules into a continuous flow sequential workflow connected through tubing. As a particularly prominent example, this strategy has laid the foundation for the field of flow chemistry and microfluidics (as discussed in more detail in section Reaction Optimization). Owing to the simplicity of this hardware setup, it has also found applications in a series of higher level SDLs, as discussed throughout the course of this review. An alternative strategy to statically coupling individual modules is the idea of flexible automation.40 This approach emphasizes dynamic connections between modules using robotic systems for transfer between different workstations. This approach, imitating a human researcher operating the different modules, is particularly evident in the use of robotic arms, and, for example, has been used in foundational SDLs in drug discovery (Adam37 and Eve,38 see section Drug Discovery and Biochemistry), and thin-film material synthesis (Ada, see section Optoelectronics). The Chemputer by Cronin and coworkers connects different modules, including vessels, reactors, pumps and further specialized units, using selection valves, enabling a diverse range of automated synthetic chemistry workflows.46 Recently, Cooper and co-workers have extended the concept of flexible automation to the use of mobile robots, operating multiple workstations which are distributed across the laboratory, mimicking a human researcher.11 The idea of flexible automation has recently spurred commercial solutions, particularly from companies such as Chemspeed Technologies and Unchained Labs, based on gantry systems reminiscent of the HTE platforms discussed above. Despite higher costs, these solutions have garnered considerable interest in both industrial and academic SDLs"
        }
    ],
    "2.1.2 General-Purpose Robot Applied for Chemistry": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "While specialized chemistry hardware excels in conducting predefined experiments, their limited modularity can prove inconvenient for specific SDL configurations. Therefore, the application of general-purpose robotic arms for chemistry has been investigated due to their flexibility and multi-purpose nature. A well-known example of demonstration of generalpurpose hardware is the mobile robotic chemist by Burger et al. 11 (Figure 2b) In the study, they used a mobile robot arm, capable of moving around a traditional laboratory and operating various instruments, to search for optimal photocatalyst mixtures. They also demonstrated the reconfigurability of the setup, repurposing the system to perform solubility screening and crystallization.47 General-purpose robots have advantages over purpose-built flow platforms in that they can perform experiments that require physical interaction with tools and objects in the laboratory, thereby minimizing the reconfiguration and/or adaption of proprietary equipment or instruments designed for humans. However, major challenges in perception and decision-making limit the robust deployment of general-purpose robotic systems for flexible lab automation. For this reason, many works in the literature address lab automation for specific tasks�for example, mechanical tasks such as retrieving samples of crystals by scraping the wall of a vial48 and grinding powder with a soft jig.49 Pouring liquid using visual feedback50 and weight feedback51 have been studied as an alternative method of transferring liquid. Custom hardware built to assist robots in handling liquids have also been proposed, for example, Lim et al. used a custom syringe pump operated by a robot arm to conduct a molecule synthesis experiment.52 Knobbe et al. developed a robotic finger for operating electronic pipettes,53 and Zhang et al. used a designed end-effector for operating manual pipettes. Solid dispensing has also been demonstrated using a dual-arm robotic manipulator.54 Yoshikawa et al. demonstrated the use of a robotic arm for the more specific task of polishing electrodes used in electrochemical experiments.55 Nevertheless, besides the advantages of generality, multi-purpose robotic arm systems are lower in efficiency and hard to"
        }
    ],
    "2.1.3. Open Hardware for Lab Automation": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": " The cost of hardware automation is a limiting factor for SDLs. As a means of lowering the hardware cost and crowd-sourcing development and testing, various open hardware for lab equipment has been proposed.56 Users typically print the published design files with their own 3D printer and build the equipment. In addition to labware for human use, lab automation devices such as liquid handlers have been developed as open hardware. FINDUS57 is an open-source liquid handling workstation that costs less than US$400. OTTO58 demonstrated qPCR with a 3D printed liquid handler. Both systems benefit from readily accessible parts and sensors for error checking, though space efficiency and generalizability remain as challenges. PHIL59 is a personal pipetting robot that is compatible with microscopes, making it ideal for live cell studies but implementation in chemistry is limited. EvoBot60 is a reconfigurable liquid handling robot that improved modularity by introducing layers and modules. Building upon a well-established 3D printer technology, it is easy to implement, but the fixed tool design makes it challenging for complicated tasks. Jubilee61 is an open-source multi-tool gantry-style motion platform also based on 3D printing technology, which has been used to demonstrate liquid-handling tasks for synthesis of nanoparticles (NPs).62 It can mount/dismount tools automatically to perform multiple tasks, while community contributions are needed to develop more tools for chemistry applications. Sidekick43 is a liquid dispenser that features an armature-based motion system with a fully 3D-printed chassis and home-built syringe-pumps to realize lower costs, and can handle only a limited number of liquid identities at a time. 3D printers have been utilized for producing microfluidic devices63 or building a pipette for a two-finger robot hand to enable accurate liquid handling.64 Open hardware is beneficial to lowering the cost of building SDLs and their customizability is helpful in meeting individual requirements in different experimental settings which are not met by existing commercial hardware.65 However, the technical difficulty of setting up open hardware and the wide variety of similar hardware proposals hinder widespread adoption in laboratories other than the developers of the hardware. Further support by user communities is needed in facilitating the adaptation, and efforts in growing user communities have been made using online communication platforms."
        }
    ],
    "2.1.4. Perception and Computer Vision": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Execution of chemistry experiments autonomously requires several layers of feedback. Mimicking the visual feedback of a chemist’s eyes, a perception system should track the progress of the chemistry experiment and provide information to the robot such that it can achieve the high-level goal or direction of a given experiment. Computer vision can play a key role in this aspect. For example, HeinSight can provide perceptual information about the chemistry experiments.14,67,68 Connected to an experiment planning algorithm, that information could be used to guide the robot throughout the experiment. More recently, Sun et al. presented a vision-guided liquidliquid extraction platform, using image processing and computer vision to identify phase boundaries.69 In another example, authors used visual feedback to train a 3D-CNN model for viscosity estimation of fluids.70 At a lower level, the robot also requires visual and kinesthetic perceptual feedback in order to perform manipulation tasks successfully and robustly. Robots need to be equipped with accurate perception skills to work in unconstrained open workspaces. One of the characteristics in a chemistry laboratory is the use of transparent objects, such as glass containers. Transparent objects have different optical properties from opaque objects that make object detection challenging. Transparent glassware detection algorithms using depth completion13 and multiple images71 have been proposed. Public datasets, such as VectorLabPics dataset,12 have been published in order to accelerate the development of ML models for laboratory related computer vision"
        }
    ],
    "2.1.5. Manipulation Skill Learning and Digital Twin": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "In SDLs, general-purpose robots can interact with tools, objects, and materials within the workspace and require a repertoire of many laboratory skills. Those tools and objects can be in different forms, for example rigid objects like glassware, articulated objects with joints like cabinet doors, or soft objects like rubber tubes or powders and liquids. Some skills can be completed with existing heterogeneous instruments and sensors in chemistry laboratories, such as scales, stir plates, and heating instruments. Other skills are currently done either manually by humans in the lab or with expensive specialized instruments. In an SDL, robots should acquire those skills by effectively using different sensory inputs to compute appropriate robot commands. To effectively endow the robots with many skills in a scalable fashion, one approach would be to allow robots to “learn” those skills in a digital twin, a simulated laboratory environment in which the robotic system can interact with, using AI techniques.72 Digital twins can also be used for testing the workflows, algorithms, and scale-up developments.73 For example, ChemGymRL74 was proposed as an interactive framework for reinforcement learning in chemistry. Some examples of physics-based simulators include Gazebo,75 MuJoCo,76 and NVIDIA Isaac Sim.77,78 Recently, NVIDIA Isaac Lab,77 a modular framework on top of Isaac Sim, was introduced to simplify common workflows for robot learning that is pivotal for robot foundation model training. Some examples of robotic simulation environments and benchmarks are iGibson,79 MetaWorld,80 and BEHAVIOR1K.81 Closer to tasks related to laboratory automation, RB2 proposed a robotics simulation benchmark with pouring, scooping, and insertion tasks.82 In another work, a differentiable environment FluidLab83 was proposed for simulating complex fluid manipulation tasks. An example of using digital twin for the SDL is provided in Vescovi et al., where simulated environments have been used to visualize and compare tools, verifying the laboratory operations.74 These simulators leverage different physics engines, such as Bullet,84 FleX85 and PhysX86 and rendering happens via OpenGL87 or Unity.88 Although robot actions can be trained in a simulation environment at low cost, there are gaps between simulation and real-world settings. Multiple efforts have been made to close this sim-to-real gap,89 including for chemistry laboratory robotics. For example, Kadokawa et al. have trained a powder weighing action in a simulator and realized precise weighing in the real world.44 Nevertheless, high-fidelity and performant simulation of deformable objects and particle systems (such as fluids and powders), as well as the simulation of chemical phenomena in the context of SDLs, remain areas for future exploration."
        }
    ],
    "2.1.6. Robotics Safety in Laboratories": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "In chemistry labs, several types of safety risks put humans and the environment in danger, including mechanical, electrical, and chemical hazards. Therefore, multiple levels of regulations and guidelines are implemented in chemistry labs for safety and accidents.90,91 The presence of robotic systems in chemistry labs can affect the risk of accidents in several ways, necessitating a diligent focus on safety and risk management. Generally, automated experiments with robotic systems inherently create a safer workplace for humans, as the users are less exposed to hazardous materials. Even when autonomous experimentation is not possible, chemists can tele-operate robotic systems to perform experiments with hazardous materials�which has been pioneered, for example, in the handling of radioactive materials,92 or explosive compounds.93 However, particular attention should be given when humans are in proximity to robots or in the same lab space, especially when employing mobile robots for tasks such as sample transfer. The choice of robotic systems, whether collaborative or industrial, can affect the safety protocols. For example, when using industrial robots, safety fences or laser scanners are commonly used in the robot workspace. Ensuring human safety in shared spaces requires a comprehensive approach that encompasses both physical and psychological aspects. For physical safety, the literature advocates employing control and motion planning techniques to facilitate safe physical interactions and address pre- and post-collision scenarios. In the realm of psychological safety, considerations such as robot motion, speed, adaptability, and appearance play pivotal roles in reducing stress and fostering a sense of safety in humanrobot interactions. More information about robotics safety standards can be found in ISO 1021882, ISO/TS 1506683, and survey papers by Lasota et al. 80 and Zacharaki et al. 81 Additional safety issues may arise from the manipulation and perception capabilities of robotic systems, as these remain open problems in the community. When deploying such robotic systems in chemistry labs, if manipulation policies and the robot's decision-making abilities are not robust enough, it may lead to failures, increasing the risk of accidents. An approach to rectify this shortcoming is to consider constraints on the robot policies. For example, Yoshikawa et al. 94 used constrained motion planning when transferring liquids with a robotic arm to reduce the risk of spillage. Moreover, the ability of robots to detect accidents, take immediate actions, and notify humans are other important considerations. Overall, safety in laboratories with robotic systems is a multifaceted challenge that requires additional research at several levels, from generating potentially hazardous chemicals to experimental planning and automated experiments where robots are used."
        }
    ],
    "2.2. Software": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The software component of an SDL is composed of three distinct parts, which are executed by some orchestration software (Figure 3): (1) the control and communication system of the automated hardware of the laboratory, (2) the data extraction, management, and analysis of experimental output, and (3) the decision-making experimental planner. In recent years, the fields of chemistry and materials sciences have undergone a paradigm shift with the rise of AI. ML algorithms, particularly DL models, have proven to be indispensable tools in deciphering complex patterns, predicting chemical properties, and accelerating the design of novel materials with tailored properties."
        }
    ],
    "2.2.1 Orchestration": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The true potential of individual automated devices in chemistry is most evident when they are interconnected in order to orchestrate some comprehensive chemical tasks. Consider, for instance, the typical course of a chemical analysis, which involves a sequence of actions, including compound synthesis, thorough characterization, and meticulous processing of resulting raw data. The intricate interplay between these sequential steps underscores the indispensable need for standardized protocols to ensure the coherence and reproducibility of chemical experiments. Traditionally, these protocols were conveyed through research articles and manually executed by chemists. However, contemporary practices enable the translation of these protocols into orchestrated workflows executed by computational software, signifying a pivotal departure from historical methods. The integration of automated workflows has flourished within the field of computational chemistry, where the automation of repetitive and error-prone tasks is straightforward due to the programmatic nature of the field. This is evident in the emergence of tools such as AiiDA,95 Fireworks96 and Snakemake97 among others,98−107 which excel in constructing and managing software workflows for ab initio simulations. On the experimental side, autonomous chemical laboratories constitute an emerging field where the adoption of such orchestration techniques has been hampered by the physical challenges inherent to wet laboratories, the lack of an orchestration standard, and the scarcity of resources for automated instrumentation.108 Thus, it is imperative to acknowledge that the development of orchestration software for SDLs faces numerous challenges rooted in the methods used by the majority of researchers. These challenges include: • The absence of standardized application programming interfaces (APIs) provided by instrument manufacturers, often necessitating the development and utilization of workarounds that place a substantial burden on researchers; • The inherent software complexity of managing and orchestrating the transporting of items between chemicals processing stations;41 • Limited exposure to programming in current chemistry and materials science curricula. Addressing these challenges requires collective efforts from various stakeholders involved in current research, implementation, and deployment of SDLs. The widespread adoption of SDLs, particularly in industry, will compel manufacturers to provide user-friendly SDL solutions, efficient transfer systems between devices, and graphical user interfaces (GUIs) that facilitate the seamless integration of these platforms into the workflows of chemists. Numerous tools have surfaced in recent years to bridge this gap, with initiatives like the SiLA2 standard,109 a communication protocol aiming to replicate a robot operating system (ROS) and adapt it for chemical devices. Within this context, various in-house orchestrators have emerged in different laboratories across diverse chemical fields, with notable examples including ChemOS,110,111 Helao,112 and AresOS,113 among others.47,74,114−121 These experimental orchestration platforms have achieved significant advancements in key orchestration features that are standard in computationallyoriented platforms, such as queue management, logging capabilities, data handling, and, more recently, the implementation of asynchronous execution of laboratory tasks and their integration with computational frameworks.122 However, a lack of consensus between these platforms still prevails, and they often remain tailored to specific laboratories, lacking the required level of generalizability to cater to the diverse spectrum of SDLs."
        }
    ],
    "2.2.2. Communication and Protocol Management for SDLs": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "In the context of SDLs, where human researchers are the intended users, effective communication between researchers and the orchestration manager is paramount. This communication enables researchers to issue complex commands to the orchestrator that will be transformed into chemical operations, while receiving feedback and real-time updates on the status of laboratory processes in a readable format. In this regard, programming languages serve as the essential communication bridge, allowing users to convey instructions and request information from the orchestrator. Although general programming languages are frequently used to program chemistry hardware,123 for example the MOCCA124 open-source Python package which directly analyzes HPLC raw data and extracts relevant information, or Chemspyd125 open-source Python software for communication with proprietary Chemspeed software, specialized programming languages are proposed to efficiently describe chemistry experiments. Chemical Description Language (χDL)126 is an XML-based language used to describe chemistry experimental procedures, which was demonstrated by translating chemistry literature into χDL, and then synthesizing the described molecules. Chemical Markdown Language127 is another chemistry domain specific language to describe or assist in experimental documentation. While such languages are more tailed for communicating chemistry specific tasks, they will require a low learning barrier to ensure adoption in other SDLs. In recent years, there have been multiple examples of asynchronous workflows, in which SDLs operating in separate regions, with different research teams and equipment, work on the same discovery or optimization task. This requires extensive software infrastructure for the communication and coordination of results between the SDLs and the respective research teams. Multiple studies have demonstrated the use of internet cloud servers to manage and control distributed laboratory equipment.116,128 Decentralized databases can then allow for communication of experimental protocols, experimental results, and coordinated experiment planning over multiple laboratories, which have been demonstrated in some SDL orchestration softwares.110,111,129 Dynamic knowledge graphs have been proposed130 and demonstrated131,132 as an effective way of coordinating distributed SDLs. Ontologies are developed to capture various aspects of chemical research, including reactions, design of experiments, and hardware setups. Software agents are deployed at each lab site and act as executable knowledge components that can query, update, and restructure the knowledge graph autonomously, as the campaign progresses. Given the recent rapid developments in their capabilities, large language models (LLMs) have been investigated to accelerate chemistry research.133−136 In terms of communication, LLMs are able to interface with human users through text and conversation, translating between natural and machine language. For example, Boiko et al. demonstrated that LLMs can design and perform chemical experiments with a liquid handler based on natural language input from a user.137 The ability of LLMs can be expanded for specialized use cases by collaborating with external programs. ChemCrow138 is an LLM specially designed for chemical tasks, being able to observe, plan, and execute actions with integrated chemistry tools. CLAIRify139 introduced an iterative prompting strategy using automated verifiers to generate χDL, and demonstrated chemical experiments with a general-purpose robot. Likewise, ORGANA140 is an experimental planner that uses a LLM to communicate with chemists, and then plans and interfaces with a robotic arm to perform parallel tasks in an SDL experiment. The role of LLMs is discussed further in the subsequent sections."
        }
    ],
    "2.2.3. Data Management": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Automation accelerates data generation, and the large datasets must be managed efficiently in order to process and disseminate the generated data, particularly for the downstream use in data-driven techniques such as ML. Data management can be categorized into private databases, adept at housing all laboratory-generated data, and public databases designed to share curated and processed data for widespread use. Individual research laboratories often use private databases to facilitate record-keeping of chemical processes within the laboratory, and track chemical inventory and equipment availability. Traditionally, researchers have relied on laboratory notebooks and inventory software for these purposes, manually recording and annotating changes in experimental procedures. Annotated data would then be transferred for curation and processing, although inconsistent information tracking, and missing or biased data due to human error remain as issues. However, improvements in information technologies have changed data collection practices, with electronic laboratory notebooks emerging as modern alternatives to traditional notebooks.141,142 Efforts have been made in integrating private databases with SDL orchestration frameworks to keep track of the status of the laboratory47,110−112 or the status of simulations.95 However, it's worth noting that the adoption of these tools is not standardized across the chemistry community. Conversely, public databases play an indispensable role in the open science paradigm, adhering to the FAIR (findable, accessible, interoperable, and reusable) data principles143 by providing transparent access to experimental data for other scientists, thereby enhancing reproducibility. Computational chemists hold a long tradition of publishing standalone computational databases hosted on cloud platforms like Zenodo.144 These encompass a broad spectrum of materials, including MOFs, organic molecules, and heterogeneous catalysis. More advanced platforms such as the Harvard Clean Energy Project,145 IoChem-BD,146 Materials Project,147 NOMAD,148 The Protein Databank,149 Materials Cloud,150,151 Open Quantum Materials Database (OQMD)152 and Catalysis-Hub153 serve as noteworthy examples of public materials databases. These are typically built on generalpurpose database frameworks; for example, Materials Project147 uses MongoDB and OQMD152 uses SQL. The variety of public materials databases for computational data typically provide supplementary tools for data parsing, querying, and publishing. However, in the realm of experimental chemistry, there is a lack of tradition in publishing chemical results in structured and open databases; reaction and characterization data are commonly published as standalone datasets or in commercial databases. Notable examples include the Spectral Database for Organic Compounds,154 Reaxys,155 SpectraBase,156 SciFinder,157 and the chemical reaction patents from the United States Patent and Trademark Office.158 However, substantial efforts have been made to establish dedicated databases for storing experimental reactions and characterization data, with platforms such as Pubchem,159 Open Reaction Database,160 GNPS,161 Mass Bank of North America, 162 Crystallography Open Database, 163 MNRShiftDB164 and Molar.165 Due to the automation capabilities of SDLs, these databases are poised to play a critical role in the expansion of SDLs as they serve as a common interface bridging diverse research laboratories, facilitating seamless collaboration and data sharing among geographically dispersed research teams. Before concluding this subsection, it is worth mentioning the recent emergence of HuggingFace Hub,166 introducing an open database focused on collecting datasets and ML models. While the effort is currently focused on DL research, the future standardization of SDLs will likely require the adoption of similar solutions. This will enable laboratories to share components of research workflows more effectively. For instance, a synthesis, characterization, and ab initio simulation workflow could be assembled for an SDL setup by downloading independent parts from the repository, connecting them, and subsequently customizing them to align with the specific laboratory needs. This collaborative approach facilitates the sharing and improvement of research components among different laboratories, fostering innovation and efficiency within the scientific community, as has been demonstrated for the AI community."
        }
    ],
    "2.2.4. Role of Artificial Intelligence in Cheminal Discovery": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": " As large datasets of experimental and computational chemical data became accessible, data-driven statistical methods became more relevant to chemical discovery.167 Cheminformatics have been developed since the 1960s, particularly driven by advances in computing technology, and the development of ab initio techniques such as density functional theory (DFT).168 Early work focused on prediction of chemical properties, identifying quantitative structureactivity relationships (QSAR), for virtual screening of large libraries of pharmaceutical compounds.169,170 Statistical analysis of feature importance, such as through Shapley additive explanation (SHAP) values,171 have been used to provide intuition into the effect of certain chemical structures, properties, or experimental parameters in the model performance.172 ML methods such as (1) tree-based methods: random forests (RF)173 or gradient-boosted trees;174 (2) kernel-based methods:175 Gaussian process (GP)176 or support vector machine (SVM);177,178 and (3) clustering algorithms: knearest neighbors (kNN)179 or k-means clustering,180 were used to capture complex QSAR in chemical and material space. Chemical compounds can be described by machine-readable chemical descriptors (Figure 4), represented by vectors of physicochemical descriptors,181,182 unique fingerprints183,184 (e.g. extended-connectivity, path-based fingerprints),185,186 graph representations,187 and structured strings (e.g. SMILES,188 SELFIES,189 and group SELFIES190).191,192 More complex forms of chemical representations include 3D information, such as through Z-matrix or cartesian XYZ coordinates.149 Additionally, chemical transformations can be represented as SMIRKS193 and SMARTS,194 which extend beyond SMILES to facilitate the textual representation of chemical reactions, while graph encoding has emerged as a powerful approach for capturing the complexity of reaction networks.195−198 More recently, DL methods using neural networks have had successes in chemical applications, with the downside of sacrificing interpretability and requiring large amounts of training data.199,200 Neural networks are highly expressive nonlinear models that can be fit to complex data through backpropagation, capturing complex relationships in highdimensional input data. DL methods are now state-of-the-art for many chemical prediction and classification tasks, for example, graph neural networks (GNN) on molecular chemical data.201−204 Additionally, successes in natural language processing have led to LLMs which are able to extract meaning and context from natural language, and generate coherent responses.205 Molecular language of string representations have been incorporated with language models for property prediction.206 Various applications of language models to SDLs include allowing for orchestrator-to-human interactions through language, or translating natural language to robotic commands.139 Language models have also been used to gather data from the scientific literature, generating datasets in an automated way.207,208 Additionally, DL allows for data-driven generative modeling and ideation, reaching category 3 in software automation (Figure 1). Generative models incorporating neural networks have been used to generate novel chemical compounds and materials without human intervention, through the use of architectures like variational autoencoders (VAEs),209,210 generative-adversarial networks (GANs),211,212 gradient flow (i.e., diffusion) models,213,214 deep genetic algorithms,215,216 language models for chemical strings,217,218 and deep reinforcement learning (RL).219−221 By directly learning the chemical space of a dataset, the model can interpolate and extrapolate new compounds, and even directly optimize within the latent space through inverse design.222 Various in silico campaigns in generative inverse design and benchmarking have already been demonstrated,217,218,223−225 but issues with synthesizability and chemical stability of generative compounds remain a barrier to automated empirical validation. DL methods are capable of transfer learning, a technique commonly used in low-data settings, in which the model is pretrained with more readily available data that provides the model with implicit information about the main task.221,226,227 By leveraging the libraries of computational results, and historical empirical results, models can be preconditioned with physicochemical information for SDL campaigns that typically start in the low-data regime. Such models can even be used to encode chemical compounds as task-specific descriptors, compressing the chemical information into expressive abstract representations.187,228 Both traditional ML and DL techniques are now commonly used as part of optimization algorithms and experimental planners, which will be discussed in the next section"
        }
    ],
    "2.2.5 Experiment Planning": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The availability of data, coupled with the robust ML and DL models mentioned earlier, has created a demand for tools adept at processing the datasets. For practical examples, we direct the reader to various reviews that illustrate the utility of these techniques.199,229−235 Traditionally, brute force methods like combinatorial gridsearch and random sampling236,237 have been combined with high-throughput techniques to sample systems of interest. For instance, the Haber-Bosch process,238 which involved testing up to 4000 catalysts in 6500 experiments239 to identify suitable catalysts and experimental conditions for ammonia synthesis, required significant human effort and resources to complete. While such methods may work well and be preferable when dealing with a small number of parameters and low experimental costs, they quickly become unfeasible as the number of variables increases. In such cases, a methodical approach in the experimental space becomes necessary, particularly when computational or experimentation costs are a concern. Similarly, to make use of the improved precision of modern chemical apparatuses and sample preparation devices, exploring continuous variables in finer increments necessitates an increased number of experiments. Conventionally, scientists and engineers have used DoE strategies to systematically scan the experimental space, in an effort to reach the optimum, and identify the important parameters.240−243 A naive approach may be the one-factor-ata-time (OFAT) design, which involves manipulating a single parameter, assuming there are no correlated effects between the factors. In response surface methods such as Box-Wilson central composite design (CCD) and Box-Behnken design, the experiment list is populated by equally spaced points in design space, followed by polynomial fitting of the parameters to the response variable(s) for creating a response surface. This response surface can be then used for finding the optimum while the fitting parameters can be evaluated through statistical tests such as t-test,244 or analysis of variance (ANOVA)245,246 to assess the relative importance of variables as well as to confirm validity of the strategy. While amenable to the computation power available at the time, a disadvantage of DoE methods is the rigidity of the list of experiments, which remains the same as the results are collected. Furthermore, the equal spacing of selected points gives a diverse yet course-grained sample of design space, sacrificing precision in identifying the optimum. As the dimensionality of design space increases, DoE strategies become impractical, and likely insufficient. Especially when targeting software autonomy levels 2 and 3, advanced experiment planning algorithms must be capable of realizing closed-loop workflows. Such iterative global optimization algorithms must fulfill the following requirements: (1) the algorithm must take into account experimental observations from previous iterations, and use this knowledge to make more informed experimental recommendations; (2) as experiments are generally expensive and time consuming, optimization should proceed with the minimum number of required experiments; (3) the algorithm must treat the underlying response surface as a black-box the functional form of the optimization surface, or any gradient information, is usually not available from experiment. Some early approaches to the black-box optimization challenges in chemical and material domains is mimicking the successful strategies observed in biology, such as the evolutionary algorithms or genetic algorithms (GA) inspired by natural selection.247,248 In the context of experiment planning, each experimental setting refers to an individual species in a population. The fitness of each individual, which is associated with the quantities the algorithm is maximizing, is then used to assign a chance of producing offspring via crossover operations with other high-fitness species. With each generation, the population evolves to a greater fitness while random variations can be introduced through mutations that help prevent the GA from getting stuck in local minima. Similarly, particle swarm optimization (PSO) is an optimization tool inspired by flying flocks of birds where each particle represents a point in experimental design space and the velocities of the particles are analogous to update rates of the experiment parameters.249−251 The covariance matrix adaptation-evolution strategy (CMA-ES) is another evolutionary strategy, in which the species of a generation are selected by a probability distribution based on high-fitness individuals.252 Many of today’s pressing challenges such as catalyst discovery for sustainable energy applications, drug discovery, and synthesis optimization are analogous to finding a needle in the haystack, where only some narrow areas of the experimental space are highly promising and require detailed exploration. While GAs and PSO have proven suitable for complex problems, such methods tend to require a large number of samples as the design space grows. Simplex optimization, notably the modified simplex,253,254 has been another common heuristic optimization strategy that is also relatively simple and straightforward. Without any mathematical assumptions, simplex optimization gradually and systematically alters a virtual simplex constructed by vertices of previous experiments within the experimental design space, shrinking and expanding towards optimal settings with each subsequent evaluation. Despite multiple successful applications in chemistry255−263 since its first use in analytical chemistry27, simplex optimization’s final performance may be hindered by local optimality traps, noisy data and large number of variables. Another common systematic approach for experiment planning is Stable Noisy Optimization by Branch and Fit (SNOBFIT)35,264−266 which uses branching for exploration and bounding for elimination of irrelevant parts of experiment design space. SNOBFIT further aims to improve optimization efficiency by including local searches and deals with noisy data via robust sampling combined with statistical modeling of the noise. Finally, gradient-based numerical methods for optimization have also been used in chemical process optimization, such as the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm.267,268 These algorithms require more computational power, but typically converge faster than non-gradient methods, like SNOBFIT or simplex optimization.269 We provide a comparison of some common experimental planning algorithms in Table 2. There are a number of aspects to consider when designing or choosing experimental planning algorithms in the context of SDLs. For example, SDLs in chemistry often involve the simultaneous optimization of continuous, discrete (i.e., ordinal discrete parameters), and categorical parameters. For example, the optimization of a reaction may involve parameters which include continuous variation of temperature, discrete increments of reaction times, and categorical selection of reagents. In scenarios where there are small amounts of data, or when experiments are too expensive to perform, multi-fidelity optimization may be used to accelerate optimization. Multifidelity learning leverages information from both low-fidelity (cheaper, faster, but less accurate), such as ab initio calculations, and high-fidelity (expensive, slower, but more accurate) data sources to guide the search for optimal conditions. This can be done, for example, through transfer learning with DL methods,270−273 or delta-learning optimization.274−276 And in many automated experimental platforms, batch experimentation is typically used to parallelize synthesis or characterization, such as through the use of well plates. Experimental planners need to be able to optimally suggest batches of solutions while balancing exploration and exploitation of the search space. This is also known as the multi-armed bandit problem.277−279 Multi-objective optimization is particularly important in the optimization of chemical processes, and the design of new materials or molecules�for example, maximizing the activity of drug molecules, while targeting a specific solubility. Scalarizing functions are one of the simplest ways to incorporate multiple objectives into a single objective, with the simplest function being a weighted average of the various targets. Other more sophisticated scalarizers have been proposed, such as Chimera,280 which allows for user-specified hierarchical optimization of each objective, or the Pareto front hypervolume. The Pareto front represents the set of solutions where no single objective can be improved without degrading at least one other objective. Maximizing the hypervolume of the Pareto front yields a set of solutions that dominates a larger portion of the objective space, indicating better overall performance across all objectives and providing decisionmakers with a diverse range of trade-off solutions. Some such methods include: parallel efficient global optimization (ParEGO),281 which uses the Chebyshev scalarization; nondominant sorting genetic algorithm (NSGA)-II,282 and NSGAIII,283 which are GA approaches that optimizes by considering crowding distances of solutions within the Pareto front; smetric evolutionary multi-objective optimization algorithm (SMS-EMOA),284 which directly optimizes the hypervolume indicator. For more detailed discussion, we refer readers to Sharma and Kumar,285 Rangaiah et al., 286 Vel et al, 287 and Angelo et al. 288. "
        }
    ],
    "2.2.6. Bayesian Optimization and Active Learning": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "One of the most common strategies today for SDL experimental planning is Bayesian optimization (BO), which aims to maximize or minimize some black-box function, such as a measurable chemical property, as a function of controllable experimental parameters.289,290 To do this, a surrogate model with some prior distribution is fit, or trained, on the available data, and the predicted posterior distribution is used to create an acquisition function. The acquisition function contains information about the prediction and the uncertainty of the prediction based on the posterior distribution, and can be used to control the exploitative or explorative nature of the subsequent experiments. Some common acquisition functions include the upper confidence bound (UCB), a linear combination of the mean and standard deviation of the posterior, and the expected improvement (EI), the expectation value of the next point that most improves upon the best value. In multi-objective optimization tasks, some commonly used acquisition functions include the expected hypervolume improvement (EHVI),291 the noisy EHVI (NEHVI),292 or ParEGO. BO algorithms have since incorporated more complex nonlinear models using ML and DL models as surrogates, and have found success in more difficult chemical optimization tasks. Similarly, active learning is a sequential optimization algorithm, however the goal is to improve the performance of the surrogate model with each additional data point. The goal of both algorithms is to optimize to the respective goals in as few evaluations as possible, minimizing the number of expensive black-box function calls. This entire loop of experimentation, model training, and decision-making is repeated until a given experimental budget is reached or until the given target is achieved. While any model can serve as a surrogate model, GPs and RFs are typically chosen, as they train quite well with small amounts of data, which is typically the case in the early iterations of an SDL campaign. While GP models learn distributions of functions and intrinsically provide uncertainty estimates, RFs provide an uncertainty based on the ensemble of decision trees. DL surrogate models like multi-layer perceptrons (MLPs), convolutional neural networks (CNN), recurrent neural networks (RNN), or GNNs are less common in SDL applications, and usually require pre-training from larger datasets, (e.g., computational datasets). However, probabilistic DL models such as Bayesian neural networks (BNNs)293 have been shown to work relatively well with lower amounts of data, due to the regularization effect of the neural network weight distributions. Given the versatility and recent success of the BO methods for experiment planning, there has been an intense effort to provide software libraries targeting multi-objective optimization, compatibility with specialized material/chemical optimization tasks, better handling of categorical variables, increased accessibility as well as benchmarking. Shields et al. introduced an open-source Python package Experimental Design via Bayesian Optimization (EDBO)294 and provided multiple benchmarks. The authors showed BO with GP performs statistically better when compared to common DoE techniques suitable for both continuous and discrete variables in maximizing Suzuki and Buchwald-Hartwig reaction yields. While EDBO was capable of recommending experiments in batches with seemingly no performance loss, Tores et al. published EDBO+295 that extended the platform to tackle multi-objective tasks. They further augmented the platform with a cloud powered web-interface to provide accessibility to non-coder scientists as well. Hasë et al. developed the Bayesian optimizer Phoenics296 that addressed the issue of large numbers of samples required for chemical global optimization tasks, particularly where evaluation of a point in chemical design space is costly. By utilizing an autoencoder-like BNN for kernel density estimation from observations, a surrogate function can be constructed with higher efficiency. Phoenics was benchmarked on a reduced Oregonator model for the Belousov− Zhabotinsky reaction, and, when compared to RF, GP, PSO, and CMA-ES, Phoenics outperformed the other methods after only 25 evaluations. Later on, Hasë et al. introduced Gryffin,297 an extension of Phoenics with a particular focus on handling categorical variables. Additionally, Gryffin considers the correlation among the variables through the use of descriptors; for example, the physicochemical descriptors of selectable solvents. Among multiple optimization strategies and packages such as PyEvolve,298 SMAC,299 HyperOpt,300 and GPyOpt,301 the authors reported the best performance with the Gryffin optimizer. Other chemistry specific BO algorithms include Gemini,274 which extends Gryffin to multi-fidelity optimization, Golem,302 which identifies optima that are robust to input or measurement uncertainties, and Anubis,303 which incorporates unknown experimental constraints into the acquisition function. Recently, Hickman et al. introduced Atlas,304 an open-source software library incorporating the functionalities of the aforementioned BO softwares, including mixed parameter BO with a priori known and unknown constraints, as well as multi-objective, multi-fidelity and robust optimization capabilities. With an emphasis on integration with SDLs, the authors showcased the Atlas library embedded in the ChemOS 2.0111 SDL orchestrator for oxidation potential optimization of metal complexes using electrochemical measurements. It is also worth noting that there are many off-the-shelf and general-purpose BO packages available. BoTorch306 is a modern BO library built on top of PyTorch307, offering modular components, Monte Carlo (MC) acquisition functions, and a variety of advanced optimization features such as high-dimensional, multi-fidelity, multi-objective, mixed-variable, and constrained optimization. Notably, the Ax (Adaptive eXperimentation) platform is a high-level wrapper to BoTorch managed by the same developers which significantly reduces the required learning curve and has seen recent usage in materials informatics.308−310 GAUCHE, by Griffiths et al., 311 is an open-source GP framework built atop GPyTorch,312 BoTorch306 and RDKit,184 with a suite of custom kernel functions for GPs, and built-in performance and BO benchmarks for molecular and reaction discovery. Dragonfly313 is an optimization library for handling both multi-fidelity and high-dimensional optimizations, emphasizing adaptability to various domains. SMAC3 (Sequential Modelbased Algorithm Configuration)314,315 combines BO with RFs, and is particularly suited for algorithm configuration tasks. GPyOpt301 utilizes GPs as surrogate models for BO, offering a variety of GP-based acquisition functions. HEBO316 (Hierarchical Evolutionary Bayesian Optimization) integrates evolutionary strategies with BO, presenting a hierarchical approach to enhance search efficiency. HyperOpt300 is a Python library for serial and parallel optimization over challenging search spaces, utilizing techniques like tree-structured Parzen Estimator (TPE)317 rather than traditional BO. Thompson sampling efficient multi-objective optimization (TS-EMO)318 is a general-purpose BO with a GP surrogate allowing both multi-objective optimization and batch suggestions. While off-the-shelf models are convenient, studies have shown that careful selection of surrogate models can greatly influence the performance BO or active learning campaign. For example, for the GP surrogate, Noack and Reyes stress the importance of understanding the physical system in selecting hyperparameters, kernel and mean functions.319 Many studies commonly use standard kernels like the radial basis function, or Materń kernels, without considering the underlying physics of the system, anisotropy in the input features, or stationarity of the data points. Ziatdinov et al. proposed GPax,320 a novel approach that augments GPs with structured probabilistic models to incorporate prior physical knowledge into BO and active learning tasks. Unlike standard GP-based BO, GPax balances the flexibility of non-parametric GPs with the rigidity of parametric models encoding known physical behaviors. The authors demonstrate GPax's capabilities on synthetic test functions, as well as physical lattice models like the 1D and 2D Ising models, where it outperforms classical GPs in discovering optimal regions and reconstructing phase boundaries with fewer observations. Further studies have demonstrated the potential for GPax in improving optimization in highthroughput experimental studies,321−323 increasing explainability of the surrogate model in hypothesis learning.324−327 There can be significant performance variance between experiment planning approaches for different chemical and material optimization/engineering tasks, even for slightly different tasks within the same domain.328 Therefore, the benchmarks have been developed to evaluate the various methods, which can be useful when initially choosing an experiment planning algorithm and the associated hyperparameters. Olympus329,330 and Summit331 are examples of BO benchmarking platforms with realistic chemical tests and experiments. For finding efficient black-box approaches, Tom et al. studied the effect of different chemical featurizations and surrogate models on the predictive performance and uncertainty calibration on different small chemical datasets, and the optimization performance in the context of BO.332 Liang et al. benchmarked different BO flavors specifically on the materials science datasets covering a wide domain ranging from electrical conductivity of drop-casted composite blends to shape scores of 3D printed materials.333 The authors further defined useful metrics for evaluating the acceleration and overall performances of optimization. Rohr et al. compared performances of linear ensembles (LEs), RFs and GPs for active learning based minimization of multi-metal oxide catalyst compositions’ overpotential toward oxygen evolution reaction.334 We summarize some commonly used BO tools for experimental planning in Table 2."
        }
    ],
    "3. Analytical Process Optimization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The earliest examples of chemistry SDLs that automatically perform a sequence of experimental tasks, planned by a data-driven algorithm in a closed loop, largely stem from the field of analytical chemistry. As early as in the 1960s, the optimization of measurement parameters to maximize e.g., the response of a single instrument, has been addressed in an iterative closed-loop fashion. Whereas these approaches do not fall under the scope of this review, and are routinely implemented into modern (analytical) instruments, it is remarkable that this iterative optimization, taking into account data from previous iterations (in contrast to e.g., PID controllers) has already been achieved more than 50 years ago[cite: 208]. As an example, Ernst et al. demonstrated the autonomous optimization of magnetic field homogeneity in a nuclear magnetic resonance (NMR) spectrometer by adjusting currents along the spinning axis, controlled by a gradient-based and a simplex-based algorithm[cite: 208]. To the best of our knowledge, this work represents the first published example of a simple Level 2 SDL in the field of chemistry.\n\nLevel 3 SDLs are realized when coupling an analytical technique to an upstream operation such as automated sample preparation or separation[cite: 225]. In these setups, the goal definition of the SDL is the identification of those process conditions that optimize the detectability or quantifiability of specific materials. Whilst these tools can be regarded as components of larger SDLs in materials discovery, the development of robust analytical methods has been an active field of research for multiple decades, and has been addressed using SDL approaches early on. In this section, we will review the autonomous optimization of such analytical processes, and further related experimental procedures."
        }
    ],
    "3.1. Composition and Detection Process Optimization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The identification of ideal measurement parameters for analytical processes is of enormous importance across all chemical industries, and has attained considerable attraction from the standpoint of autonomous optimization[cite: 226]. Beyond simple PID-type controllers, however, due to the limitations of computational power and automated laboratories before the 2000s, experimental planning was done primarily by simplex optimization, and automation typically done through simple step motors and flow systems [cite: 226-227]."
        }
    ],
    "3.1.1. Sample Preparation": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The earliest examples of what would be considered a Level 3 SDL as per the definition of this review, have been reported in the 1970s, addressing the optimization of analytical procedures for spectroscopic detection[cite: 227]. Typically, in these works, the optimization would target the automated sample preparation for a subsequent spectroscopic detection, in order to maximize the spectroscopic response for the material of interest.\n\nOne of the first examples of automated optimization of chemical detection was from King and Deming in 1974[cite: 228]. The SDL used a rotating motor with attached pumps to dispense various reagents into a flow system. The chemical system studied was the acid-dependent chromate-dichromate equilibrium. Characteristic peaks in the measured UV-Vis absorption spectra correspond to an equimolar solution. Using simplex optimization, the intensity of the characteristic absorbance peak was maximized as a function of the chromate pump speed over the course of 26 automated experiments[cite: 228]. Following this initial work, a series of similar publications appeared that used automated continuous flow analyzers to perform Simplex optimizations over multiple variables for the detection of glucose [cite: 228-230].\n\nMieling et al. developed a more sophisticated flow system by introducing automated flow-stopping, controlled by a magnetic-tape minicomputer with an analog-to-digital converter[cite: 230]. The flow stopping method allowed for automated solution preparation. The authors demonstrated the capabilities of the platform by creating a series of solutions of varying concentrations. The platform was then used in the detection of titanium through its reaction with hydrogen peroxide in the presence of ethylenediaminetetraacetic acid (EDTA), monitored by absorption spectroscopy and optimized by simplex optimization[cite: 230]. Further advances in analytical laboratory automation led to works involving the Zymark robotic arm, capable of sample preparation, solution addition, and sample transfer into a spectrophotometer[cite: 230]. Lochmüller et al. utilized this SDL to optimize the concentration of MgIn, monitored by UV-Vis absorption spectra, through a reaction that is dependent on Ca2+ ion concentration and pH[cite: 230]."
        }
    ],
    "3.1.2. Separation and Chromatography": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The most prominent and widespread class of coupled analytical techniques are chromatographic methods, in particular gas chromatography (GC) and liquid chromatography (LC)[cite: 231]. While, as of today, commercial instruments are sold as integrated solutions, the underlying process is composed of two major operations: first, separation of the analytes occurs on a stationary phase column, and the mobile phase stream is transferred to a downstream detector for measuring the analyte response as a function of time. In this context, identifying the right conditions that enable good and efficient separation of unknown compound mixtures, represents a major challenge [cite: 231].\n\nFoundational work towards the use of data-driven algorithms, particularly the simplex algorithm, for chromatographic separation optimization had been performed in the 1970s[cite: 231]. Shortly after, the first autonomous examples of separation optimization for high-performance liquid chromatography (HPLC) were reported by Berridge in 1982[cite: 231]. Using the simplex algorithm for experiment planning, the authors optimized the eluent composition to maximize the chromatographic resolution for mixtures of 4-5 organic compounds, detected on a single-wavelength UV spectrometer (Figure 5). Building on this foundational work, Berridge and co-workers reported a series of further advancements, including multi-parameter optimization, constrained optimization, and multi-wavelength detection [cite: 231].\n\nVery recently, Boelrijk et al. show the use of BO tools for a fully autonomous optimization of multi-step gradients in HPLC[cite: 234]. Using a multi-objective strategy for simultaneously optimizing resolution and elution time, the authors showcase the autonomous development of separation gradients for complex dye mixtures consisting of up to 50 different analytes while only performing ~30 experiments[cite: 234]."
        }
    ],
    "3.2. Other Properties": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Similar to the discussed works on automated sample preparation, other properties such as solution properties of pH or solute concentrations (for crystallization) have been addressed using automated liquid handling systems, instructed by data-driven algorithms in a closed-loop fashion[cite: 235]. Solid state properties such as X-ray diffraction (XRD), and small-angle X-ray scattering (SAXS) signals have also been optimized in a closed-loop manner.\n\nAs a prominent example, Clayton et al. used an automated flow system SDL to determine the solvent volume ratio and pH for liquid-liquid extraction[cite: 237]. The authors studied the separation of a-methylbenzylamine and N-benzyl-a-methylbenzylamine dissolved in toluene, while the solvent and hydrochloric acid flow rates, temperature, and the residence times were varied. The outputs from the separator were analyzed by an on-line HPLC, with the goal of maximizing the amine purity for the two compounds. To extend into multi-step process development, the liquid-liquid extraction was performed in tandem with the reaction of a-methylbenzylamine and benzyl bromide to form N-benzyl-a-methylbenzylamine[cite: 237]. The purity of the product was the optimization objective, with the reaction mixture containing unreacted reactants and various amine-containing impurities. The optimal purity was at 71% which was identified in 53 experimental iterations (Figure 6)[cite: 237]."
        }
    ],
    "4. Reaction Optimization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "For over a century, materials discovery has been governed and constrained by the ability to synthesize chemical compounds, and make materials. This applies to molecular discovery (drug discovery, agricultural chemistry, molecular optoelectronics) in particular, where synthesis often represents a tailored sequence of highly specific reaction steps, each of which comes with a set of variable parameters and process conditions. Both the discovery of optimal synthetic routes and the optimization of optimal reaction conditions for each step is therefore critical to all fields of materials discovery, and chemical industries. In fact, the industrial need for economic and ecological synthesis processes has led to the discipline of process and reaction chemistry, targeting route identification, reaction conditions, as well as design and engineering of reactors for synthesis on scale. Due to the importance of reactions in chemistry and chemical applications, a range of closed-loop workflows for reaction optimization have been developed over the last decades, particularly targeting the identification of optimal reaction conditions. This section aims to summarize these literature-known approaches and examples of self-optimizing reactors. In contrast to the following chapters of this review, we will not focus on the optimization of materials properties, but rather on optimizing the ways to synthesize a specific material. Given that the vast majority of studies has focused on organic reactions in solution, this chapter will introduce the major concepts using this class of transformations. In the following subsections, approaches toward other solution-phase reactions, as well as non-solution-phase reactions, will be discussed. Afterwards, this section will provide a comprehensive overview of all works in which the automated integration of robotic reaction execution and data-driven optimization has been demonstrated in multiple iterations for enabling autonomous reaction or process optimization (i.e., Level 4 SDLs, Figure 1). For a more global discussion from the perspective of reaction optimization, we refer the reader to existing reviews and perspectives in the field.375−383 For solution-based reactions, the search space comprises a wide series of categorical (identity of reagents, catalysts, solvents, additives etc.) and continuous (relative stoichiometries, concentration, temperature(s), reaction time(s) etc.) variables. With the goal of automating the optimization process, the choice of the appropriate automation platform for performing the reaction (see Hardware) facilitates or complicates the variation of specific parameters. Generally, the variation of continuous parameters, particularly reaction quantities, can be readily performed on a wide variety of experimental systems (Figure 7). This has led to a large number of studies for optimizing continuous reaction parameters in an automated fashion. On the other hand, optimizing over categorical parameters, such as specific choices of chemicals, poses additional challenges both from the hardware and the software side; the physical availability of reactants and reagents, as well as the storage capacities on the automated platforms, pose physical constraints to the number of available categorical options. As a consequence, most examples of closed-loop reaction condition optimization have operated on comparatively few options (usually < 10) for categorical variables. Additionally, categorical parameters are not readily represented numerically, and lack an unambiguous order or measure of similarity (e.g., between solvents, or catalysts). This requires human decision in selecting an appropriate representation for chemicals for the optimization algorithm. Optimization algorithms that operate on molecular entities, as well as mixed continuous−categorical parameter spaces, are discussed in detail in Software."
        }
    ],
    "4.1. Spezialised Hardware and Software": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The primary objective of optimizing a synthetic reaction is generally the reaction yield, i.e., the quantity of the desired reaction product that is formed. Reaction selectivity, defined as the ratio between the desired product and an undesired side product (e.g., a constitutional isomer or a stereoisomer) can be regarded as an auxiliary measure of product formation. In terms of prediction, this a particularly challenging objective; reaction yields do not only depend on the rates of all steps in the desired reaction sequence, but also on the rates of a multitude of possibly unknown side reaction pathways, which renders the physics-inspired modeling of reaction yields highly difficult. Added complexity stems from the coupling of chemical reaction kinetics with physical transport phenomena (e.g., mass transfer/diffusion, and heat transfer). This dependence on unknown steps and mechanisms can lead to crossdependencies between the assumedly independent variables, and unforeseen activity cliffs upon small variations. In these scenarios, traditional OFAT optimization approaches, which have been the method of choice for reaction optimization in most laboratories, face severe challenges. In this context, the optimization of chemical reaction yields is particularly well− suited for data-driven, system-agnostic optimization approaches (see Software for further details). When it comes to larger-scale reactions and industrial process optimization campaigns, reaction yield or selectivity is no longer the sole optimization objective. Economic and ecological considerations present further constraints to the optimization problem, which can include: reagent and energy costs, atom economy, or environmental impact factors, such as measures of waste formation, or operational and environmental hazards"
        }
    ],
    "4.1.1. Reaction Execution": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "On a laboratory scale, the execution of solution-phase chemical reactions can be classified into two complementary strategies: batch and continuous-flow operation (Table 3). Both strategies come with distinct chemical (dis)advantages for performing specific types of reactions, which have been thoroughly discussed in the literature, and are outside the scope of this review. Instead, we aim to provide a discussion of these strategies in the context of automation, and developing autonomous self-optimizing reactors. For over a century, solution-phase reactions have been predominantly performed in batch reactors384�a fact well reflected in the practical chemistry education, where synthesis is mainly taught using beakers, flasks and vials as batch reaction vessels. In a teaching laboratory, and in many research laboratories, this approach is highly advantageous, as it allows the execution of a variety of different chemistries with minimal hardware requirements. In fact, most reactions can be executed by a human operator in standard, general-purpose glassware. However, batch reactions face severe challenges when it comes to isolation and purification, or the execution of multi-step reactions. The human-centric approaches to reaction workup and purification, such as extraction, crystallization or chromatography, are often based on specific operations that require large degrees of adaptive, intuition-guided decisionmaking. This has rendered their direct, programmatic translation into automated workflows challenging, and may be one of the reasons why batch reactors have not found widespread application in closed-loop discovery workflows yet. Recent advances in biotechnology and the corresponding liquid handling systems (see section on Hardware for further details) have enabled the miniaturization and parallelization of batch reaction execution in multi-well plates. In such cases, analyzing directly on the crude reaction mixture can significantly increase the automated experimental throughput, particularly when it comes to varying categorical variables such as the identity of reactants, reagents, catalysts or solvents. Therefore, such HTE systems have been primarily used for large condition or substrate screening campaigns for important catalytic reactions. As a complementary approach, the past decades have seen the development of flow reactors, in which reactions are performed in a continuously flowing stream of liquid. Importantly, with the requirement to continuously pump the reagent, reactant and reaction streams, this strategy to reaction execution is inherently automated. Still, flow reactions require highly specialized hardware and software, preventing the widespread adoption of this technology as a tool for chemical synthesis. Reactants and products must also be gaseous or liquid, and the liquid medium cannot be too viscous. Developments in microfabrication have led to miniaturization of flow systems into microfluidic systems (sometimes called lab-on-a-chip).385 The small footprint and low reagent consumption of microfluidic systems make them useful for high-throughput synthesis of compounds at small scales. The operation of chemical reactions in microfluidic systems provides a series of distinct advantages in terms of heat and mass transfer, interphase reactivity or safety. For more detailed discussions on flow and microfluidic reactors, we refer the reader to review articles in the literature.385−387 As such, both academic researchers and industrial teams have focused on developing automated in-flow synthesis platforms, leading to a large variety of specific, custom-built setups, with few standardized solutions on the market. From the perspective of autonomous optimization, the inherently automated nature of flow reactors made them ideal platforms for early explorations of autonomous operation modes, and a vast majority of examples of closed-loop reaction optimization (see below) have been performed on continuousflow platforms. These platforms have allowed for optimization of continuous parameters such as stoichiometries, reaction times, and temperatures, which can be readily varied in sequential experiments, leading to high experimental throughput. The exploration of larger numbers of categorical entities such as reactants or reagents, however, comes with increased hardware requirements and experimental efforts. Recent work has also demonstrated autonomous flow systems in Schlenk lines, allowing for studies involving highly reactive or sensitive compounds.388 Another major advantage of flow chemistry lies in the ability to telescope individual operations (including purification steps) into longer sequences, enabling the automated operation of multi-step sequences. This capacity, which has been reviewed comprehensively in the literature,389,390 has enabled the closed-loop optimization of multi-step reactions in solution, which will be discussed in the Multi-step organic reaction section. For screening and optimization purposes, segmented-flow approaches (often referred to as droplet reactors) provide an attractive approach for HTE in flow systems.386,391,392 Rather than having a continuous flow of liquid in which the reaction occurs, the stream is divided into small separated segments by an inert gas (such as argon) or immiscible liquid (such as perfluorinated oil). Each of these segments can be regarded as an individual batch, and precise operational control can allow for screening distinct reaction conditions (e.g., reagent identities or quantities) in each of these batches. Moreover, the smaller reaction volumes and high surface-to-volume ratios have demonstrated significant acceleration of reactions, leading to micro-droplet approaches such as microfluidic flow droplet reactors, and even free-standing non-flow systems.392"
        }
    ],
    "4.1.2. Reaction Analysis": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Irrespective of the reaction execution platform, the second major component of a selfoptimizing reactor is a module to quantify the optimization objective, such as the reaction yield. The most widespread, general-purpose approach to this is the use of an automated chromatographic separation technique, usually LC, or GC, coupled to a quantitative detection technique. In this approach, an aliquot from the reaction mixture is taken and analyzed on the external instrument (Figure 7). Importantly, the required instruments have been commercialized for decades, and offer robust hardware solutions, which are available in most experimental laboratories. As an alternative, in situ monitoring techniques can be used to directly analyze the crude reaction mixture and monitor changes in the reaction composition to quantify possible optimization objectives. Spectroscopic tools such as NMR spectroscopy (implementable through benchtop spectrometers with flow cells) or infrared spectroscopy (flow cells or in-situ probes) can be used to identify and quantify compounds if unique, compound-specific signals exist. Further, in situ probes such as UV-Vis spectroscopy or conductivity cannot, in most cases, be used to quantify specific materials, but allow for the monitoring of global properties of the reaction mixture, which can serve as a valuable proxy for the actual optimization objective. Analytical techniques that enable the quantification of reactants, intermediates or products throughout the course of a reaction can enable decision-making in real time, e.g., for adjusting reaction times, temperatures or reagent quantities. Such adaptive optimization of reaction conditions, however, does not follow the iterative closed-loop definition of SDLs, and therefore exceeds the scope of this review."
        }
    ],
    "4.1.3 Early Examples of Autonomous Condition Optiimization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Attempts to develop SDLs for aut nomous reaction condition optimization date back to the 1970s, when Winicov et al., from pharmaceutical company Smith, Kline & French, describe a fully automated batch reactor which shows remarkable similarities to modern open-source systems for automating batch reactions such as the Chemputer. 393 The authors describe automated modules for liquid addition (through pumps), stirring, heating and cooling, as well as reaction analysis by coupling to an HPLC-UV system. Remarkably, they even discuss the coupling of their platform with a simplex algorithm for automated experiment planning. However, no actual experiments have ever been publicly reported with this platform, neither in the initial publication from 1978, nor in any follow-up works; this is likely due to the proprietary nature of the research at Smith, Kline & French laboratories. To the best of our knowledge, the first published example of a self-optimizing reactor stems from 1987. Matsuda et al. reported the autonomous optimization of the adduct formation reaction between phosphotungstic acid and basic drug molecules, namely, chlorpromazine hydrochloride and levomepromazine hydrochloride.260 For this purpose, the authors had developed a robotic platform consisting of a Zymark robotic arm (see Figure 8 for a related experimental setup by Frisbee et al. 394) with two exchangeable tools for liquid transfer and vial transport, respectively. The reagents were added as solutions to a batch reactor vial, and the entire vial was first transported to a vortex mixer for stirring, and subsequently to a water bath for heating. Since the desired adducts are strongly colored, they can be detected quantitatively via steady-state absorption spectroscopy. After completion of the reaction, the vial is transferred to a UV-Vis spectrophotometer by the robotic arm, and the absorption at wavelength 538 nm was recorded as a proxy for the reaction yield. The simplex algorithm was used for iteratively planning the next experiment, varying the quantity of phosphotungstic acid and the reaction time. Remarkably, the authors show that the optimal reaction conditions can be found in less than ten iterations for both drug molecules. Using the same experimental setup, the authors demonstrated the optimization of a significantly more complex reaction in 1988 (see Figure 9):259 the conversion of a carboxylic acid to the corresponding hydroxamate using N,N’- dicyclohexylcarbodiimide (DCC) and hydroxylamine, followed by the complexation of the hydroxamate with an iron(III) salt to give a colored, UV-Vis-detectable complex. The authors demonstrate the optimization of up to four continuous parameters (quantities of DCC and hydroxylamine, reaction times of both steps), showing that optimization can be performed in under 30 experiments. The authors benchmarked their optimizer against a grid search strategy, demonstrating a significant reduction (>75%) in the number of required experiments. Inspired by these early results, and the sophisticated automation platforms developed in the 1980s (see e.g., Figure 8), Lindsey and co-workers made a series of contributions to early SDLs in solution-based synthesis. On the hardware side, the design of their “automated chemistry workstation”395 is of note; in parallel with the developments of small-scale pipetting robots in biochemistry that spilled over to chemistry only a decade later, their system enables the miniaturization of chemical reactions (to μL scale), as well as advanced analytical techniques, including, but not limited to, automated thin-layer chromatography. In addition to a series of automated data generation workflows, as a proof-of-concept, they demonstrated the closed-loop optimization of an “optical filter,” creating a specific absorption profile by mixing different dye solutions. Experiments are planned iteratively by the simplex algorithm, and are executed sequentially on the automated platform until the desired absorption profile is reached.396 Showcasing an application in synthetic chemistry, the authors perform the closed-loop optimization of the synthesis of porphyrin dyes from aromatic aldehydes and pyrrole under acidic conditions.397 The concentrations of both reactants (in 1:1 stoichiometry) and of the acid additive were used as independent variables. The yield of the product, obtained by quantitative DDQ oxidation, was determined by UV-Vis spectroscopy. The authors demonstrate accelerated experimentation by comparison with a full factorial design approach (Figure 10). Beyond these works, Lindsey and co-workers made a series of important contributions to advance optimization and decision-making algorithms beyond the native simplex algorithm,375 for example, using decision tree algorithms to handle screening and optimization of categorical variables. While such systems may have been utilized in industrial settings, given the research effort from both academic and industrial researchers,398 publications from the late 1990s for SDL optimization of synthetic reactions are sparse. The cost and reproducibility of the robotic hardware, as well the transferability of software may be influential factors in this regard."
        }
    ],
    "4.2. Single-Step Organic Reactions": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The rise of flow chemistry as a versatile, automated tool for reaction execution, as well as the increased accessibility and distribution of software via the internet, have sparked new interest in the development of self-optimizing reactors in the late 2000s and early 2010s. Since then, a large number of examples focusing on the optimization of organic reactions in solution have been reported in the literature. This section will present the most important concepts and advances using selected examples. A complete, to the best of our knowledge, list of further examples from the literature, including the target reaction, independent optimization variables, hardware, software and optimization objectives is given in Table 4. Detailed discussions of single- and multi-step reactions optimization are provided after."
        }
    ],
    "4.2.1. Self-Optimizing Flow Reactors and Analytical Advances:": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The first modern examples of self-optimizing reactors were reported by Jensen and co-workers in 2010.269,399 In their first work, McMullen et al. show the optimization of reaction conditions for the Heck-coupling between 4-chlorobenzotrifluoride and 2,3-dihydrofuran in a flow microreactor.399 The optimization was carried out to maximize the HPLC-determined yield of the mono-arylated reaction product, which is prone to undergo an undesired second coupling. Categorical parameters such as solvents, phosphine ligands and palladium sources were systematically screened to find conditions under which ammonium salts are soluble and the formation of palladium black is minimized, as this leads to clogging of the microreactor. Subsequently, a closed-loop optimization campaign over the continuous parameters residence time and alkene:aryl chloride ratio was carried out using the Nelder-Mead simplex algorithm. The authors show that the optimal conditions can also be carried out in a meso-reactor while preserving the optimum yield, demonstrating the successful transfer from micro- to mesoscale systems. Notably, even though over 20 years had passed since the initial demonstration of self-optimizing reactors, the selected optimization algorithm is highly similar to the early works discussed above. Using a similar setup, McMullen et al. reported the evaluation of multiple “black-box” optimization algorithms,269 namely the steepest descent algorithm, the Nelder-Mead Simplex algorithm253 and SNOBFIT,335 for the closed-loop optimization of the Knoevenagel condensation of p-anisaldehyde and malonitrile in a flow microreactor (see Figure 11).269 All algorithms converged to essentially the same optimum conditions within 12 hours. The authors optimized a weighted objective function of product yield and flow rate, thereby showcasing the first multi-objective self-optimization. Shortly after, in 2011, Poliakoff and co-workers reported a series of examples in which they employ their SDL for the optimization of reactions using γ-alumina as a heterogeneous catalyst in supercritical CO2 as the solvent.400 Parrott et al. optimized the yield of the dehydration of ethanol, the yield of the carboxymethylation of 1-pentanol with dimethyl carbonate (DMC), and the yield of the methylation of 1-pentanol with DMC. All of these optimization runs used a super-modified simplex algorithm to optimize temperature, pressure and CO2 flow rate as variables (see Figure 12). The latter optimizations were each completed in approximately 1.5 days, whereas a combinatorial search in the condition space would have taken more than 50 times longer, showcasing the efficiency of selfoptimizations. Later, Bourne et al. re-evaluated the methylation of 1-pentanol using different methylating agents in a fourvariable optimization, using the super-modified simplex algorithm.443 In a following study, Jumbam et al. evaluated different objective functions for optimizing this transformation:401 the yield, the space-time yield, the E-factor, E+ (the Efactor including all wastes) and the weighted space-time yield, calculated by the product of the space-time yield and the yield. The different criteria were shown to result in different optimal conditions. Surprisingly, a low E-factor led to a high value of E +. Overall, this shows the importance of designing an appropriate objective function when considering multiple inherently competitive optimization targets. After these initial developments of self-optimizing reactors, a diversification of analytical techniques took place rapidly, allowing researchers to harness the different advantages of each technique (see discussion above). In 2012, Moore and Jensen reported an in-line flow IR cell to optimize the Paal-Knorr synthesis of pyrroles.402 With this IR setup (Figure 13), steadystate conditions can be ensured before objective functions are evaluated. As a first objective function, the authors aimed to maximize the ratio between conversion and residence time. However, this led to poor conversions, and a quadratic loss function was applied to yields lower than 85%. The newly designed objective function resulted in an optimum of 81% conversion, demonstrating the difficulty of selecting combined objectives in multi-objective optimization, and highlights the importance of multi-objective optimization algorithms. This initial development has inspired the adoption of in-line IR techniques in a variety of self-optimizing platforms, such as the optimization of an Appel reaction,117 a [2+2] cycloaddition413 and others,116,265,403,419 pointing out the efficiency improvements, owing to the ability to circumvent time-intensive chromatographic methods. In 2015, Sans et al. reported the use of in-line NMR spectroscopy for the self-optimization of the acid-catalyzed imine formation between 4-fluorobenzaldehyde and aniline.404 Recorded 1 H NMR spectra were automatically phased and baseline-corrected, and the peak integrals were automatically evaluated to optimize an objective function related to the space-time-yield (Figure 14). Even though benchtop NMR spectrometers are commercially available at a reasonable price, their low sensitivity, as well as the difficulty of identifying and resolving characteristic signals, have prevented a more widespread usage in self-optimization platforms. A notable exception is the synthesis of the natural product Carpanone by Felpin and co-workers.416 In the same year, Holmes et al. demonstrated the usage of online quantitative MS for self-optimizing the synthesis of N’- methyl nicotinamide from methyl nicotinate and aqueous methylamine, varying the flow rate of methyl nicotinate, the quantity of methyl amine and the temperature as continuous independent variables (Figure 15).406 Before the selfoptimization experiments, HPLC was used to calibrate a benchtop MS, in order to use the latter for product quantitation without prior purification in the SDL campaign. The authors compared a self-optimization using the SNOBFIT algorithm335 with a classical DoE statistical design approach, finding that both methods found high-yielding conditions. However, while the SNOBFIT algorithm took 12 hours to find the optimum, the DoE approach only took 5.5 hours, due to the human intuition provided in the DoE: heating and cooling of the reactor is time-consuming, so avoid large jumps of temperature in the selected experimental parameters. Despite being commonly used for product identification in conjunction with HPLC, online MS was not widely adopted for quantification in self-optimizing platforms. Other examples are the hydrolysis of 3-Cyanopyridine by Ley and coworkers,117 and the optimization of multiple reactions with RL by Zare and co-workers (vide infra).336 In addition to the integration of more analytical techniques into self-optimizing systems, the used reaction platforms have also seen a significant diversification. On the level of equipment, Fitzpatrick et al. have demonstrated the LeyLab, whose components were designed to communicate via the Internet and are thus accessible through every browser with Internet Access.117 The LeyLab consists of four parts, a graphical user interface (GUI), a database for information storage, an equipment communication module and an equipment command module. Among others, they used this platform to optimize the Appel reaction of 1-phenylethanol, or the hydrolysis of 3-Cyanopyridine over a heterogeneous MnO2 catalyst using a flow reactor setup. The same group further used their internet-based lab in an across-the-world optimization of the syntheses of multiple active pharmaceutical ingredients.116 The optimization was initiated by a researcher residing in Los Angeles (California, USA), directed by remote servers in Japan and carried out in Cambridge (UK). Similarly, Skilton et al. demonstrated remote controlled self-optimizing reactors,128 where collaborators from China, Ethiopia and Brazil directed the optimization of self-etherification of npropanol and methylation of n-butanol and n-propanol through the cloud. In their commentary, the authors note that “watching an optimization in progress can be quite addictive, rather like watching the bids rising during an eBay auction” and further comment on the safety issues, intellectual property and financing of such cloud-based laboratories. A modular flow system was introduced in 2018 by Bedard ́ et al. for autonomous reaction optimization.265 The system consisted of several bays that each could fit a modular unit, e.g., a photo-reactor, a heated reactor, a cooled reactor, a packed bed reactor, a liquid-liquid separator or a bypass (Figure 16). Moreover, the system was connected to in-line analytics such as HPLC, MS, IR and Raman. The authors showcased the modularity of the platform by optimizing the conditions for maximal yield of a multitude of reactions, namely a BuchwaldHartwig Cross coupling, a HWE Olefination, a reductive amination, a Suzuki-Miyaura cross coupling, an SNAr reaction, a photoredox reaction and a multi-step ketene generation followed by a 2+2 cycloaddition. For each reaction, the authors manually designed the appropriate flow system, which was then used to autonomously optimize reagent equivalents, residence times and bay temperatures as independent variables. After each successful optimization campaign, the optimal conditions were examined for different substrates. In one case, where the conditions did not lead to a satisfactory yield for a specific substrate, a re-optimization was conducted with a subset of variables within 6 hours, improving the yield from 67% to 97%, demonstrating the flexibility and efficiency of the flow platform. Despite the prevalence of flow reactors, other reactor types have also been applied recently in self-optimization campaigns.In particular, Clayton et al. demonstrated multiple cascaded CSTR reactors which can provide conditions similar to a flow system while decoupling mixing performance from flow rate, thereby facilitating multiphasic reactions.421 Further, they also allow experiments with reactions involving solids and slurries, for which clogging is often a problem in conventional flow systems. The latter was utilized by Nandiwale et al. in 2022 for the successful self-optimization of a Pd-catalyzed SuzukiMiyaura coupling, as well as two metallaphotoredox-catalyzed Csp3−Csp3 and Csp3−Csp2 couplings of alkyl carboxylic acids and halides, respectively.428 Each of the investigated reactions involved at least one solid reactant, catalyst, additive or product, which could be transferred as a slurry in the CSTR. Leonov et al. report the development of an integrated selfoptimizing programmable chemical synthesis and reaction engine.442 They incorporated various sensors, including those for monitoring color, temperature, conductivity, pH, and liquid transfers, into their previously discussed Chemputer robotic platform. Additionally, they integrated analytical instruments like HPLC, NMR, and Raman spectroscopy, enabling closedloop reaction optimization via feedback control. Adaptive execution of chemical procedures on the Chemputer was made possible by the dynamic χDL programming language. The authors demonstrated the platform's capabilities through temperature-controlled reagent additions, optical endpoint detection, and hardware failure detection. The authors optimized several organic reactions, including the Ugi fourcomponent reaction, Van Leusen oxazole synthesis, manganese-catalyzed epoxidation, and trifluoromethylation reactions, utilizing various optimization algorithms like BO with GP surrogate, Phoenics BO, SNOBFIT, and genetic algorithms. The optimization led to improved product yields of up to 50%. Furthermore, the authors showcased an experimental pipeline for exploring unknown reaction spaces, combining digital discovery and optimization, exemplified by the discovery and optimization of two previously unreported reactions."
        }
    ],
    "4.2.2. Discrete and Categorical Optimization and Batch-Type Reactors": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The previously discussed studies have primarily focused on the optimization of continuous variables such as reagent stoichiometry, temperature or reaction time. Chemical reactions, however, are highly governed not only by the continuous parameters defining the process details, but also, most importantly, by the involved reactants and reagents, which are inherently categorical parameters. As discussed above, such optimization over categorical variables requires specific adaptations both in terms of software and hardware, and are often better suited for parallel batch reactor setups. To the best of our knowledge, the first example of autonomous, closed-loop reaction optimization in batch reactors (since the early examples from the 1980s) was reported by Burger et al. in 2020,11 tackling the homogeneous photocatalytic water splitting reaction. Notably, their work stands out owing to the highly advanced robotic setup used for performing and analyzing reactions (Figure 17). In this work, the authors introduce their “mobile robotic chemist” (for a more detailed discussion, see section on Hardware), a KUKA mobile robot that is designed to operate human-centric workstations. The robot transfers reactors between workstations for solid dispensing, liquid dispensing, inertization, capping and GC analysis. They utilized their robot to investigate the water splitting reaction catalyzed by the photoactive polymer P10. However, to circumvent the need for categorical optimization, the authors treated the quantities of each additive as a continuous variable, enabling the use of an off-the-shelf GP surrogate with an upper confidence bound acquisition function for BO. With their highly advanced experimental setup, the authors demonstrate 43 fully autonomous batches of experiments in approximately 8 days, resulting in an almost 10-fold increase in hydrogen evolution. Similarly, Ha et al. recently reported SynBot,432 a platform for autonomous organic synthesis in batch reactors, which was demonstrated for carbon-coupling reactions. SynBot consists of an AI layer, an AI−Robot layer and a Robot layer. As the AI layer, the authors trained a retrosynthesis model as well as a GNN that proposes suitable reaction conditions in combination with BO on a search space that consists of commonly used catalysts, bases and solvents for multiple reactions: Suzuki coupling, Buchwald amination, and Ullmann reaction. The SDL features an integrated robotic system capable of executing various tasks, including chemical dispensing, reaction handling, sampling, and analysis. The system aims to iteratively refine and optimize synthetic routes and reaction conditions in order to maximize the reaction yields. As an alternative to classical batch reactors, Jensen and coworkers reported a series of SDLs using a segmented-flow system in which each droplet�i.e., each “batch”�contains a specific reaction with a unique set of conditions (Figure 18).405 In their first work from 2015, Reizman et al. use this platform for screening potential solvents and optimizing continuous reaction conditions for the mono-alkylation of trans-1,2- diaminocyclohexane. To address this mixed categorical− continuous optimization, the authors performed an initial fractional factorial DoE for every solvent, followed by another fractional factorial design at experimental conditions close to the predicted optimum, and a feedback DoE search to minimize the uncertainty on the maximum predicted yield for each solvent separately. Subsequently, insufficiently performing solvents were disregarded and an automated gradient-based search around the predicted optimum was carried out for the remaining solvents to optimize the yield. Similar principles for the incorporation of categorical parameters in self-optimizations were subsequently used to select optimal precatalyst scaffolds and ligands for a SuzukiMiyaura coupling,408,412 organic base and Ni precatalyst for a photoredox Ir−Ni dual-catalyzed decarboxylative arylation,411 organic base for several Pd-catalyzed C-N coupling reactions,420 catalyst for a Suzuki-Miyaura coupling, base for a metallaphotoredox-catalyzed sp3 −sp3 cross-coupling of carboxylic acids with alkyl halides or photocatalyst for a metallaphotoredox-catalyzed decarboxylative cross-coupling reaction.428 Slattery et al. made use of readily available internet-of-things phase sensors to detect the relevant reaction slugs in their flow system, dubbed RoboChem.439 The authors integrated off-theshelf hardware and custom software to build a modular platform, which contained a GUI to enable operation by nonexpert chemists. The platform contained a light source with tunable intensity, enabling the autonomous optimization and scale-up of a multitude of photo-catalyzed reactions. The RoboChem platform employs multi-objective BO, as implemented in Dragonfly, to autonomously plan and execute experiments. This autonomous experimentation capability allows the system to explore complex chemical spaces efficiently, identifying optimal reaction conditions tailored to each substrate. The authors demonstrated the platform's versatility by optimizing a diverse set of 19 photocatalytic transformations, including hydrogen atom transfer photocatalysis, photoredox catalysis, and metallaphotocatalysis, which are relevant to pharmaceutical and agrochemical synthesis. An alternate approach integrates the selection of categorical reaction variables directly through a suitable encoding of chemicals into appropriate optimization variables, rather than creating a separate response surface for each categorical reaction variable and comparing the response surfaces. This was done by framing the choice of each categorical variable through one-hot encoding all categorical possibilities or calculating descriptors for each categorical variable. The former method was used to find an optimal base for a regioselective SNAr reaction, a suitable phosphine ligand for a Sonogashira coupling,434,444 optimal solvents and phosphine ligands for multiple C-H activation reactions,436 optimal electrophiles and solvents for a Schotten-Baumann reaction.445 The selection of expert-crafted, physically meaningful descriptors is an important strategy to introduce additional knowledge into the optimization campaign.366 In 2021, Christensen et al. reported on this concept for the optimization of a stereoselective Suzuki−Miyaura coupling.425 Notably, the authors used a batch system for reaction execution, namely a Chemspeed SWING platform coupled to a HPLC-UV system, to run parallel reactions in 96-well plates. The use of this batch reactor system enabled the authors to optimize a wide, representative set of 23 phosphine ligands selected in a fully data-driven fashion. Furthermore, the authors considered several other continuous parameters such as reaction temperature, palladium loading, boronic acid equivalents and phosphine to palladium ratio to optimize the yield of the Ediastereomer, while minimizing the Z-diastereomer yield and the quantities of used reagents. Mixed continuous-categorical optimization was performed using the Gryffin package relying on a BNN surrogate.446 While finding a similar optimum, the descriptor-based optimization campaign converged slower than a reference campaign based on one-hot encoding only, which was attributed to the introduction of unproductive bias through the selected descriptors. A remarkable example of categorical optimization was reported by Angello et al. in 2022, who�rather than optimizing reaction conditions for a single substrate combination�targeted the discovery of general reaction conditions.431 The authors defined the most general conditions of a reaction type as those conditions that provide the highest average yield across the widest range of substrate space. The authors showcase this concept at the example of heteroaryl Suzuki− Miyaura couplings using protected boronic acids. To identify the “widest range of substrate space”, data-driven clustering techniques were employed to identify a set of 11 representative reactions for which general conditions should be identified. Optimization was performed over the identity of solvent, base, catalyst and ligand, and the reaction temperature as independent variables. Experiments are performed on a custom-built automated reaction platform that is capable of performing 36 parallel batch reactions with 20 distinct reagents under inert gas conditions.447 The authors developed a custom BO workflow for maximizing generality, the yield over multiple reactions. Notably, the fully explorative acquisition strategy is designed in a way that it does not require evaluating each of the 11 representative reactions in every iteration. Using this approach, the authors managed to efficiently cover a wide space of conditions and substrates, ultimately identifying conditions that double the average yield compared to benchmark general conditions. Related work in optimization of generality of reaction conditions have also been done, although no automation is involved.448 While not optimizing for general reaction conditions, Schilter et al. recently performed a simultaneous optimization over multiple substrates.440 Using a robotic batch system containing six reactors, the authors performed a single optimization campaign in which the yield and conversion for an alkyne iodination was jointly optimized for multiple substrates. Notably, in the optimization campaign, the substrate was a parameter that could be chosen by the optimization algorithm to optimize the reaction conditions. In order to find the optimal conditions for all of the substrates, a substrate could not be selected by the algorithm after a satisfactory performance (conversion > 80%) was obtained. Remarkably, the optimization campaigns showed high transferability as the optimization run was primarily conducted on one of the substrates, and after a satisfactory performance was obtained for this substrate, the same was achieved for the other substrates of interest, requiring a total of only 23 experiments to find suitable conditions for all substrates."
        }
    ],
    "4.2.3 Pareto Optimizations and Further Algorithmic Advances": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "So far, all discussed SDL reaction optimization campaigns were conducted as a single-objective optimization, where the objective is reaction yield in most cases. Optimizing for multiple objectives allows researchers to consider multiple metrics of a reaction, such as yield, conversion, productivity or ecological factors. The most straightforward approach is to scalarize multiple objectives into a single objective value, which, however, requires pre-defining an often unknown tradeoff between different objective values. In an earlier example of this section, we have shown that this can lead to undesirable outcomes of the optimization campaign, particularly if the two objective functions show opposing trends. In such a scenario, impurity yield for a benzylation reaction of a primary amine.417 The elucidation of the entire Pareto front allowed the researchers to identify suitable trade-offs, which was particularly useful in the SNAr reaction, where the space-time yield could be significantly improved while almost not impacting the E-factor. Such a relation would not have been uncovered if only one optimal point was identified. Building on this work, Jeraal et al. evaluated the Pareto front between yield and cost of the mono-aldol-condensation of acetone and benzaldehyde using the TS-EMO algorithm.423 In order to benchmark the algorithm’s performance, the authors ran the campaign twice, once with 20 experiments and once with two low-yield (3% and 5%) experiments as a starting set. Both campaigns converged to the same Pareto front, even though the latter run needed roughly twice as many experiments. In addition, the authors demonstrated the general applicability of their approach by further uncovering the Pareto front between the space-time yield and the E-factor. Similarly, Karan et al. also employed the TS-EMO algorithm for the Pareto optimization of the yield and impurity for an ultra-fast lithium-halogen exchange reaction.438 The authors performed three optimization campaigns with either different initial experiments or different reactant mixing equipment, showing that the algorithm efficiently converges to similar Pareto fronts. Since the TS-EMO algorithm is computationally expensive for categorical parameters, a GP-based BO with the qNEHVI acquisition function291 to find the Pareto front for an optimization with continuous and categorical parameters.445 After demonstrating improved efficiency over TS-EMO in silico, experimental optimization revealed the Pareto front between the space-time yield and E-factor for a SchottenBaumann reaction. Similarly, using their GP-based mixedvariable multi-objective optimization (MVMOO) algorithm,444 Kershaw et al. identified the Pareto front for the yield of orthoand para-products of a SNAr reaction, where the optimization variables included continuous and categorical (solvent) variables.434 Interestingly, their method showed that different solvents are responsible for different regions of the Pareto front, enabling researchers to select the right solvents for the desired product. In a further experiment, the authors uncovered the entire Pareto front between the reaction mass efficiency and space-time yield for a Sonogashira crosscoupling, finding that the Pareto front is almost exclusively dominated by one phosphine ligand. In 2017, Zhou et al. demonstrated the applicability of RL for optimizing chemical reactivity.336 RL was used to learn a policy that determines the next experiment to conduct, where RNNs were used to fit the policy function. Owing to the high cost of experimental data, the algorithm was pre-trained on cheap simulated reaction data, obtained from non-convex mixture Gaussian density functions with multiple local minima. The performance of the RL algorithm was benchmarked against the Nelder-Mead simplex method, the SNOBFIT algorithm and the CMA-ES449 on the simulated data, and was found to outperform all of the established methods on average. However, no benchmarking against standard BO algorithms was performed. The pre-trained policy was then integrated into an SDL using micro-flow reactors with MS quantification, and was used to optimize the conditions of four different reactions: the Pomeranz-Fritsch synthesis of isoquinoline, the Friedlander ̈ Synthesis of a Substituted Quinoline, the synthesis of Ribose phosphate and the reaction between 2,6-Dichlorophenolindophenol and ascorbic acid. Again, the RL algorithm was compared with CMA-ES and a OFAT optimization, showcasing that RL consistently outperforms the other two methods. The optimization campaigns were carried out successively, with the policy improving after each completed optimization campaign, demonstrating the function and generalizability of the pre-trained DL agent model (Figure 20). Recently, Bennett et al. developed Fast-Cat,441 a gas-liquid segmented flow platform suitable for high temperatures and pressures. The platform enables the rapid identification of Pareto fronts for transition-metal catalyzed reactions through BO with the qNEHVI acquisition function. The authors utilized Fast-Cat to identify the Pareto front between the yield and the linear/branched selectivity of the hydroformylation of 1-octene for six different phosphine ligands. Each ligand encompasses different trade-offs between yield and selectivity, demonstrating the importance of efficient automation to uncover optimal conditions. The modular system integrates advanced process automation, in-line reaction characterization using GC, and closed-loop feedback algorithms to dynamically update its belief model and autonomously select new experimental conditions. By leveraging AI approaches, FastCat accelerates reaction space exploration, rapidly identifies optimized conditions, and generates high-quality in-house experimental data to construct digital twins of the catalytic reactions under study. Bai et al. demonstrated a closed-loop distributed SDL within The World Avatar project, aimed at creating a comprehensive digital twin based on a dynamic knowledge graph.131 This architecture utilizes ontologies to capture data and material flows in the design-make-test-analyze cycle, and employs autonomous agents to execute the experimental workflows. The authors demonstrated the framework's application by linking two robotic systems in Cambridge and Singapore for a collaborative optimization of a pharmaceutically relevant aldol condensation reaction, mapping out the Pareto front for costyield optimization within three days. The optimization was done with the TS-EMO algorithm. This setup involved flow chemistry platforms with automated liquid handling and reagent sourcing, showcasing the integration of dynamic ontological knowledge graphs to streamline and coordinate separate SDLs."
        }
    ],
    "4.3 Multi-Step Organic Reactions": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The synthesis of most organic molecules can hardly be achieved in a single step, and can easily require tens of steps for complex natural products. From an SDL standpoint, multi-step reactions can be approached in two distinct ways: (1) each reaction step considered and optimized separately, and the reaction product is purified and isolated before being subjected to the subsequent step; while purification, particularly in batch systems, poses significant hardware challenges, condition optimization can be performed following the approaches discussed in the previous section; (2) alternatively, all steps are run sequentially in the same batch reactor or sequential flow reactors, which is referred to as “one-pot synthesis” or “telescoped synthesis,” respectively. In the latter approach, optimization does not only become a higher-dimensional problem, but the presence of impurities and by-products can complicate the optimization of down-stream steps. For example, Coley et al. demonstrated a system with a robotic arm that can assemble the required unit operations (reactors, separators) into a continuous flow path according to the recipe, connect reagent lines, and carry out the telescoped reactions.450 Furthermore, in telescoped systems, flow rates and reaction times cannot be modified independently, posing an additional optimization constraint. This section will first summarize examples that fall under approach (1) and optimize each step individually, before discussing SDLs that feature selfoptimizing telescoped reactors (approach (2))."
        }
    ],
    "4.3.1. Sequential Single-Step Optimizations": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "To our knowledge, the first published example of autonomous optimization of a multi-step reaction was presented by Cortes-Borda ́ et al. in 2018, where the authors described the synthesis of the natural product Carpanone.416 For the fourstep synthesis, the authors performed four different selfoptimization campaigns, involving allylation, [3,3]-Claisen rearrangement, base-catalyzed isomerization and oxidative dimerization (Figure 21). For each campaign, up to three continuous variables, corresponding to temperature, residence time and stoichiometry/loading of one reactant species were optimized using a modified simplex algorithm. Depending on the reaction, either the HPLC or an in-line benchtop NMR spectrometer was used. Overall, the authors managed to optimize the synthesis to yield 67% of the natural product Carpanone with a total of only 66 experiments. The fact that it was manageable to conduct multiple different reactions resulting in a highly complex product on the same selfoptimizing platform demonstrates the adaptability and efficiency of such systems. A similar example of multi-step synthesis was reported by the same group in 2019, targeting the two-step synthesis of pyridine-oxazoline (PyOX) ligands (Figure 22, top panel).418 Performing two sequential optimization campaigns (three and four continuous variables, respectively) using a custom modification of the Nelder-Mead Simplex algorithm, Wimmer et al. managed to obtain a yield of 75% with only 34 experiments. Notably, the use of the flow system allowed for a significant divergence from the conditions originally reported in batch reactors: Whereas the first step of the original batch route took place at room temperature overnight, due to the thermal instability of the reaction product, the high heat transfer efficiency of flow systems allowed for shorter reaction times under thermal activation, as revealed by the sequential optimization. Transferability of the conditions was further demonstrated through the synthesis of six similar ligands, with yields ranging from 66%−92%. Related examples of multi-step synthesis SDLs were reported by Jensen and co-workers (Figure 22, middle panel), as well as Ley and co-workers (Figure 22, bottom panel). From a practical standpoint, maximizing the yield alone is not a sufficient criterion for successful synthesis�the product needs to be isolated from the crude reaction mixture in high purity, which is usually achieved through phase transfers and phase separations (e.g., extraction, filtration, chromatography). Ley and co-workers, using the LeyLab, reported the autonomous optimization of two two-step syntheses of lidocaine and bupropion, respectively, where each step was optimized separately.116 In the case of bupropion synthesis, after successful optimization of the reaction conditions for both steps, the authors demonstrated the telescoping of both steps into a single, continuous synthesis process (Figure 23). For this, the authors joined the crude product stream of the first step (bromination) with an aqueous sodium bisulfite stream to quench excess bromine. After mixing and subsequent phase separation, the organic phase was joined with the solvent stream for the second reaction (amination) before being transferred to a thin-film evaporation column, in which the dichloromethane from the first reaction step was selectively evaporated. The outflow of this evaporation column, ideally containing the purified product, was then transferred to the reactor in which the amination occurs. This discussion illustrates the hardware considerations required for successfully telescoping individually optimized reactions into a single production workflow�and showcases the existing constraints to a simultaneous optimization of telescoped reaction sequences"
        }
    ],
    "4.3.2. Simultaneous Multi-Step Optimization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Owing to the hardware challenges regarding purification, the first examples of telescoped reactor SDLs did not involve any purification steps, but performed the second step directly using the crude reaction mixture from the previous step. Whilst this enables the use of simpler hardware setups, it not only requires that both reaction steps are compatible with the same solvent, but also necessitates some chemical “cross-compatibility.” In other words, the first reaction step either needs to proceed in a clean fashion without producing major by-products, or the second reaction must be robust and selective enough that side products do not interfere with the desired reaction step. While telescoped syntheses had been reported in the flow chemistry literature for some time, the first examples of autonomous optimization have been described by Bedard ́ et al. in their report on the modular flow platform, as described above. In this work, the authors show the sequential combination of multiple reactor bays to a telescoped reactor system, with the addition of further reagent streams between two reactors. Using this setup, the automation of two two-step sequences is shown: a photoredox-catalyzed oxidative αfunctionalization of amines, and a Lewis-acid-catalyzed [2+2]-cycloaddition of phenylacetic acid chlorides with alkenes. In both cases, the first reaction step consists of the generation of a reactive intermediate (an iminium ion or a ketene, respectively), which is subsequently reacted with an appropriate reaction partner. A further example of a telescoped reaction was shown by Ahn et al., 426 where they conducted an ultrafast lithiumhalogen exchange reaction directly followed by an additioncyclization reaction of phenyl isocyanate. The authors designed an automated microreactor platform, which integrates a microreactor system with syringe pumps, solenoid valves, a thermostat and an in-line FT-IR spectrometer for real-time reaction monitoring. The authors use this platform to optimize the synthesis of a biologically active thioquinazolinone compound. The authors performed optimization campaigns over both only continuous (temperature, flow rate, reactor volume) as well as continuous and categorical (lithiating reagent) parameters. The BO algorithm employed by the authors achieved the same yields within 10 experiments that the authors previously found within 80 experiments of manual planning. Lastly, the authors also optimized the conditions to synthesize a library of S-benzylic thioquinazolinone derivatives. A telescoped Heck coupling of a vinyl ether, followed by selective O-deprotection, was reported by Clayton et al. in 2022 (Figure 24).433 The authors utilized a flow system combined with HPLC for quantifying the reaction yield, and a GP-based BO algorithm for iterative experiment planning. Notably, in order to obtain insights into their reaction, HPLC multi-point sampling, inspired by daisy-chaining from electrical engineering, allowed the sampling and investigating reactor outputs from both reactors separately. With this, the authors were able to uncover an alternative (but preferred) reaction mechanism for the deprotection step, which deviated from the initial working hypothesis, and turned out to be crucial for the identified deprotection conditions. This was only possible since the reaction was optimized as a telescoped process; if all of the three originally assumed steps had been optimized individually, a suboptimal process would have been found. A highly complex example of multi-step synthesis was reported by Nambiar et al. in 2022 for the synthesis of Sonidegib.430 The authors started out with the planning of the synthesis by a Computer-assisted Synthesis Planning (CASP) algorithm, which proposed a two-step route, consisting of an SNAr reaction and an amide coupling reaction. Due to unfavorable electronics in the SNAr step, the authors opted to synthesize the product via a three-step route, consisting of an SNAr reaction, a nitro reduction and an amide coupling (Figure 25). The reactions were carried out in a robotically reconfigurable continuous-flow synthesis platform that allowed for the exchange of different modules by a robotic arm. As analytical modules, FT-IR and LC-MS were integrated to allow for monitoring reactor outputs. In their optimization, the authors considered a series of continuous parameters, including reaction times and stoichiometries, as well as multiple categorical parameters, such as the leaving group for the SNAr reaction, the identity of the amide coupling reagent, or the reactor size for the last reaction step. Their modular platform allowed the robot to exchange the reactor, which in turn enabled the researcher to alleviate constraints on the interdependencies on residence times due to the flow rate of earlier steps. The computer-proposed and human-refined synthesis pathway was subsequently attempted to be optimized in one telescoped reaction. In preliminary experiments, the LC-MS module after the first reaction showed that the SNAr reaction proceeded with > 80% yield, however the FT-IR module after the nitro reduction revealed catalyst deactivation. Further experiments showed that this deactivation was caused by a byproduct of the SNAr reaction, rendering a fully telescoped process without thorough intermediate purification impossible. Thus, the authors decided to run the first reaction separately, and subsequently perform a telescoped reaction for the last two stages. As a consequence, the SNAr reaction was run as a multiobjective optimization campaign, optimizing the yield, productivity and cost with respect to temperature, residence time, stoichiometry of reagent and base, as well as the leaving group as a categorical parameter. Optimal conditions were found in thirty experiments over 10 hours, with the algorithm providing multiple Pareto optimal points. The offline-purified product was subsequently used as a starting material for the telescoped reaction towards Sonidegib, optimizing yield and productivity simultaneously. Optimal conditions were found after fifteen experiments and 13 hours with a total yield of > 90%. The above-mentioned SDL example reflects the challenges in fully autonomous, self-driving systems for organic synthesis particularly well. On the hardware side, telescoping multiple reaction steps offers a highly attractive solution to operating complex multi-step synthesis in a continuous fashion. However, generalizability of this strategy requires the development of advanced purification modules to minimize undesired cross-influences between individual reaction steps, e.g., to remove side products, or to enable solvent exchange. From an analytical standpoint, the introduction of automated reaction monitoring systems at multiple stages of the process provides access to important data that, in turn, can enable invaluable insights into the reaction progress and potential failure modes.451 At the same time, the fully automated interpretation of this data, as well as downstream open-ended decisionmaking, usually require large degrees of expert knowledge, laboratory experience and adaptive decision-making (often referred to as “chemical intuition”). This applies to the integration of automated algorithms for synthesis planning in particular, where, at the current stage, human decision-making is required for ranking routes or identifying reasonable condition search spaces. Integration of these advanced, and often open-ended, decision-making capabilities into AI systems represents an active challenge for the field and leaves room for future developments towards true, reliable SDLs for smallmolecule synthesis."
        }
    ],
    "4.4. Further Solution-Phase Reactions": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The concepts discussed above can readily be translated to synthetic chemistry domains beyond traditional small-molecule synthesis. Importantly, many polymers�with numerous applications in plastics, fibers, electronics, or drug delivery (materials-focused SDLs are discussed in later sections)�are synthesized in solution-phase processes, which makes these amenable to self-optimization. The major distinction to the previous discussions of small-molecule synthesis is the analytical methodology. Whereas for small molecules, a single, well-defined molecular entity needs to be determined in a quantitative fashion, the quantification of a “polymer yield” is less straightforward; in addition to the amount of formed polymer, the targeted size distribution, degree of (co- )polymerization, or other physical properties need to be controlled, which leads to a greater variability in the analytical methods and the resulting optimization objective. One of the earliest examples of polymer SDL was performed in 2002. Vieira et al. demonstrated the closed-loop optimization of molecular weight and composition for copolymer latex.452 Using a series of pumps and agitators, the authors automated emulsion polymerization, in which the monomers are dispersed as tiny droplets in aqueous phase, with emulsifiers and stabilizers to initiate and terminate polymerization, respectively. The copolymers were characterized by a near infrared spectroscopy (NIRS) probe of the solution, detecting the monomer concentration, polymer holdup, and the mean polymer size through the use of the partial least squares (PLS) model.453 The goal was then to minimize the fitness, a weighted sum of differences between the desired and current molecular weights, over the feed rates of precursors, which was done using the iterative dynamic programming (IDP) method.454,455 IDP considers discrete time intervals of previous iterations, and adjusts the flow rates to drive the system toward the desired synthesized polymers. Houben et al. performed similar experiments with the use of multi-objective ML techniques to optimize the recipes of emulsion copolymerization reactions.456 The authors used a setup similar to the one described before, however the analysis of particle sizes and conversion rates were done off-line, using dynamic light scattering and chromatography, respectively. Rather than only considering the flow rates, the 12 other experimental parameters were also varied. After each iteration of experimentation, the results were fed into the multiobjective active learner (MOAL) algorithm, with suggestions produced by a GA, and predictions generated from a GP model.457 Starting with 5 random initial experiments, and 15 additional experiments guided by MOAL, the authors found the conditions needed to produce high conversion polymers with particle sizes of 10 nm. Rubens et al. used continuous flow microreactors, rather than batch reactors, to develop an SDL capable of highthroughput synthesis of reversible addition fragmentation chain transfer (RAFT)458 polymers with precise molecular weights.459 The polymer from the flow reactors were then characterized in situ by size exclusion chromatography (SEC), measuring the molecular weight, and dispersity of the polymers. The results were then fitted using a linear regression model with the results at each iteration. The flow rates with the best predicted results were then used for the next iterations. Most recently, Knox et al. studied the same RAFT polymers with an SDL guided by BO, with the temperature and residence time as continuous optimization parameters.460 Furthermore, the automated characterization techniques included both an in-line chromatography and an online NMR spectrometer. Using TS-EMO with a GP regressor surrogate, the authors were able to map out the Pareto-front for the polymer conversion and molar mass dispersion with higher resolution when compared to DoE. The BO iterations would suggest the next experimental parameters: the temperature and the residence time of the reactor."
        }
    ],
    "4.5. Cataöyst amd Reactopm Discovery": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": ""
        }
    ],
    "4.5.1. New Catalyst Materials": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Beyond the optimization of reaction conditions for a specific synthesis process, the discovery of novel highly active catalysts can allow for novel and more efficient synthetic processes, and can open up new production avenues. While a catalyst is formally defined as a species that accelerates a given reaction, in reality, catalysis enables reactions that would otherwise only occur under impossible conditions. As such, catalysis has an enormous economic value, and it is assumed that >80% of all synthetic consumer products have gone through at least one catalytic process in their production. At the same time, discovering new catalysts is a considerable challenge, since their design requires the knowledge of a series of reaction pathways and modes of action, which also makes it extremely difficult to simulate catalytic efficiency from first principles. As a result, the last century has mainly seen empirically or heuristically driven campaigns for catalyst discovery. One of the most prominent examples is Mittasch’s large-scale screening for heterogeneous catalysts for the Haber-Bosch process,21 where they empirically test over 4000 possible catalysts�yielding an optimal catalyst that is still used as of today in almost unaltered form. More recently, Lai et al. demonstrated a LLM capable of suggesting catalyst synthesis conditions, drawing from the decades of results in the scientific literature.461 Major challenges in automating such a discovery process, and implementing it into an SDL, stem from the requirement to first synthesize and purify the catalyst candidate, which can involve a series of intricate experimental steps, and subsequently evaluate its activity in the catalytic reaction of interest. This challenge is illustrated in a pioneering example from Corma et al., who tackled the challenge of identifying heterogeneous titanium silicate catalysts for olefin epoxidation.462 Here, the catalyst synthesis alone involves gel formation from all involved reagents, followed by hydrothermal crystallization and post-synthesis treatment. The authors use a sophisticated robotic setup to automate these steps. The efficacy of the newly synthesized catalyst in the epoxidation of cyclohexene with a peroxide oxidant is then evaluated in a parallel batch reactor, which is coupled to ultrafast GC for on-line analysis. Even though the authors demonstrate an advanced level of automation (especially given that the work was published in 2005), transfer of samples between the workstations required a human experimentalist. Experiment planning is performed through a GA,463,464 enhanced by a neural network for applying a selection pressure on the newly proposed candidate generation, to optimize the quantities of four catalyst ingredients. The authors demonstrate three generations of 21 experiments each, and show the discovery of two new families of catalysts with improved activity, which are structurally characterized in detail. In 2010, Kreutz et al. reported an SDL for homogeneous catalyst discovery for the partial oxidation of methane with molecular oxygen. 465 The catalytic system, which can be prepared by mixing all ingredients in an aqueous solution, is composed of three components: the active metal, a co-catalyst, and a ligand. These three categorical variables are optimized through a GA. To perform the required experiments, the authors have developed a sophisticated experimental setup based on droplet-flow reactors (Figure 26). Solutions containing the different catalyst compositions are prepared in 96-well plates, and are then injected into a microfluidic reactor. Both methane and oxygen are added by diffusion through the teflon walls of the flow reactor. The formed methanol was quantified by diffusion into neighboring microdroplets that contained a methanol-selective indicator, thereby allowing for semiquantitative analysis using UV-Vis spectroscopy. Per generation, the authors performed 48 experiments (in quadruplicate), and demonstrated that over 8 generations, a significant improvement in methanol formation (up to 3-fold increased catalytic activity) can be obtained. Zhu et al. reported an autonomous system for discovering catalysts for the electrochemical oxygen evolution reaction (OER) from Martian meteorites, simulating the development of an oxygen-generating system on Mars.466 For this purpose, the authors demonstrate a complex synergistic workflow consisting of multiple experimental and computational components. By analyzing the available Martian ores through automated atomic emission spectroscopy, the available elements�and therefore, the accessible materials search space�are defined autonomously by the platform. Within this search space (>3 million combinations of 6 metals in discrete quantity steps), a diverse set of ∼30,000 possible catalyst compositions are first screened computationally, by molecular dynamics and DFT calculations. This data is used to train a surrogate neural network model for the computed catalytic properties as a function of the elemental composition. These computed properties, together with the elemental composition, are then used as inputs to a second neural network for predicting the experimental catalytic activity. The latter network was trained on a small seed dataset of < 300 experiments, which were conducted in a fully automated fashion using a robotic arm operating multiple workstations for dissolving the raw ores, creating reagent stock solutions, precipitating, drying and formulating the catalysts, and determining their catalytic activity in an electrochemical measurement. The authors then performed virtual BO within the entire search space, using the predictions of the trained neural network as their objective, and validated that the identified best candidate indeed outperforms all previously obtained catalysts. Even though the authors do not demonstrate multiple iterations of closed-loop of experiments and data-driven decision-making, the computational definition of the search space, as well as the advanced automation workflows are remarkable�making this work a Level 3 SDL, as by the definition of Figure 1. In 2024, Ramirez et al. demonstrated the optimization of a heterogeneous catalyst for the reduction of CO2 using BO.467 As a catalyst, the authors explored systems containing up to three metals among iron, cobalt, copper, zinc, iridium and cerium with a maximum loading of 5 wt%. Additionally, the algorithm could choose between the presence or absence of potassium as a promoter, the amount of water as solvent as well as having silica, alumina, titania or zirconia as support. The authors synthesized 144 catalysts over six generations. Over the performed experiments, the BO algorithm was able to identify a catalytic system that maximizes the CO2 conversion and MeOH selectivity while minimizing the CH4 selectivity and the cost, where the latter was only considered throughout five generations to demonstrate the adaptability of the algorithm. Even though the algorithm is capable of finding a performant catalyst, the authors point out that this is performed within a well-studied and expert-restricted chemical space, demonstrating the hurdles for autonomous novel catalyst discovery. The discussed works showcase examples of how catalyst discovery could be addressed in a closed-loop fashion� provided that the search space is sufficiently narrow, and the experiments can be automated in a useful manner. Particularly catalyst synthesis poses a major challenge in this regard; the diversity of catalyst space, and the fine nuances that can influence catalytic activity, however, render the development of generalizable automation schemes difficult. In homogeneous catalysis, making new catalysts requires synthesizing new molecular species, which usually require multi-step reaction and purification sequences, which, in turn, we had previously identified as a major challenge for automation. On a purely computational level, this bottleneck can be circumvented, which has led to impressive and experimentally validated examples of closed-loop catalyst design, for example in organocatalysis.468 In heterogeneous catalysis, on the other hand, synthesis requires intricate thermal treatment and annealing steps, which possess inherent automation constraints, and can often lead to structurally ill-defined materials, adding further complexity to the data-driven prediction problem. As a consequence, the last decade has produced rare examples of true SDLs for catalyst discovery, which remains a grand challenge for autonomous discovery, both from the software and the hardware standpoint."
        }
    ],
    "4.5.2. New Reactions and Reaction Types": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "While all previous discussions have focused on specific reactions�the product (and reactants) are given, and the goal is to find the catalysts, reactants, reagents or reaction conditions that maximize the product quantity�SDLs can also be used to search for new reactions or products.469 In fact, this problem of discovering new reactivity or catalytic activity has been an active field of research in organic chemistry for more than a century. While the predominant search strategy in this field has been rational design, the importance of “serendipitous” discoveries has been emphasized numerous times.470 As an example from an SDL, Amara et al. reported the detection of an unexpected side product when attempting the selfoptimization of a γ-Al2O3-catalyzed methylation in supercritical CO2 (for a more detailed discussion of these reactions and the self-optimization algorithms used, see the section on Selfoptimizing flow reactors).471 Careful characterization of the side product by a human researcher allowed for its unambiguous identification, and a second closed-loop campaign towards the yield of this side product was carried out, which eventually resulted in the discovery of optimized conditions for a new reaction type. This example demonstrates the possibility of discovering new reactions through SDLs. At the same time, especially from the standpoint of automation and experiment planning, it poses the open-ended analytical challenge of detecting and identifying newly formed, unknown reaction products from a crude reaction mixture, which is often addressed by analyzing changes in the bulk properties of the reaction mixture (UV-Vis spectra, IR spectra, NMR spectra), or through coupled separation−detection techniques (GC- or HPLC-MS). Early examples in the field of “untargeted” reaction discovery have focused on non-iterative screening campaigns using combinatorial chemistry and HTE , which have been reviewed elsewhere.472,473 The first example of a truly closed-loop campaign for discovering new reactivity was reported by Cronin and co-workers in 2018, who developed an SDL for finding new two- or three-component reactions in a pool of reactants (Figure 27).474 In a proof-of-concept work, Granda et al. selected a set of 18 reactants with diverse functional groups, which can be reacted under fixed reaction conditions in a fully automated fashion. Crude reaction mixtures were analyzed by automated IR and 1 H-NMR spectroscopy, and the spectra, along with the spectra of the starting materials, were processed by a pre-trained SVM classifier to label the reaction as “reactive” or “non-reactive”. Based on this data, a linear discriminant analysis (LDA) model was trained to predict reactivity across the entire search space, and new experiments were selected in a fully exploitative fashion. With this search strategy, the authors demonstrate a significantly improved hit rate compared to trivial random search algorithms, and report a series of nontrivial reactions which had not been published before. Later work from the same group, Caramelli et al. used a similar platform to discover new unreported reactions in an automated fashion:475 the photochemical reaction of phenyl hydrazine and bromoacetonitrile, and the reaction of ptoluenesulfonylmethyl isocyanide (TosMIC) and diethyl bromomalonate. For decision-making, Reactify is a CNN that is trained on the NMR spectral data of 440 reactions with reactivity classified by a chemist. A neural network, using the junction-tree VAE embedding of the molecules as features, is trained to then suggest new reactants for the SDL platform. Both the Reactify and the surrogate neural networks were retrained at each iteration. The reaction mechanisms of the novel reactions were further studied by the authors. In related work, Mehr et al. demonstrated a probabilistic approach to reaction discovery, both in silico and as part of an SDL.476 Reactants were assigned prior distributions which were then combined to form a joint probability prediction of the reactivity between them. Following Bayes’ theorem, the distributions were updated based on the feedback results of an automated HTE platform. The experiments were carried out in a flow-based system, with on-line NMR, HPLC, and MS characterization. The authors were able to rediscover known reactions such as the Buchwald-Hartwig amination, and the Wittig-Horner reactions. The question of whether an identified reaction can be considered “novel” has been subject of an ongoing debate. While the previously unknown formation of a reaction product�the definition used by Granda et al. and Caramelli et al.�clearly constitutes a new reaction, the term novelty lacks an unambiguous definition. In both studies, the authors were maximizing the reactivity, or rather, maximizing the number of reactions classified as reactive. Any SDL targeting the discovery of novel reactions therefore requires a series of assumptions and simplifications for defining the optimization objective. Porwol et al. later applied a similar discovery strategy for finding new polyoxometalate clusters composed of metal ions and bridging ligands.477 Following in situ assembly of the ligands in a threecomponent coupling, a metal precursor is added to form potential polyoxometalates. By using a series of characterization techniques including UV-Vis spectroscopy, MS, and pH measurements, novelty is measured as the cumulative difference between the data of starting materials and products, respectively. As independent variables, the authors selected the ligand precursor identities, metal ion, reagent volumes, reaction temperature and reaction time. To maximize the novelty, the authors used a custom surrogate-free search algorithm, which samples each experiment in a given distance from the previous experiment, depending on the novelty of the previous experiment. Following this strategy, the authors discovered a range of new polyoxometalate clusters. This is discussed further in the section on state materials synthesis optimization."
        }
    ],
    "4.6. Determination of Reaction Kinetics": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Especially on the process chemistry level, knowledge about the reaction conditions that lead to optimized reaction yields is not sufficient for safe and reliable reactor operation. In these contexts, detailed information about the kinetics of a reaction is required in order to predict and adjust the behavior of a reactor system. At the same time, kinetic knowledge can enable important insights into the mechanism of a reaction�which is of high relevance for informed decision-making, both at the discovery and at the process stage. SDLs can, and have been, used to iteratively acquire kinetic data, refine kinetic hypotheses, and eventually obtain reliable kinetic models. This has, in a simple proof-of-concept study, already been demonstrated in the late 1970s in the context of derivatization reactions for analytical chemistry.478 Decades later, in 2011, McMullen and Jensen utilized a microfluidic system to optimize the parameters of a kinetics model for the Diels-Alder reaction of isoprene with maleic anhydride.479 Following the Box and Hill method, the probability of a particular rate model describing an experiment can be formulated in a Bayesian context by a posterior probability function based on the experimental conditions and the outcome concentration.480 Using an in-line HPLC, the microfluidic system returns the output concentration of isoprene, which is used to update the distribution until a predefined probability threshold is met. After deciding the rate law, the microreactor was then used to optimize the parameters of the rate constant through plug-flow reactor kinetics. Finally, for validation, the authors performed 4 additional experiments and found good agreement with predictions from the optimized rate law. In a related study, Reizman and Jensen presented a continuous-flow SDL for studying multi-step reaction kinetics.481 Using high-throughput synthesis methods enabled by flow reactors, the authors studied the conversion of 2,4- dichloropyrimidine to 4,4′-(2,4-pyrimidinediyl)bis-morpholine. There are two reaction pathways, each with two reactions, which are all modeled as second-order bi-molecular reactions. The product concentrations were measured after the reaction by online HPLC, and the kinetic model parameters were leastsquares fit to the results. The sensitivity coefficients, a measure of how sensitive the predicted concentrations are to the synthesis parameters, were then calculated for the optimal parameters. By minimizing the sensitivity coefficient, the next experimental conditions were generated, and the reaction kinetic models’ parameters were iteratively optimized. Most recently, Sheng et al. applied a closed-loop SDL to study the electrochemical reaction of cobalt tetraphenylporphyrin (CoTPP) with organohalides.482 The electrochemical platform uses a flow system to control the flow of reactants into a 3-electrode cell, which is monitored by a potentiostat for cyclic voltammetry (CV). The platform first identified reactions which can be modeled by the EC mechanism, which consists of an electron transfer step followed by a solution reaction. This was done by analyzing the CV data with a ResNet CNN previously trained to extract relevant electrochemical quantities.483 In the second stage, the EC mechanism is probed by optimizing the rate constant (k0) of the solution reaction step as a function of the voltammetric scan rate, and the organohalide concentration. Both stages were guided by a Bayesian optimizer from Dragonfly."
        }
    ],
    "4.7. Solid State Materials Synthesis": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Solid state materials, such as molecular crystals, zeolites, metalorganic frameworks (MOFs), covalent organic frameworks (COFs), polyoxometalates and alloys, have a variety of applications, particularly in catalysis of reactions. Porous materials like molecular crystals, MOFs, COFs, and zeolites are characterized by voids in the crystalline structure, typically on the nanometer to micrometer scale, and high surface areas, giving the material the ability to adsorb molecules for storage or catalysis. This has applications in gas storage and separation (i.e., methane, hydrogen gas), filtration, and drug delivery.484,485 While there are many SDLs focused on optimizing the function of solid state materials, the SDLs discussed in this section are focused on finding optimal synthesis conditions for the structure and crystallinity of the material. For SDLs related to energy storage and optoelectronic applications, we refer the reader to the respective sections. The primary advantage of solid state materials is their tunability beyond the chemical component; by varying both the composition and synthesis parameters, the material properties and structure can be tuned for specific applications. Considering the space of possible materials and structures is intractably large, traditional approaches based on manual synthesis are insufficient to explore these materials efficiently. A number of high-throughput methods, both computational,373,486−489 and experimental,490−492 have been developed and successfully applied to combinatorially exploring the material space. These have resulted in large datasets of possible structures and materials, as well as their measured or predicted properties, paving the way for data-driven strategies.151,493,494 Conventional data-driven applications of these highthroughput methods are through compound screening: a predefined space of compounds and structures are filtered down based on predictions from statistical models trained on the datasets, or theoretical calculations.495,496 There are extensive works in the literature completely within the computational domain: developing descriptors and models that can predict the properties of interest from the datasets,486,497−499 and extending these models for computational SDLs, performing active learning campaign based on in silico model predictions from models trained on high-throughput experimental or computational results.500,501 In the synthesis of zeolites, Moliner et al. utilized a highthroughput robotic arm platform capable of liquid/solid handling, stirring, and crystallization to generate a combinatorial DoE study of 144 triethylamine:SiO2:Na2O:Al2O3:H2O zeolites.491 Using the MLP model, the authors were able to attain better predictions of the crystallinity of the zeolites from the experimental dataset than the typical multivariable quadratic models. The crystallinity is measured via XRD: the spectral peaks are fitted with Gaussian functions, and the average full-width half-maximum (FWHM) of the peaks are used as a measure of crystallinity. In a related study, Corma et al. performed a similar study for SiO2:GeO2:Al2O3:F−:H2O:4- (2-methane sulfonylphenyl)-1,2,3,6-tetrahydropyridine hydrochloride zeolites, which have demonstrated successful crystallization into ITQ-21 and ITQ-31 zeolites.502 The authors improved the crystallinity predictions by including structural descriptors derived from the XRD spectra, along with the synthesis descriptors, in the MLP neural network input. Nikolaev et al. demonstrated an Autonomous Research System (ARES)503 capable of autonomously conducting iterative materials experiments to study carbon nanotube (CNT) synthesis�a pioneering example of an autonomous SDL for materials research. Experiments were conducted by heating catalyst-coated silicon pillars, which each serve as CNT microreactors, with a laser while varying growth parameters like temperature, pressure, and gas composition. Raman spectroscopy measured the CNT growth rate in real-time. Using linear regression models, the authors were able to map out the effect of experimental conditions on the resulting growth of single-wall or multi-wall CNTs.504 In a later study, the same system was providing feedback to a RF model and genetic algorithm to propose new experimental conditions. Over hundreds of closed-loop iterations with minimal human intervention, ARES successfully learned to grow CNTs at targeted growth rates by optimizing the multi-dimensional parameter space. More recent work from the group modified ARES to use BO with GP surrogates for the maximization of CNT growth rate.505 These demonstrations showcase ARES's ability to autonomously navigate complex experimental domains and obtain insights into growth kinetics, which is valuable for controlled nanotube synthesis. As one of the first implementations of SDL for materials science, this work highlights the potential of autonomous research systems to accelerate the scientific understanding and development of complex functional materials. Similar ML directed discovery have been demonstrated in the synthesis of MOFs, for example Raccuglia et al. further incorporated reactant and reaction descriptors in the prediction of successful synthesis and crystallization of organic templated vanadium selenite materials.506 Training a SVM on experimental results from both failed and successful reactions, and comparing the recommended reactions from a human chemist, the model was shown to have a higher success rate and provide more diverse reactions. More recently, Xie et al. utilized XGBoost, a gradient boosting tree-based model, to determine the reaction parameters for crystallization of metalorganic nano-crystals.507 To test their model, validation experiments not found in the training set were conducted to demonstrate the use of the XGBoost model for extrapolating to new MOF nano-crystals. Luo et al. later developed the MOF Synthesis Prediction tool, using natural language processing DL models to extract synthesis conditions of MOFs from the literature and create a dataset and prediction tool for synthesizing new MOFs.508 The experiments in these works were not conducted in an automated fashion. The earliest examples of closed-loop SDLs for solid state materials were by Corma et al. in 2005, previously discussed in greater detail in the section on catalyst discovery. The authors were interested in optimizing the catalysis of olefin using a Ti-based zeolite catalyst.462 Various concentrations of hydroxide, titanium and surfactants were combined in the hydrothermal synthesis of the zeolite using a robotics system. The batches of zeolites were then tested for catalytic activity using ultrafast GC. In the development of MOFs, Moosavi et al. developed an SDL that optimizes the crystallinity of the HKUST-1, first synthesized by Chiu et al. 509 at the Hong Kong University of Science and Technology.510 The synthesis was performed using a high-throughput robotic platform, capable of handling and stirring reactants, transferring the samples into a microwave reactor cavity for synthesis and to a powder X-ray diffractometer for crystallinity measurement. The exploration of the parameter space was done using a GA dubbed the SyCoFinder, over the course of three generations, with 30 synthesis conditions tested in each. Similar to previous work,506 results from successful and failed experiments were collected in order to train a RF model to identify the synthesis parameters of importance. By weighting the 9 dimensional parameter space by the identified importance, the parameter space becomes smaller and more confined, allowing for more efficient exploration guided by chemical intuition. Further optimizations were not performed. Xie et al. performed a similar analysis with the zeolite imidazolate framework (ZIF), ZIF-67.511 They developed a new ZIF synthesis protocol based on a custom low-cost gantrystyle robot SDL platform that injects precursors onto laserinduced graphene microreactors fabricated on a thin film (Figure 28).512 The microreactors were then Joule-heated to create ZIF-67 in a high-throughput manner. The synthesized samples were transferred for XRD characterization. Rather than using a GA in the experiment planning, the authors used BO with a RF surrogate model. For the synthesis, the molar ratio of metal ions to organic molecules, the volume of precursors, the applied DC voltage, and the heating duration was varied. After an initial 12 random samples, three additional generations with 12 samples each were suggested by the BO algorithm using the expected improvement acquisition function. Figure 28 shows the improvement in the crystallinity as a function of BO iterations. Extending into thin films of MOFs, Pilz et al. developed an SDL optimizing surface anchored MOFs that are formed layerby-layer.513 Like previous work,510 the authors used the SyCoFinder GA for synthesis planning, with the goal of optimizing multiple objectives: the crystallinity, the [111]- orientation of the crystal, and the phase purity, all of which are measured from the XRD spectra. The objectives are combined with a summation and then normalized to a fitness between 0 and 1. The parameter space included the metal and linker concentrations, the amount of water, and cleaning time via sonication and spray cleaning. The samples were transferred across the various modules via a 6-axis robotic arm. The SDL started with a diverse random set, and two more generations were carried out, with increasing fitness found with subsequent generations. Harris et al. demonstrate an autonomous synthesis platform for pulsed laser deposition (PLD) of thin films by combining real-time diagnostics, automated synthesis and characterization, and ML algorithms. The platform utilizes GP regression and BO to autonomously explore a 4D parameter space of background pressure, substrate temperature, and laser fluences on two targets, tungsten and selenium, aiming to optimize the crystallinity of WSe2 thin films based on in situ Raman spectroscopy feedback�sharper peaks indicate higher crystallinity. Having only sampled 0.25% of the parameter space, the autonomous workflow discovered two distinct growth windows and mapped the process-property relationships governing film quality. Notably, the automation achieved at least a 10-fold increase in throughput compared to traditional manual PLD workflows. The combination in situ Raman spectroscopy monitoring, and ML driven decisionmaking can be used for PLD fabrication of other solid state thin film systems. Duros et al. studied the crystallization of a new polyoxometalate structure with an SDL driven by active learning, and also provided a comparison with random and human-guided experimental planning.514 A series of syringe pumps fed aqueous precursor solutions into a reactor, and the products were visually inspected for crystallization. The platform was capable of performing batches of 10 crystallization experiments per day, and an initial dataset of 89 points was acquired to start as a training set. A SVM classifier was trained on this dataset to classify successful crystallization experiments. The subsequent experiments were then conducted using an active learning loop, with the goal of maximizing the number of polyoxometalate structures and the explored synthesis parameter space. When compared to human and random exploration of the space, the SDL explored more of crystallization space, while still finding a similar number of crystallization points as human decision (Figure 29). Beyond MOFs and COFs, van der Waals superlattices�i.e., stacks of graphene-like atomic monolayers bound through dispersion interactions�have emerged as an attractive class of 2D crystals with multiple applications in e.g., semi- and superconductors, or topological insulation. The layer-by-layer assembly of these materials could allow precise control over materials properties, but requires delicate physical handling. As an important step towards SDLs for van der Waals superlattices, Masubuchi et al. developed a multi-step robotic workflow: in the first step, pre-synthesized 2D crystals deposited on Si chips are automatically detected and characterized using optical microscopy and computer vision.515 Subsequently, the detected crystals are robotically transferred to a stamping apparatus, aligned and assembled to the desired superlattice. While this work does employ iterative data-driven decision-making, the advanced automation and computer vision approaches can justify the classification as a Level 3 SDL, laying the foundation for autonomous materials discovery for van der Waals superlattices. Kusne et al. developed CAMEO for the self-driven discovery of phase-change memory (PCM) materials.516 These are inorganic materials capable of switching between amorphous and crystalline states, altering the optical and electrical properties of the material. CAMEO uses a physics-guided ML model for BO of Ge-Sb-Te ternary PCM. Synthesis was not part of the design process; rather, a combinatorial library of Ge-Sb-Te material was loaded onto the system, along with data from DFT simulations. Because the target property was dependent on the phase of the material, the first iterations maximize the phase map of the material. After some defined threshold for phase map exploration, the BO algorithm, based the predictions GP models with an UCB acquisition function modified with an additional term based on the distance from the phase boundary, selected the next material for automatic synchrotron XRD characterization and human-in-the-loop evaluation of the optical gap. While not fully automated, the authors were able to discover a new photonic PCM with an optical gap difference between crystalline and amorphous phases of 0.76 ± 0.03 eV, over three times larger than the conventional GST225 material. In the quest to understand the phases of specific solid state inorganic materials, Ament et al. demonstrated a self-driven high-throughput platform for determining the phase boundaries of Bi2O3 system.517 Bi was sputtered in an atmosphere of Ar and O2 onto Si wafers to create thin-films of Bi2O3, which were annealed in stripes using a laser. By varying the annealing temperature and time, different phases of Bi2O3 can be observed. The samples were characterized by optical microscopy and reflectance spectroscopy to determine the phase boundaries, and the next conditions are suggested by GP models with custom kernels based on the physics of the experiment; the algorithm was dubbed Scientific Autonomous Reasoning Agent (SARA). The authors were able to map the phase boundaries of the system two orders of magnitude faster than random or exhaustive search methods. A major obstacle to the development of a fully automated SDL for solid state materials is the need for powder handling and XRD characterization. Lunt et al. developed the PowderBot, an autonomous robot capable system capable of automated Powder XRD.10 Powder-Bot successfully synthesized molecular crystals using a Chemspeed liquid-handling platform. A single-arm mobile robotic manipulator transfers the crystalline material to a grinding station where a dual-arm stationary robot produces the powder, and then takes the powder XRD samples to a diffractometer for analysis, totaling thirteen distinct steps. The manipulator operates the diffractometer as a human chemist would, and the XRD spectra is recorded. While the work is not a true closed-loop SDL due to the lack of intelligent experimental design, the authors demonstrated a landmark single iteration of automated synthesis and powder XRD characterization using conventional processing and characterization equipment. In another notable advancement, Chen et al. present ASTRAL, a robotic platform that seamlessly integrates powder-precursor synthesis including powder dispensing, ball milling and oven-firing into XRD characterization of reaction products.518 Most recently, Szymanski et al. presented A-Lab,519 an SDL for solid state synthesis of metal oxides and phosphate powders, with fully automated sample preparation, heating, and XRD characterization capabilities. Solid state synthesis pathways were selected using the ML-based precursor selecting algorithm ARROWS3 , which incorporates decomposition energies from both ab initio calculations and previous experimental outcomes to find the best reaction pathways.520 Air-stable synthesis targets were identified based on ab initio calculations from the Materials Project, and a dataset from Google DeepMind.521 Recipes obtained from text-mining sources in the literature were used to train ML models to generate recipes for compounds not found in the training dataset. We note that this is an unguided systematic search of the proposed synthesis routes; however, if these recipes fail to produce high enough yields (> 50%), A-Lab defaults to the ARROWS3 algorithm, which utilizes information from prior experimental results. The autonomous platform then carries out the recipe, performing dosing, syntheses, and analysis on three different stations, with a robotic arm transporting the sample between stations. The collected XRD spectra were analyzed using a probabilistic ML model trained on the ICSD, as discussed previously in Analytical Process Optimization. The resulting weight fractions of the synthesis products were fed back into the orchestrator of A-Lab to inform further experimentation. Over the course of 17 days of continuous experimentation and 355 experiments, A-Lab successfully synthesized 41 out of 58 target compounds, of which 9 of the targets were optimized by the data-driven ARROWS3 algorithm for improved yields. The authors further claim the discovery of multiple new compounds and structures, although this has been called into question due to the non-standard analysis of XRD results, and the under-characterization of the compounds.522,523 Still, the A-Lab has demonstrated advancements in the development of inorganic solid state SDLs. These examples represent significant steps towards accelerating the discovery of feasible solid-state materials in a design space that contains a large fraction of unstable and metastable materials, and closing the automation design loop for arguably the most difficult-to-automate piece. We expect these endeavors will set precedents for application-driven, inorganic solid-state SDLs to come."
        }
    ],
    "4.8. Outlook and Perspectives": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Within this chapter, we have provided a comprehensive overview of SDLs for chemical reaction optimization, which has arguably been the most widespread application of SDLs as per definition of this review. While first, foundational examples of autonomous reaction optimization have been laid in the 1980s, the field has seen an enormous boost in the 21st century, owing to advances in digitization, computational resources and software distribution. The largest body of work has focused on the autonomous optimization of single-step reactions in solution. Notable examples include: heterogeneous catalysis, photochemical reactions and photocatalysis, nanoparticle catalysis, the use of supercritical fluids as reaction solvents, and many others. These works have also led to notable automation and advances in related disciplines of modern synthesis, including catalytic technologies like electrocatalysis524,525 and organocatalysis,526 or economically important applications like biomass or waste valorization.527 We expect to see pioneering examples of SDLs in these fields in the years to come, leading to a further diversification of SDLs for chemical reaction optimization. Importantly, optimization campaigns have not been limited to maximize the yield of a chemical reaction, but have been extended to economic considerations (e.g., time, cost, and produced waste), kinetic information, or the information content of the obtained reaction data.528 It is important to note that all of these works have relied on two main pillars. First, the availability of open-source solutions for both automated reaction hardware and optimization software has enabled the implementation of autonomous systems across a variety of labs, and has proven to be a (figurative) catalyst for the spread of SDLs. We highly advocate for such open-source initiatives�accessible solutions (such as EDBO+ platform from the Doyle group295) have shown to serve as inspiration for further groups to adopt important SDL technologies.529 Secondly, domain expertise and laboratory experience has been instrumental to set up the required hardware and, more importantly, define and constrain the experimental search problem. The use of AI for those open-ended decision-making tasks represents an important open challenge to the community, in addition to adaptive decision-making in synthetic laboratory scenarios. These software requirements go hand in hand with the development of flexible, reconfigurable hardware systems that enable such adaptive operations. Addressing these challenges, as discussed in detail throughout this chapter of the review, can build the foundation for the next generation of SDLs for chemical synthesis, and eventually bring us one step closer to the dream of autonomously synthesizing any molecule (or material) on-demand. As such, autonomous synthesis can be an integral component of any autonomous materials discovery initiative, including the efforts detailed in the following sections."
        }
    ],
    "5. Drug Discovery and Biochemistry": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Drug discovery plays a pivotal role in modern society and in the chemical industry, not only as a major consumer of chemical compounds, but also as a driving force behind chemical innovations: indeed, the pharmaceutical industry invests billions in research and development (R&D) every year,530 and some of the first examples of automated and highthroughput experiments were first developed by pharmaceutical companies. The reason behind this huge investment is the high cost associated with drug development: it usually takes US$2.6 billion and 10 years to put a single drug on the market.531 This long and costly pipeline can be roughly split into five main stages: early-stage discovery, preclinical studies, clinical trials, FDA review and approval and finally, post-market monitoring. Early-stage discovery includes disease-related proteins target identification, compound screening against selected target, assay development and compound property optimization. Preclinical studies focus on drug profiling, delivery and dose range finding. However, while the R&D budget increases over the years, the composite average approval rate of drugs keeps falling down.530 Analyses of clinical trial data from 2010 to 2017 show four possible reasons attributed to 90% of the clinical failures of drug development: (i) lack of clinical efficacy (40%−50%), (ii) unmanageable toxicity (30%), (iii) poor drug-like properties (10%−15%), and (iv) lack of commercial needs and poor strategic planning (10%).532 Given those statistics, it is apparent that success in early-stage discovery and preclinical studies stages is key to overcoming the high attrition rate. In those stages, researchers are confronted with multi-objective optimization problems that span the chemical and biological space. Not only are those vast, but the understanding of them is also incomplete. For efficient exploration, the pharmaceutical industry has thought to employ automation relatively early compared to other industries:533 Automation in drug discovery dates back to the 1980s with the advent of high-throughput screening platforms, which leverage robotics to manage the handling of thousands of bioassays.534 Spurred by large investments by pharmaceutical companies, robotic drug discovery platforms have evolved towards a higher level of automation and complexity. A notable example is Eli Lilly’s state-of-the-art automated synthesis laboratory, among others.535 Along with hardware automation, the field has benefited significantly from advances in computational molecular design and synthesis planning, which have proven to be powerful tools for accelerating drug discovery.536,537 Notably, while the idea of applying ML methods to drug discovery dates back to the 1990s, the recent achievements of DL methods sparked a high interest in the field for AI-driven early-stage drug discovery.538,539 Indeed, exploiting the capacity of DL to leverage vast amounts of data to create efficient biochemical representations could transform how early-stage research is conducted. For example, Stokes et al. used a DL GNN to identify new antibiotic compounds, and were able to successfully demonstrate the repurposing of halicin, originally used in the treatment of diabetes, as a lead compound for inhibiting E. coli bacterial growth.540 Additionally, the release of AlphaFold541 has revolutionized the approach to computational protein structure prediction, holding great implications on structure-based high-throughput virtual screening, a routinely used method in early-stage drug discovery.542,543 Generative DL approaches have recently been used to design new small molecules and proteins,224,544,545 with multiple drug discovery companies now progressing AI-driven designed molecules into clinical trials.538 Notably, in 2019, Zhavoronkov et al. showed one of the first examples of generative DL accelerated drug discovery, with the 6 possible lead compounds for DDR1 kinase inhibitors verified by manual biological assays.224 Ren et al. later demonstrated that the same workflow was effective in finding lead compounds for dark proteins�those with no experimentally known structure� using AlphaFold to find the protein structure and binding pocket.546 While automation is now routinely used in the pharmaceutical industry, and AI has made its debut into the pipeline, these components have mostly remained disconnected from each other. Therefore, extensive human input, interface between different steps, and external control is still needed. By combining both into a closed-loop manner, SDLs could help reduce the current bottlenecks and also eliminate human biases in hypothesis generation.547 However, there are two important challenges that drug development does not share with any of the other topics discussed in this review: (i) drug development spans vast length- and time-scales unlike any other SDL system and (ii) biological experiments provide very noisy responses, especially as the complexity of the organism increases. Since the stages of drug discovery typically occur sequentially with target identification, hit discovery, hit-tolead, and lead optimization being distinct stages, it is not surprising that SDLs for drug discovery typically focus on optimizing one stage of the pipeline at a time. Therefore, we will assess the progress in the adoption of SDLs in the pharmaceutical industry by looking at their implementation at different stages of the small-molecule discovery pipeline, mainly focusing on early stage research and preclinical studies. We also dedicate a section to discuss the broader application of SDLs to protein engineering and synthetic biology. We limit our discussion to SDLs applied to biochemistry, such as the development of small molecule drugs, molecules and polymers for biologics, nanomedicines, and production of chemical matter through biological systems."
        }
    ],
    "5.1. Drug Discovery Pipeline": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": ""
        }
    ],
    "5.1.1. Target Identification and Validation": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "In modern early-stage drug discovery, identifying a target, a gene or protein that is involved in a disease, is a critical initial step.548 A great demonstration of the benefit of automation in target identification is the robot Adam that was developed by King et al. to perform high-throughput automated microbial batch growth experiments which are individually designed.37 Adam was used to identify which genes encoded locally orphan enzymes in Saccharomyces cerevisiae (i.e. enzymes with unknown encoding genes).549 The stages of Adam’s workflow included generating hypotheses; generating, designing, and performing experiments, collecting optical density (OD) data, forming growth curves from the OD data; recording and analyzing data; relating the data back to the hypotheses. The hypotheses suggested potential encoding genes for locally orphan enzymes. They were generated using bioinformatics software and databases. For the experiments, several modules including a robotic arm, plate slides, plate centrifuges, and plate washers were embedded in the high-level automation workflow, shown in Figure 30. Notably, the hardware did not require human intervention other than replacing materials, and could hypothetically run for a few days without human supervision. However, it was still at risk of encountering problems where a human would be needed to solve them. In addition, its hypotheses were indirect and required additional experiments and literature searches by the authors to verify Adam’s hypotheses. Along with the hardware-enabled acceleration of target discovery, AI has emerged as a powerful engine in finding targets. Recent developments in AI for target identification and validation were reviewed by Pun et al. 548 While the authors suggest that combining AI with automated target validation and screening can potentially increase the efficiency of these stages of early drug discovery, the integration of AI approaches into SDLs has remained elusive. The lack of robotic automation in target identification studies could be due to the fact that biological experiments have inherent challenges including the extrapolation of results from small-scale experiments to emergent behaviors in biological systems, and predicting the phenotype of systems with altered DNA.550"
        }
    ],
    "5.1.2. Hit Discovery": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Once a target is identified, the traditional drug discovery pipeline enters the compound screening phase, where compounds are screened to find “hits,” compounds that display interaction with the target or desired activity during screening.551,552 This includes assay development and high-throughput screening to conduct pharmacological, chemical, and genetic tests. In recent years, developments have been made in the automation of various aspects of hit discovery, such as virtual and experimental screening, and assay optimization, which represent important steps towards closed-loop drug discovery.38,543,553−555 In 2015, Williams et al. reported the development of the robot scientist, Eve (Figure 30). Eve was developed to perform high-throughput screening of more than 10000 compounds per day for drug discovery.38 Eve operates in three modes: a library-screening mode which involves grid search testing of a randomly chosen set of compounds from its library, a hit confirmation mode in which Eve re-assays hits, and an “intelligent screening” mode where Eve autonomously hypothesizes and tests QSARS. Figure 31 shows how these three modes fit into the greater early stage drug discovery pipeline. Eve generates QSARs using a GP with a linear kernel.176 In addition, active learning using a greedy strategy is implemented to select batches of 64 compounds to test Eve’s hypotheses. The authors made a semantic data model of the screening assay results. The flexibility of Eve’s design allows for the easy definition and modification of assays, including, e.g., general, standardized assays (such as computational assays), targeted assays (such as biochemical assays), and biologically realistic assays and screens for toxicity (such as a cell-based assay). All three modes are integrated with software that communicates with the robotics within Eve’s framework. For the robotics, Eve uses off-the-shelf automation equipment for laboratories. Examples include robotic arms and linear actuators for plate transfer, liquid handling systems for sample transfer, and shaking incubators for screening reactions. For analysis, Eve can measure fluorescence, absorbance, cell morphology (using microplate readers), and bright-field and fluorescence images, with an automated microscope. Once the assay is created and the QSAR problem is defined, Eve can run with minimal human intervention. Remarkably, Eve can further be used to discover new targets for existing drugs; Eve uncovered a second target for an anti-cancer drug which makes it a potential candidate for treating malaria. Eve was also used to compare its intelligent screening with grid search screening, with the authors concluding that intelligent screening is less expensive than grid search screening for pharmaceutical screening which uses large libraries and expensive compounds. While Eve shows great strides towards an SDL since it can optimize the activity of drug molecules for a particular target in a closed-loop fashion, and is proven to be useful for repositioning drugs, one drawback of the platform is that it is not connected to an automated synthesis platform, and therefore it is limited to only testing compounds in its library. Integrating the automated synthesis of new compounds into the pipeline would greatly expand the capabilities of Eve. More recently, Grisoni et al. developed an automated pipeline for hit discovery of liver X receptor (LXR) agonists.553 They combine a DL generative model and automated synthesis in one platform. This modular system, shown in Figure 32, consists of a design module that uses a RNN based generative model with long-short term memory cells to design new molecules as SMILES strings, a verification module that virtually confirms the synthesizability of the designed molecules, and an automated bench-top microfluidics platform that runs the synthesis. The microfluidics platform retrieves reagents, optimizes reaction conditions, and performs one-step reactions to synthesize compounds. The reactions are monitored using HPLC-MS, and the crude reaction mixtures are collected automatically. The only human intervention needed to operate the entire platform is selecting the compounds for pretraining and fine-tuning the model. The authors demonstrate one “iteration” of their pipeline, and do not feed results back from the reactions to the design module. The platform synthesized 61% of the computationally designed molecules in this study. In addition to the automated experiments, the authors performed batch synthesis and further screening of select compounds to confirm activity. Through this study, 12 novel, active LXR agonists were found. Although this platform is not closed-loop, it is a successful example of automated drug design and synthesis, and shows potential to be incorporated in a closed-loop platform. An enzyme assay is an experimental method which qualitatively or quantitatively assesses the activity of an enzyme. 556 With assays being an important part of highthroughput screening, optimizing assays is an area of research in itself, and therefore, automating the assay optimization process is pertinent to creating an SDL for hit discovery. One demonstration of automated assay optimization comes from Elder et al.. 554 They used a cloud-based BO based algorithm, along with automated experiments to optimize a cell-free papain biochemical enzymatic assay for papain inhibitors. The optimization involves minimizing final enzyme concentration, final substrate concentration, and incubation time, while maximizing the value of K’, which is a statistical parameter that uses control data to assess the quality of assays.557 The automated platform included liquid dispensers, microplate reader for fluorescence measurements, and automated microplate washing. Their platform tested, on average, 21 assay conditions in order to find the best conditions, therefore being more efficient and less expensive than other methods such as grid search which requires testing all 294 conditions. This demonstrates the advantage of a closed-loop experimental platform, where the experimental results are fed into the optimizer to suggest future experiments. In addition, the automated platform allows the optimization process to be controlled remotely. Other assays could be optimized on this platform and the technology can be applied to other areas of drug discovery, such as reaction screening and hit selection. Finally, Kanda et al. reported BO combined with automated experiments studied in another context: optimizing a cell culture to produce induced pluripotent stem cell-derived retinal pigment epithelial (iPSC-RPE) cells.555 In this study, the target protocol (differentiation of iPS cells to RPE cells), seven parameters (one parameter for reagent concentration, four parameters for the duration of certain steps, and two pipetting parameters), and validation function are defined by users. The robot booth included a microscope, dry bath, plate and tube racks, an aspirator, a dust bin, a tip sensor, pipette tips, micropipettes, a CO2 incubator, and a dual arm robot. While the seeding, preconditioning, passage, RPE differentiation, and RPE maintenance steps of the experiments were performed by the robot, there was still a considerable amount of human labour involved in the process: initiating and preparing cell suspensions, preparing various reagents, importing plates into and out of the robot booth, taking images of the samples and analyzing them, further processing and testing the cells and media collected from the experiments. In addition, the conditions used for this study, including the robotic equipment, parameters, and scores, are not necessarily directly transferable to different protocols, and must be reevaluated when designing a new study. This platform was able to improve iPSC-RPE production by 88% in 111 days through testing 143 different cell culture conditions. The authors also found that the robot generated cells which satisfy the criteria for research applications in regenerative medicine. While the work focused on the study of regenerative medicine, the authors’ method is not unlike the other examples shown above for hit discovery, and may be applicable to hit discovery platforms as well. This platform has the advantage of being closed-loop, with three rounds of BO performed with a GP surrogate, however, there is room for improvement. Making the platform more flexible to accommodate different types of experiments, and increasing the amount of automation could reduce the amount of human labour required to run and design the experiments, bringing this platform closer to an ideal SDL"
        }
    ],
    "5.1.3. Hit-to-Lead and Lead Optimization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The goal of the hit-to-lead stage is to evaluate and perform optimization on the “hit” compounds from the previous substage to identify which ones are most susceptible to turn into “lead” compounds. Once a lead is found, it usually undergoes multiple rounds of optimization to improve potency and reduce side effects. The integration of SDLs at this substage would answer one of the core demands of the pharmaceutical industry. In fact, while it is relatively straightforward to identify numerous hit compounds virtually or via HTS, prioritizing those for further stages requires medicinal chemistry intuition and testing those hypotheses more thoroughly. Since this process is iterative, it is well amenable to the DMTA paradigm, and therefore to SDL integration. In 2013, Desai et al. designed a fully integrated flow-based autonomous platform assisted by an algorithm design (CyclOps) to perform hit-to-lead optimization, showcasing its use in the case of AbI Kinase inhibitors (Figure 33).558 Starting from ponatinib as a hit compound, the authors defined a chemical space of 270 molecules that could be synthesized in the automated workflow by structural analysis of potanibbound AbI Kinase. The design algorithm would then select compounds from this space to be synthesized on the platform using Sonogashira reactions in flow, purified by in-line preparative HPLC, and analyzed for kinase activity in realtime. The authors used a RF model for activity prediction that used drug-like molecular descriptors involving the Lipinski rules and molecular fingerprints, initially trained on 36 literature compounds. Three design strategies were set up : (i) “chase potency,” an exploitative strategy selecting topscoring compounds based on predicted activity, (ii) “most active under sampled one,” an explorative strategy accounting for the number of times certain reactants have previously been employed and (iii) a hybrid strategy combining (i) and (ii). Overall, the flow chemistry, purification, and bioassay proceeded with a success rate of 71%. In all, 11 key compounds were identified as potent inhibitors of Abl1/ Abl2, with IC50 values in the low nanomolar range. Those were retested with conventional bioassay methods, and the data generally showed a high level of correlation with data generated via the microfluidic platform. In a subsequent paper, Czechtizky et al. demonstrated the reproducibility and consistency of their platform by applying it to replicate xanthine-based dipeptidyl peptidase 4 (DPP4) inhibitors.559 This time, the compounds were synthesized via a two-step synthetic protocol using a Vapourtec R4 flow chemistry system. Overall, 29 compounds were prepared in high purity and tested in only three days with a chemistry success rate of 93%. Close correlation between the microfluidics platform data and data generated within traditional approaches was observed once again. Recently, the CyclOps platform was used to develop hepsin inhibitors selective against urokinase-type plasminogen activator (uPA).560 Over the course of 9 days, 142 novel compounds were generated and assayed with hepsin and uPA. The algorithm explored a virtual chemical space of 5472 molecules, spanning three types of commercially available reagents�a sulfonylating/acylating agent, an amino acid and an amino amidine. Each closed-loop cycle took approximately 90 min on the platform. The authors alternated between exploitative and explorative strategies, but also conducted several grid-search rounds focused on the variation of a specific reagent. The progression from the initial hit to the lead compound was accompanied by an improvement in inhibitory activity against hepsin from ∼1 μM to 22 nM. The selectivity over uPA was improved from 30-fold to >6000-fold. The lead compound found was also further ADMET-profiled (i.e., absorption, distribution, metabolism, excretion, and toxicity) and tested in oncogenic functional assays. When assayed against a panel of 10 serine proteases, it displayed promising selectivity. The CyclOps platform is a great example of concrete application of SDLs to drug discovery development. Leveraging microfluidics for compound synthesis in a combinatorial fashion and coupling it to the RF algorithm allowed saved experimentation time and chemical resources, while leading to the discovery of compounds with enhanced properties. A weakness of such demonstration was that the RF algorithm was only optimizing the compound activity, so it could not be part of decision-making in the event of any synthesis- or process-related issues, e. g. poor reactivity or solubility. One can find more discussion on SDLs integrating Reaction optimization. Recent work from Novartis Medical Research addresses synthesis optimization within hit-to-lead optimization in their microscale SDL. Brocklehurst et al. developed the MicroCycle561 platform, an integrated workflow that connects the infrastructure of Novartis with software tools and a robotics system to create a closed-loop cycle. Candidates are designed through an RF model trained on in-house data for QSAR of physicochemical and biochemical properties. The RF model is then incorporated in a BO campaign, in some cases along with protein docking results, for selecting molecules in the synthesis step. From acquired building blocks, the robotics platform, equipped with automated solid dispensing, liquid handling, and a robotic arm, autonomously performs optimization of reaction conditions and high-throughput microscale synthesis. For the test stage, the MicroCycle platform includes an integrated plating process to prepare microscale assay-ready plates and can perform many types of assays automatically, including physicochemical assays, ADME (i.e., absorption, distribution, metabolism, and excretion) in vitro assays, and target-specific biochemical assays. Starting from a hit compound with moderate activity, the authors used MicroCycle to generate 13 libraries of compounds from 8 reaction types, showcasing the use of their predictive models, automated synthesis, and purification. Over 440 molecules were made and an average success rate of about 50% was achieved, meaning that about half of the syntheses were sufficient for running an assay. With additional analysis and contributions from medicinal chemists, molecules with improved activities and potency were identified, while maintaining good solubilities, and appropriate molecular weights."
        }
    ],
    "5.1.4. Formulation Optimization and Bioavailability": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Drug formulation is an essential stage in the discovery and development of new medicines, allowing to improve bioavailability and targeted delivery. Traditionally, designing drug formulation relies on iterative trial-and-error, requiring a large number of resource-intensive and time-consuming in vitro and in vivo experiments. However, the field has recently experienced a growing interest in integrating ML and automation approaches into the design process, as described in the review of Bao et al. 562 As optimizing drug formulations implies varying multiple parameters related to the drug, excipients, and manufacturing conditions, SDLs could help navigate this highly dimensional space. One example of formulation optimization using an SDL is the work conducted by Cao et al., 563 although this example is not directly tailored to a pharmaceutical application. In this work, a commercial formulation consisting of a mixture of three different surfactants, a polymer and a thickener was optimized in a closed-loop fashion according the following multi-objective (Figure 34): (i) stability and low turbidity, (ii) high viscosity and (iii) low ingredients costs. The TS-EMO algorithm was chosen to suggest formulation parameters318 and coupled to an SVM classifier (trained on initial experimental runs) that classified its temporary suggestions based on their stability. The algorithm was run until the classifier identified eight stable formulations amongst the suggested ones, which were then synthesized automatically using a first robot, and transferred to a second one that performed pH, turbidity, and stability tests. Unfortunately, the samples had to be taken offline to measure viscosity. In 15 working days and without providing any explicit physical intuition to the system, the authors were able to obtain satisfactory formulations. Another example in the literature of SDL for formulation is the work of Grizou et al. 564 in which a new high-throughput droplet dispensing robot was coupled to a Curiosity Algorithm (CA) to study the behavior of dynamic oil-in-water droplets, which serve as promising protocells models�a synthetic celllike entity that contains non-biologically relevant components. The authors defined their parameter space by choosing mixtures of four oils and set a budget of 1000 experiments to observe how varying the oil mixture impacted the speed of the droplets and their division. This observation space was chosen for its simple life-forms-like behavior, which can move and replicate. To do so, small oil droplets are placed at the surface of an aqueous medium, the droplet movements are then video recorded and analyzed using traditional image processing techniques to deduce the speed and division of the droplets. To select the next oil mixture to be tested, the CA first feeds previous observations to a locally weighted linear regressor that approximates the mapping between input parameters and observations. A random target observation is then selected and fed to the numerical inverse of the regressor to infer the most probable experimental parameters that will lead to the target observation. The fully closed-loop platform can conduct more than 30 experiments per hour by leveraging parallelization, a six-time throughput increase from previously reported ones. By leveraging the CA, the speed observation space was more efficiently explored, with only 128 experiments needed to cover the portion of the observation space that random parameter search covered in 1000 experiments. The number of droplets deemed active (with speed >3 mm/s) was also improved 14-fold, without it being an explicit objective. Moreover, two whitepapers on SDL concepts have recently been reported in the formulation development literature, demonstrating the high interest of this field for automation. Hickman et al. have proposed an SDL named NanoMAP, which focuses on the development of nanomedicines for pharmaceutical formulations.565 Nanomedicines commonly consist of a combination of polymer and/or lipid-based materials or excipients that encapsulate small molecules or biologic-based active agents.566 The authors propose to automate the preparation of nanomedicines for screening using nanoprecipitation via liquid-handling robots, while coupling it with active learning strategies. Importantly, this experimental protocol has previously been successfully implemented and can be scaled up by leveraging a microfluidics platform.567,568 On the characterization side, the drug loading capacity (DLC) and encapsulation efficiency (EE) would be automated with appropriate extraction methods and analysis via HPLC. The authors also plan on automating highthroughput in vitro stability and release assays in biorelevant media using 96-well dialysis plates, as well as particle size measurement using dynamic light scattering (DLS) plate readers. Tamasi et al. proposed the development of BioMAP for biologic formulation design (Figure 35).569 Indeed, while therapeutic proteins and vaccines�commonly called biologics�have proven their therapeutic efficacy, they remain extremely fragile under standard pharmaceutical storage and handling conditions, c.a. -78°C. Therefore, extensive formulation efforts are routinely required to avoid their denaturation, using additives such as small-molecule stabilizers, polymer excipients, or surfactants. This is also observed for monoclonal antibodies (mAbs). The authors’ plan on building on their previous experience of coupling automation and ML (see below section on Engineering the stability of proteins for a detailed discussion) to create a fully autonomous platform for optimization of tailored polymer additives. They also aim at increasing their materials library to generally recognized as safe (GRAS) excipients as well as expanding the platform to liquid NP formulation. This ambitious project necessitates careful design of the automated instrumentation, as it must remain flexible for each biologic type. The authors plan on providing extra supportive modules such as a multimode reagent dispenser, a plate heater/shaker, a plate sealer and a vacuum filtration system on top of a multi-purpose liquid-handling robotic system for cell culture and reagent mixing. For liquid NP production, microfluidics and continuous-flow fluidics would be leveraged. Testing the stability of formulations over a range of storage and handling conditions could be carried out using an automated microplate incubator with humidity and CO2 regulation, which could also be used to support cell-based assays. For characterization UV-visible (UV-vis) and DLS plate readers, size-exclusion chromatography (SEC), and a high content imager (HCI) would be employed. A proof-of-principle synthesis and formulation platform by Adamo et al. showcases technological advances in continuousflow synthesis and formulation of pharmaceuticals that could be incorporated into SDLs for formulation optimization.570 The platform’s capabilities included multistep synthesis, purification, crystallization, real-time process monitoring, and formulation. In addition, it was reconfigurable to produce pharmaceuticals with diverse chemical structures and synthesis routes on-demand. With the entire system being approximately the size of a refrigerator, and total cycle time for synthesis and formulation being up to 48 hours, the authors demonstrated a competitive alternative to the usual batch synthesis of pharmaceuticals which can take up to 12 months and involves multiple synthesis steps and formulations occurring in different locations. The authors showed the successful synthesis and formulation of four common drugs in liquid formulations, and were able to achieve a capacity of up to 4500 doses per day of diphenhydramine hydrochloride. Automated components of the platform include pumps, heating reactors, multichannel valves, and gravity-based separators, as well as precipitation, filtration, and crystallization tanks. Only one user is required to operate the entire system. While the purpose of their platform was to demonstrate the production of liquid formulations of common drugs, the design and technology integrated into the SDL could be advantageous not only for formulation optimization, but also for time and cost effective synthesis of molecules in hit discovery or hit-to-lead workflows. However, one drawback of this system that would make it difficult to use in a high-throughput setting is the turnaround time for reconfiguring and cleaning the system, which could take as long as two hours, or potentially longer depending on the complexity of the synthesis. In addition, it is not equipped for solid formulations. Ortiz-Perez et al. 571 proposed an integrated and semiautomated iterative workflow that combines microfluidicassisted nanoparticle formulation, automated fluorescence imaging and analysis with BO to design poly(lactic-co-glycolic acid)-polyethylene glycol (PLGA-PEG) NPs with high uptake in human breast cancer cells. To maximize the uptake, one process variable�the flow rate ratio between solvent and antisolvent�and 4 polymer components that directly influence physicochemical properties (size, PEGylation and charge) were varied. The polymer mixture can be automatically prepared using a syringe pump, and injected into a microfluidic chip at a constant flow rate while the antisolvent rate is adjusted. This automatically produces NPs with controllable size and composition, which can be labeled in situ by incorporating a fluorescent dye during formulation. To measure uptake in cells, NPs are added to cells in 96 well plates and fluorescence microscopy is used to acquire and process widefield fluorescence images in an automated way. This measured response per NP is used to train a BNN to predict nanoparticle uptake from nanoparticle formulation. The uptake, polydispersity index and size of a virtual library of 100,000 NPs spanning the entire design space homogeneously are then predicted using the BNN and tree-based models respectively. The next formulations are then selected from this pool, based on the models’ predictions. With two 5-day experimental cycles, the authors were able to triple the measured NP uptake. While concrete examples of SDLs for drug formulation optimization are still sparse, the excitement of the field for ML techniques and automation foresee a bright future for SDLs. Nevertheless, several questions are left to be answered; notably, how the diversity of therapeutic compounds should be handled. Indeed, contrary to most of the SDLs mentioned in other sections, one would expect the platform to be adaptable to a wide range of drugs, as proposed in the work of Tamasi et al. This is likely to complicate the coordination of experiments both on the software and hardware side. Finally, while this can also be relevant to other stages of the pipeline� and applies to other fields�Lammers et al. highlighted the lack of standardization in studies conducted to characterize formulations such as nanoparticles.572 Better guidelines and reporting is thus needed to provide the best conditions for ML application to thrive in the field."
        }
    ],
    "5.2. Synthetic Biology": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "While so far mostly the discussion has been focused on reviewing SDLs for small molecule therapeutics development, it is important to highlight that the deployment of SDLs has also sparked interest in the fields of protein engineering and synthetic biology,550,573 even inspiring space biologists.574 This naturally extends the potential applications of SDLs to a broader range of therapeutics, but also to the design of new biomaterials or biofuels for instance. The challenge resides in the inherent complexity and non-linearity of biological phenotypes, the high-dimensionality of genomic search spaces, and the error-prone and difficult-to-automate nature of biological experiments."
        }
    ],
    "5.2.1. Biosynthetic Pathways Optimization.": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The microbial synthesis of chemicals offers a viable alternative to widely employed chemical manufacturing methods. Biosynthetic production is appealing due to its ability to leverage a diverse range of organic feedstocks, operate under benign physiological conditions, and circumvent the generation of environmentally harmful byproducts. However, natural cells are rarely fine-tuned to efficiently generate a specific molecule. To attain economically feasible production, significant alterations to the metabolism of host cells are frequently necessary to enhance metabolite titer, production rate, and overall yield.575 Unfortunately, the complexity of biological systems and their multiple components and many unknown interactions among them lead to having to perform many DMTA cycles. Employing automation combined with computational approaches can help expedite this process. In their paper, Carbonell et al. improved the production of flavonoid (2S)-pinocembrin in E.Coli, a natural product, by optimizing a 4-gene pathway (2592 possible configurations) by 500-folds, going through two DMTA cycles.576 First, rulebased pathway and enzyme selection tools were employed to define a synthetic route for (2S)-pinocembrin. A combinatorial approach was then utilized to design 2592 synthetic plasmids, each corresponding to a pathway that varied in terms of gene expression and combination. Assembly recipes and robotics worklists were generated to automate the plasmid assembly� commercial DNA synthesis, part preparation via PCR followed by ligase cycling reaction. After this, growth of microbial production cultures was conducted in a high-throughput manner, products were automatically extracted, and screened via fast-LC and MS. Finally, the data was analyzed using standard linear regression to identify important experimental factors to aid in the design of the next iteration. The pipeline was designed in a modular fashion which would allow other laboratories to replace individual pieces of equipment or protocols to adopt their own methods. The authors tried to demonstrate the adaptability of their pipeline to other compounds production than (2S)-pinocembrin, choosing to optimize the expression of the (S)-reticuline�an alkaloid� pathway in E.Coli. While some of the obtained constructs matched literature titers, experimental difficulties were encountered, suggesting that the proposed arrangement was either unstable or negatively selected against in the cloning host. Further conversion of (S)-reticuline to (S)-scoulerine yielded modest results, although this alkaloid had not been produced in E. Coli previously. Each step of their pipeline leveraged automation, but the entire workflow was not fully integrated to enable autonomous operation. Indeed, PCR clean-up and host−cell transformation were carried out off deck, and plates needed to be manually transferred between certain platforms. Another point that weakens the SDL character of this work is that the “analyzing” component was conducted using a standard least squares linear regression, which could identify trends across the experimental parameters, but did not actively make design suggestions for the next cycle. HamediRad et al. focused the production of lycopene in E.Coli�a food additive and colorant recently proposed as anticarcinogenic�and demonstrated that BO could help optimize gene expression of a 3-gene pathway, outperforming random screening by 77% while only evaluating less than 1% of possible variants (over a total of 13,824 possibilities) through three DMTA cycles.577 The experiments were conducted using the iBioFAB biofoundry, a fully automated and versatile robotic platform consisting of a 6-degree-freedom articulated robotic arm that travels along a 5-meter-long track to transfer microplates among more than 20 instruments installed on the platform and a 3-degree-of-freedom arm moves labware inside a liquid handling station.578 Each instrument is in charge of a unit operation, such as pipetting and incubation. They are linked by the two robotic arms into various process modules, such as DNA assembly and transformation, and then further organized into workflows such as pathway construction and genome engineering. An overall scheduler orchestrates the unit operations and allows hierarchical programming of the workflows. In this work, the iBioFAB platform was used to automate the lycopene pathway DNA assembly with different expression levels of genes using the Golden Gate method,579 as well as the transformation, cell cultivation and lycopene extraction. The BO algorithm proposed DNA assembly designs, which were then converted into robotic commands for iBioFAB to conduct the complex pipetting work using a Tecan liquid handler. The best mutant found produced 1.77- fold higher lycopene titer than the best mutant found using random sampling and the number of evaluations was at least eight times less than the regression-based optimization scheme. This work greatly demonstrates how a flexible platform like iBioFAB and BO can benefit from each other to efficiently explore the gene expression landscape. It is important to mention that the lycopene pathway was chosen as a proof of concept for its straightforward methods of extraction and quantification, which facilitated high-throughput execution using iBioFAB. Indeed, the biofoundry faced limitations linked to compound extraction methods and analytical/quantification methods requiring equipment more complex than a plate reader (e.g., GC-MS or LC-MS instruments). Those challenges could be overcome in the future by the development of largerscale and more sophisticated biofoundries. Another possible improvement highlighted by the authors was that no initial assumptions about the landscape were made prior to using BO. Using the trained model for one system as the starting point for a similar system could potentially result in reducing the number of evaluations to find the optimum."
        }
    ],
    "5.2.2. Engineering Protein Stability": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Proteins are sparking increasing interest in the context of biomedicine and pharmaceutical sciences. However, the moderate stability of proteins, and specially enzymes, is the major drawback hindering the generalized application of these bioactive molecules at the industrial scale. Indeed, the current process conditions may include extreme temperatures, pH values, or presence of organic solvents that are outside the operating stability window of the biomolecule, but that are often necessary to solubilize poorly-water-soluble substrates in high concentration values. Therefore, ensuring protein stability is of tremendous importance to unleash their potential and is a very active field of research.580 Enabling protein stability optimization via SDLs could help to efficiently explore the combinatorial search space that relates protein sequences to their function, and ensure reproducibility of such results. Tamasi et al. investigated the use of active learning for protein-polymer hybrids (PPHs) using copolymers. PPHs are a promising way to address challenges such as solubility and low physical stability of proteins by conjugating them with synthetic polymers.581 They leveraged a GP regression model with BO to identify candidate copolymers for three chemically distinct enzymes, namely HRP, GOx, and Lip, to maximize the retained enzyme activity (REA). While the synthesis of the proposed copolymers was automated, the PPHs formation and REA characterization were undertaken manually. Their discovery process invoked five DMTA cycles and resulted in enhanced thermostability for the three distinct enzymes (46.2%, 31.5%, and 87.6% improvement in comparison to the initial seed batch for HRP, Gox, and Lip, respectively). While this work is not an SDL in itself, the same group has then later proposed the conceptual outline of BioMAP (vide supra), demonstrating the incremental nature of SDL improvements. Recently, Rapp et al. introduced the Self-driving Autonomous Machines for Protein Landscape Exploration (SAMPLE)582 platform for fully autonomous protein engineering and were able to engineer glycoside hydrolase family 1 (GH1) enzymes with an enhanced thermal tolerance (Figure 36). The protein engineering task was framed as a BO problem that was tackled using a multi-output GP, combining GP regression on thermostability and GP classification on protein activity. First, the authors designed a GH1 combinatorial sequence space composed of sequence elements from natural GH1 family members, elements designed using Rosetta,583 and elements designed using evolutionary information. This yielded a space containing 1352 unique GH1 sequences. To pick sequence candidates for the experiments, the authors designed a custom sampling strategy that constrained selection to the subset of sequences predicted as active by GPC. Within this subset, candidates for improved thermostability were then selected. To access the protein sequence space experimentally, SAMPLE relies on combining pre-synthesized DNA fragments using the Golden Gate method579 to produce a gene, which can be amplified using PCR then expressed into the desired protein using T7-based cell-free protein expression reagents. Finally, the expressed protein is characterized using colorimetric/ fluorescent assays to evaluate its biochemical activity and properties. The procedure took approximately one hour for gene assembly, one hour for PCR, three hours for protein expression, three hours to measure thermostability, and overall, nine hours to go from a requested protein design to a physical protein sample to a corresponding data point. The experimental pipeline was fully automated and implemented on the Strateos Cloud Lab.584 To ensure reproducibility, four diverse GH1 enzymes from Streptomyces species were optimized for thermostability, each trial was composed of 20 DMTA cycles. Each resulting enzyme was at least 12 °C more stable than the starting protein sequences while exploring less than 2% of the defined protein sequence-function landscape. This work demonstrates the use of SDLs for improved protein thermostability. Importantly, it confirms the reproducibility of the results across several GH1 enzymes, and reports, to our knowledge, the first example of BO coupled to a fullyautomated Cloud Lab platform. A cloud lab is a fullyautomated decentralized laboratory in which scientists can run multiple experiments simultaneously and remotely, all through a single digital interface. This type of facility allows researchers to have full control over their experiment without having to be physically present in the lab. Moreover, research can be conducted without purchasing costly lab instruments or leasing physical laboratory space.584 Another strength of this work is the exception handling and data quality control mechanism implemented to further increase the reliability of the SAMPLE platform, allowing to flag experiments as inconclusive and add the associated sequence back to the potential experiment queue."
        }
    ],
    "5.3. Outlook and Perspectives": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The use of animal models has played a crucial role in the advancement of modern biomedical research, allowing the exploration of basic pathophysiological mechanisms, but also the development of new medicines. Indeed, because of their role in evaluating new therapeutic approaches, animal models bear the weight of the “go” or “no-go” decision to carry new drug candidates forward into clinical trials. However, the discordances between animal and human studies are frequent, thus drug candidates may be eliminated for lack of efficacy in animals, or discovery of hazards or toxicity in animals that might not be relevant to humans.585 The impressive expansion of the organ on a chip field bears the promise to address this issue by leveraging the latest advances in microfabrication engineering, microfluidics, genome editing and cell culture capabilities.586 Recently, the FDA Modernization Act 2.0 was approved, allowing alternatives to animal testing for drug and biological product applications.587 This change in legislature could potentially improve the adoption of SDLs in the preclinical phase of the drug discovery pipeline. Given the great challenges of automating biotechnology, it is fundamental that laboratories collaborate and collectively develop guidelines and protocols by establishing global alliances588 and consortia.589 For instance, the adoption of the FAIR guidelines143 for data-sharing is crucial for efficient use of DL and ML, especially in a field where many processes are yet to be explained and reproducibility can be an issue. The use of “cloud labs” could also allow researchers to access standardized equipment from anywhere at any time.584 Furthermore, the stellar increase in interest for data-driven approaches applied to drug discovery calls for a reflection upon open science adoption in this industry. Indeed, a lot of success stories in DL stem from data availability and open-source libraries.166 The AI for science excitement could therefore bring us to a long-awaited moment.590,591 While we begin to see success stories where academia and industry join forces in an open science setting,592 it is important to carefully consider and control the risks of misuse associated with sharing biological data and models.593 Throughout this section, we have reviewed the adoption of SDLs across the different stages of the drug discovery pipeline and in the field of protein engineering and synthetic biology. The works we highlighted showcase the great potential of SDLs to transform the current drug discovery process, while shining light upon the challenges ahead. On the algorithmic side, the recent DL achievements have highly increased the community’s excitement and its expectations. Nevertheless, several challenges are already foreseeable.594−596 For example, the ligand-protein binding data from AI predicted protein structures is not as accurate as that which uses experimentally determined protein structures, and it is often not experimentally validated. In addition, more transparency from companies is needed in order to move the field forward as it helps to build trust in their results and allows larger sets of data to become available for ML. Federated learning is a way to overcome this challenge because it can keep company data confidential while using data from multiple companies in one ML model. On the automation side, we expect future advancements in microfluidics technology to greatly improve some of the systems covered above. Moreover, BioFoundries represent sophisticated hardware that is likely to have a great impact in the adoption of SDLs in this context, and could inspire other fields too. Furthermore, while proof-of-concept SDLs exist at various stages, the lack of seamless integration of feedback across these stages poses a potential limitation. Addressing this challenge and fostering collaboration between different phases could enhance the overall efficiency of SDLs. Overall, recent legislative changes, the growing significance of DL and the current advancement in hardware components suggest a favorable landscape for the adoption of SDLs in pharmaceutical science, paving the way for transformative advancements in drug discovery and biotechnology"
        }
    ],
    "6. Structural Materials": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "This section covers materials geared towards structural applications with a focus on mechanical performance of materials. Naturally, we focus on materials and techniques that involve automated and data-driven methods including robotics, optimization algorithms, software orchestration, experiments, and simulations. We do so for alloy design, concrete formulations, non-alloy additive manufacturing, and adhesives. Many of these materials are inorganic solid state materials. All synthesis, processing, characterization, and testing methods of mechanical properties of solid samples require mechanical motion of some kind�whether through the transfer of solid samples or the movement of experimental apparatus around or in contact with solid samples. To date and to the authors’ best knowledge, there are no published demonstrations of fully autonomous SDLs that use data-driven methods to iteratively explore inorganic solid state materials design spaces for largescale structural applications with mechanical performance measurements. However, there has been a large amount of progress towards automated, and in some cases autonomous, synthesis and characterization methods for inorganic solidstate materials such as with Powder-Bot, A-Lab, and ASTRAL, as described in section on Solid State Material Synthesis optimization. Within each section and where appropriate, we point to the data-driven approaches that accelerate the discovery of these materials and which provide the foundation for structural-focused SDLs to come."
        }
    ],
    "6.1. Alloy Design": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "From extreme-temperature Inconel alloys in jet turbines, lightweight aluminum alloys for engine blocks, biocompatible titanium alloys for hip replacement joints, to the classic corrosion-resistant 304 stainless steel found in the kitchen sink, the enabling effects of alloys are immense and often invisible. Events like the fatigue-induced catastrophic failure of “de Havilland Comet” commercial jet airliners in the 1950s, the steel corrosion-induced oil spill of the Alaskan Oil Pipeline in 2006, and the tens of thousands of metallic poisoning cases from cobalt-based hip implants up to 2010, is a reminder of how much the world relies on this class of materials and how the performance can enable (or the lack of performance can hinder) advancements in the transportation, energy, and medical fields. While many alloys are known and in usage, the alloy discovery space is a largely unexplored and high-dimensional search space. In the case of multi-principal element alloys (MPEAs)�i.e., alloys with many constituent components� Miracle et al. estimates that there are nearly 200 million potential MPEAs systems with three-to-six constituent elements. Note that these are individual systems, meaning the tunable parameters of stoichiometry and processing conditions are ignored.597 Over the course of twelve years from their discovery and documentation (2004−2017), the authors estimated that only 122 MPEA systems had been identified. Forecasting the current rate to the year 2117, and using only traditional methods, the risk of missing the best possible MPEAs system for a given application is over 99.999%, pointing to the need for physics-based, data-driven, automated, and high-throughput approaches. The alloy discovery space is not only high-dimensional, it is also multiobjective. See, for example, a spider plot with twelve performance properties for structural applications (Figure 37) such as yield strength, fracture toughness, thermal expansion, and fatigue. While not every structural application incorporates all of these objectives, depending on the application, many of the objectives must be met simultaneously for commercial viability. While fully autonomous setups that iteratively suggest new experiments with automated synthesis, processing, characterization, testing, and sample transfer are rare in materials discovery for structural applications, there has been a large amount of progress towards automating complex and difficult tasks with inorganic solid-state materials. For example, Vecchio et al. demonstrates the use of the FormAlloy tool to automatically mix powder precursors and additively manufacture alloys.598 While characterization and property testing requires manual sample transfer and intervention, the authors developed a unique platform for high-throughput characterization using a turntable-style sample holder with multiple sample positions (Figure 38). The unique benefit of this design is that they integrated it with a variety of characterization tools, which both drastically reduces the amount of sample prep time and simultaneously makes it much easier to correlate multimodal data between instruments with sample batches. For example, scanning electron microscopy (SEM) and electron backscattered diffraction (EBSD) data are spatially correlated with nanoindentation hardness measurements. While no iterative optimization took place, Vecchio et al. used both thermodynamics-based CALPHAD simulations and ML methods for property prediction, and set the stage for an autonomous and robust SDL for alloy discovery. Aside from macroscale structural applications, Kusne et al. demonstrated, in a data-driven and automated synchrotron characterization setup, a large reduction in the number of required experiments to identify optimal epitaxial nanocomposite phase-change memory materials, important for non-volatile data storage applications.516 These materials leverage heat-induced and reversible transitions between amorphous and crystalline phases to mimic the “0” and “1” binary states of conventional transistors but with higher permanence. We note that the materials search was restricted to a single ternary alloy system (Ge-Sb-Te) and the processing parameters were fixed. Additionally, the sequential characterization experiments were carried out on a pool of several hundred pre-synthesized samples on a single silicon wafer via combinatorial sputtering; meaning that only one iteration of synthesis was performed. Their benchmarking results demonstrated that the incorporation of physics-based phase mapping information led to more efficient discovery relative to both random search and BO without phase mapping awareness. In addition to mechanical and phase-change properties, automated methods have been used to search for corrosionresistant alloys. DeCost et al. built an autonomous scanning droplet cell to accelerate the discovery of a novel Al-Ni-Ti alloy composition for corrosion resistance.599 This SDL has automated serial electrodeposition with adjustable solution compositions and online processing characterization (i.e. optical camera and laser reflectance for assessing continuity, coloration, uniformity, and qualitative roughness of electrodeposits; a potentiostat for measured potential, and current; and a pH probe and thermometer for monitoring pH, and temperature). To close the loop in combining all the various process conditions and measured objectives, the authors adopt the active learning strategy using GP to predict and optimize for corrosion resistance (i.e., passivation current, passivation potential, and the slope of the passivation plateau). After several iterations, they successfully found Al-Ni-Ti alloy compositions that were near the Pareto frontier."
        }
    ],
    "6.2. Concrete Formulations": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "If all of the artificial materials in the world were to be categorized and placed on a scale, concrete would be the heaviest. Concrete is the most prevalent human-made material and the second most consumed commodity after water (approximately 30 billion tonnes per year600). Concrete is not a trending scientific topic, like one might expect for topics like superconductors, long-range EV batteries, or quantum computers; however, it is one of the most practical materials topics related to environmental sustainability. The energyintensive nature of manufacturing concrete leads to a weightshare of approximately 8% of human-derived CO2 emissions.600 Unfortunately, there are only a few public examples of iterative and accelerated science methods applied to concrete formulations and processing (at least ones that move past property prediction based on classical ML models). While the reason for this may be complex, in addition to the “hype” factor mentioned previously, there are other practical factors that constrain against deviations from existing concrete technologies, such as the safety concerns of new formulations, or the lack of long-term test data. On a related note, perhaps much of the ML and robotics focus in the field has been concentrated on detection rather than discovery�extending the life of existing concrete rather than seeking to replace it, as may be indicated by the large fraction of detection-focused ML manuscripts in the literature.601−605 Despite these considerations, one promising alternative to traditional concrete formulations involves swapping the energy-intensive “Portland cement” with eco-friendly cements based on alkali-activated binders or “geopolymers.” In an effort to validate and identify optimal data-driven routes for optimization of such concrete, Völker et al. used a set of 131 experimental data from the literature to conduct computational benchmarks, exploring the effect of algorithm choice and parallelization on the efficiency with which high compression strength materials can be identified.606 Notably, this is one of the only studies that shows modern adaptive experimentation being used in the context of concrete optimization. Concrete formulation and processing optimization also has a unique challenge relative to many other materials discovery tasks: it is highly affected by locally available materials. Concrete is dense and is used in large quantities for buildings and other structural applications at a relatively cheap cost-perweight, so transporting it large distances is infeasible. What this means is that an optimal concrete in one part of the world does not translate directly to another part of the world. While the considerations for applying accelerated science tools to the field of concrete discovery are complex, the potential positive impacts are large. We anticipate a fully autonomous concrete formulation and processing optimization tool in the near future, which will require awareness, incorporation of AI and robotics into civil engineering repertoires, and a strong understanding of the limitations and opportunities within the field."
        }
    ],
    "6.3. Non-Alloy Additive Manufacturing": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The use of AI and robotics in additive manufacturing settings holds great promise: in terms of synthesis and optical characterization, there is a relatively low barrier to automation; the equipment typically has an API and native programming abilities; the equipment is adaptable and available, and the processing parameters are straightforward. For example, Erps et al. used multi-objective BO to simultaneously maximize toughness, compression modulus, and maximum compression strength as a function of six primary formulations to form a composite formulation for photocurable resins. All processing parameters were held fixed.607 We note that this is a semi-automated platform which requires human intervention for transfer of materials between each step in the sample fabrication pipeline while all of the individual steps of dispensing, mixing, 3D printing, postprocessing, and testing are completed individually without human intervention. Such formulation systems can be adapted to other domains such as surfactants, cosmetics, foods, and paints. Brown and co-workers have also demonstrated SDL studies of mechanical properties of 3D printed structural materials. While the works predominantly focus on the effects of macroscopic structural designs rather than the intrinsic material property or formulation, the studies demonstrate the potential of autonomous BO for searching various design spaces in additive manufacturing. Gongora et al. introduced Bayesian Experimental Autonomous Researcher (BEAR), combining simulations and experimental observations to optimize the twist angle and struts of a 3D printed structure for optimal toughness.608 The authors use a GP regression model to explore the search space, while a novel automated platform provides high-throughput printing and testing of manufactured parts. Simulations through finite element analysis were shown to strengthen the Bayesian prior of BEAR, increasing the speed of optimization in the experiments.609 In their most recent work, Snapp et al. 610 introduce new mechanical designs (i.e., a generalized cylindrical shell with 8 variable parameters) and filament combinations to improve the material’s mechanical properties. Although their approach employs the SDL framework to accelerate toward an optimal mechanical design, there is an absence of chemical synthesis or materials design. While getting the formulation right and automating the exploration of a wide variety of combinations of starting materials is difficult, optimizing processing parameters within a certain material family is much more attainable. One example of this processing parameter optimization is a low-cost example that uses BO with a modified 3D printer to optimize the print characteristics of a silicone material.113"
        }
    ],
    "6.4. Adhesives": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Similar to many applications in this review, adhesives are a classic formulation optimization problem. The relative fractional share of constituents and the combinations of precursors can have a large effect on the bond strength exhibited by the adhesive. In this vein, Rooney et al. developed a semiautomated SDL platform for adhesive synthesis and characterization using a SCARA-type N9 (North Robotics) workstation, and substrates (“dollies”) to be coated with the adhesive and used with a custom shear-stress “pull-off” tester (Figure 39).308 Adhesive coating, preparation, and testing were all performed automatically via the robotic platform, while mixing of the adhesive formulations was carried out manually. This system used BO with a GP surrogate to maximize the bond strength as a function of resin to hardener ratio in a two-part epoxy system over four iterations of 5-sample batches (20 experiments). Notably from an algorithm standpoint, the BO algorithm from the Ax package uses a sophisticated batch-aware acquisition function."
        }
    ],
    "6.5. Outlook and Perspectives": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Researchers in structural materials are quickly adopting ML techniques to accelerate the discovery of novel materials, but many have yet to adopt robotics technology to automate the synthesis and characterization of such materials. Specifically in structural materials, hardware automation remains a challenge because of the inherent difficulties in solid-dispensing, extreme temperatures, and complex testing instrumentation. By reimagining our approach to synthesis and characterization, shifting from conventional human-oriented instrumentation to a hardware-centric perspective, we can harness the power of robotics to automate intricate tasks in diverse ways."
        }
    ],
    "7. Optoelectronics": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Optoelectronic materials play a pivotal role in modern technological advancements by enabling the manipulation and control of light-matter interactions. These materials are integral to a wide array of applications spanning from telecommunications and displays to solar cells and medical devices. The significance of optoelectronic materials lies in their ability to absorb, emit, and modulate light, thereby facilitating the conversion of electrical signals into optical signals and vice versa. This functionality underpins the development of high-speed data communication systems, energy-efficient lighting, and sensitive imaging devices, among other innovations. Designing new optoelectronic materials and making them technologically useful requires a comprehensive understanding of the complex relationship between their composition, structure, processing, and physical properties such as electronic structure and optical characteristics. Experimentally characterizing these attributes often requires sophisticated techniques. The synthesis and fabrication of these materials with the desired properties can be difficult and resource-intensive; optoelectronic materials are often part of devices in crystallized form or thin films, with blends and mixes of other optoelectronic materials, which can have dramatic effects on the performance of the device. In the context of SDLs, this introduces additional parameters that require further optimization, and will be dependent on the material-specific properties such as stability, scalability, and cost. Due to the complexity of the design-make-test-analyze cycle for optoelectronics, efforts have been made to combine ML and DoE without an automated laboratory, aiming only to more efficiently search the design space of possible compounds and devices. Cao et al. used SVMs to optimize only device processing parameters for an organic photovoltaic device active layer composed of a PCDTBT donor and PC71BM acceptor.611 Subsequently, Kirkey et al. extended this method to study multiple acceptor compounds.612 Conversely, Wu et al. leveraged high-throughput synthesis and characterization to speed up discovery of organic semiconductor laser materials without ML-guided experiment selection by exploring molecules similar to the prototypical BSBCz.613−615 Although these efforts did not involve SDLs, they served as initial steps toward the development of SDLs for optoelectronic materials and devices. In the following sections, we discuss SDLs for optoelectronics based on the sophistication of the proxy measurements: from solution-state and single crystals to full device fabrication and testing."
        }
    ],
    "7.1. In Solution and Crystals": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Proxy measurements of materials in solution and as single crystals offer two primary advantages. Both proxies are simple enough that they can be used to gain fundamental insights into the materials at the atomic level, especially in conjunction with quantum chemical calculations. Additionally, they are relatively amenable to high-throughput experiments.616 In the case of solution-based testing there are many options for highly parallel and high-throughput experiments to study the relationship between composition and important optoelectronic properties using simple analytical techniques such as optical spectroscopy"
        }
    ],
    "7.1.1. Perovskites": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "An example of a commonly studied crystal optoelectronic material is the perovskites. In an SDL developed by Higgins et al., the authors utilized a well plate to automatically and parallelly synthesize 96 multi-component perovskites and analyze the photoluminescence of the compounds in a high-throughput manner.617 Four perovskites systems were studied with varying compositions with the goal of maximizing the photoluminescence of the dissolved crystals. The authors also added a temporal axis to the data, looking at the spectra as a function of time, in order to capture the stability of the compounds. The results of the photoluminescence were decomposed using non-negative matrix factorization (NMF) into spectral information dependent only on the material composition, which was then fed into a GP regression model. The GP model interpolated between the low amounts of data to give a predictive map of the best compositions for the relevant perovskite systems. While the authors did not perform a second iteration based on the predictions of the GP model, the model prediction and uncertainty could have been used in a BO scheme to suggest the next round of experiments. There have also been SDL studies of only the crystallization process of perovskites. Crystallization is not only dependent on the composition of the perovskite, but also the process conditions, which will affect the structure and performance of the optoelectronic device. Li et al. studied the perovskite crystallization using a robotics-accelerated micropipetting system.618 The solution was gradually cooled in an inverse temperature crystallization (ITC) reaction,619 and the resulting crystals were visually categorized into 4 levels of crystal quality. The ITC reaction parameters were the same for all reactions, however the concentrations of the inorganic, organic, and formic acid precursors were varied. All reactions contained the lead (II) iodide, and one of 45 structurally diverse organoammonium cations. In total, 8172 reactions were performed, and the most novel structures were further studied in manual experiments, however, no ML was utilized during the crystallization experiment. Instead, the authors performed a retrospective study, and determined that a SVM model with a Pearson VII universal function kernel was most accurate in crystal quality prediction, using the reaction conditions and the organic and inorganic precursor chemical descriptors, however suffers when trying to generalize to different precursors or perform with small amounts of data. This study demonstrates the potential for a second round of high-throughput experiments based on the predictions of the model. In another work studying the crystallization of perovskites, Kirman et al. incorporated a CNN and a kNN regressor model in their SDL for automated metal halide perovskites.620 Repurposing a protein crystallization robot, the authors were able to study 96 experiments with different concentrations of precursors in parallel (Figure 40). The prepared solutions are then sealed in a chamber with an antisolvent to induce the crystallization.621 Initially, studies focused on phenethylammonium lead bromide, (C8H12N)2PbBr4 (PEAPbBr) crystallization, with 7000 images classified from an initial run as either no/bad crystals, or good crystals. This dataset was used to train a CNN, achieving 95% accuracy in crystal detection. Kirman et al. then applied the workflow to a new perovskite system, with 3-picolylammonium (3-PLA) as the ligand, and different lead halides. Additionally, a kNN model was trained to map the experimental conditions, as well as DFT-based descriptors of the precursors, to the success of the crystallization experiment. Another round of experiments was performed based on the predicted reaction parameters most likely to yield crystallization, and the authors were able to increase their rate of crystallization success from ∼1% to ∼10%. While the SDL still required human intervention to perform the crystallization, Kirman et al. demonstrated a fully closed-loop platform for perovskite crystallization, using AI methods to learn from an experiment to suggest the subsequent experiments, and improving upon their original results."
        }
    ],
    "7.1.2. Nanoparticles": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Nanoparticles (NPs) are another type of commonly studied optoelectronic material. By controlling their size, shape, structure, and composition, the absorption/emission intensity and wavelength, and their selfassembly behavior, can be controlled. A variety of methods have been developed to control NP synthesis, many of which are suitable for automated synthesis platforms. One of the first examples of SDL for NP optoelectronics is from Krishnadasan et al. in 2007.35 The authors used a microfluidics platform to control the injection of CdO and Se solutions into a reactor. The goal of the SDL was to optimize the emission of CdSe NP to some target wavelength, and maximize the intensity, combined together in a dissatisfaction utility function. The goal would be to minimize the dissatisfaction as a function of the controllable parameters using the SNOBFIT algorithm,622 which proceeds as follows: (1) the search space is discretized around sampled data points, (2) quadratic models are fit around each point, (3) the next point to sample is the model minima, (4) the model is refit using new experimental data. Krishnadasan et al. demonstrated a closed-loop SDL capable of minimizing the dissatisfaction function in a 3D search space, varying the flow rates of CdO, Se, and the reaction temperature. The optimization trace is seen in Figure 41, with the best experimental conditions identified at evaluation 71. In a related study from the same group, Li et al. studied the ligand-mediated synthesis of nanocrystals in cesium lead bromide NPs.623 Linear ligands of base:acid precursors are used to define the size and shape of the colloidal NPs. Rather than utilizing an automated experimental design, the authors systematically varied the reaction temperature and the ratio of base:acid ligands, and the effect on the photoluminescence spectra. Salley et al. developed a liquid-handling robotics platform for controlled seed-mediated synthesis of Au nanoparticles.624 Three different experiments were performed, manipulating not only the size, but also the shape of the synthesized nanoparticles. The fitness function maximized is dependent on the absorbance at particular wavelengths in the UV-Vis spectra. In order to close the loop, a GA was used to select for the next experimental conditions. In the first experiment, the authors maximized the absorbance of spherical AuNPs at 553nm, corresponding to a NP diameter of ∼80nm, by manipulating the volume of dispensed precursors. Then they moved onto Au nanorods; starting from a randomly selected set of parameters, the platform synthesized 10 generations of nanorods, with 15 experiments each, ultimately maximizing the fitness function, and converging close to the original parameters proposed in the literature.625 In the final experiment, the nanorods from the second experiment were used as the crystallization seed, generating NPs of octahedral shape, a shape with unknown optimal synthesis outcomes. The authors later extended their study to include automated exploration and optimization of multiple levels of seed-mediated AuNP synthesis.626 Jiang et al. extended the previously described automated platform with in-line optical spectroscopy in UVVis-IR, allowing for high-throughput characterization. The various hierarchical levels of seed-mediated synthesis were also automated and controlled via directed graphs. The results are fed into a custom GA based exploration algorithm based on the Multi-dimensional Archive of Phenotypic Elites (MAPElites) algorithm, as well as the sparsity of the synthesis conditions.627 Tao et al. further extended the use of microfluidics in the SDL development of metal nanoparticles by incorporating a machine learned surrogate model.628 Multiple rounds of closed-loop experiments were performed based on suggestions from the surrogate models trained on the results of previous experiments. The authors studied the formation of gold nanoparticles (AuNPs) under various concentrations of aqueous precursor compounds, and different reaction times. Solution concentrations were manually loaded into the platform based on the suggestions of the algorithm. Uniquely, the SDL optimized for multiple objectives calculated from the UV-Vis absorption spectra, such as the position, full-width halfmaximum, and intensity of the peak, through the use of a novel BO algorithm which allows for hierarchical multi-objective optimization.280,446 The authors successfully demonstrate optimization of NP spectral properties for both large and small AuNP, using kernel density regression, with the kernels estimated via a BNNs. Most recently, Low et al. presented an SDL that optimized the synthesis of silver NPs (AgNPs) using a multi-objective optimization algorithm dubbed Evolution-Guided Bayesian Optimization (EGBO).629 They develop a fully automated SDL for seed-mediated AgNP synthesis, integrating microfluidics, inline hyperspectral imaging, and closed-loop optimization. The optimization goals were to target a desired spectral signature for specific optoelectronic applications, maximize the reaction rate for high throughput, and minimize costly seed particle usage. The various objectives were modeled by a GP surrogate. The EGBO algorithm then combines a batched BO with qNEHVI acquisition function with an evolutionary algorithm, leveraging selection pressure to balance exploration and exploitation toward the Pareto front. Applying EGBO to the nanoparticle synthesis and various synthetic test problems, the authors demonstrate improved performance over state-of-the-art methods in terms of hypervolume convergence, uniform coverage of the Pareto front, and constraint handling. They also investigate pre-repair and post-repair strategies for handling input and output constraints, underscoring the importance of careful constraint treatment in self-driving laboratories. Colloidal synthesis of NPs has also been applied to inorganic lead halide perovskite NPs. Epps et al. developed a highthroughput microfluidic reactor platform with an in situ characterization module.630 Starting with CsPbBr3 quantum dots, the bandgap of the NPs was tuned via halide exchange reactions, via introductions of zinc halide precursors.631 The various precursors were varied to optimize for a joint fitness value comprised of the PLQY, emission linewidth, and the emission energy, which is related to the bandgap. The colloidal lead halide perovskite NPs were flowed through a custom inline module capable of absorption and photoluminescence UVVis spectrometry (Figure 42). The results were fed into a boosted ensemble of neural networks and the next synthesis conditions were selected via BO. The authors compared the optimization with other commonly used methods, e.g., SNOBFIT, and CMA-ES, and found superior performance with the neural networks. Additional performance gain was observed after pre-training the networks with supplemental experimental data. In related work from the research group, Abdel-Latif et al. modified the aforementioned platform to include multi-phase reactions (i.e., gas-liquid), allowing for inseries synthesis of CsPbBr3 quantum dots, and expanding the synthesis parameter space for lead halide perovskite NPs.632 To improvement the ensemble of neural networks, an initial round of 200 experiments were performed to pretrain the networks to predict the FWHM and energy of the photoluminescence spectra. Epps et al. further studied optimization of AI guided experimental design agent used in their SDL through a simulated experimentation platform.633 Using 1000 experimental data points on metal halide perovskite NPs, a surrogate model comprised of a series of GP models served as the HTE platform, and the model, fitness functions, and acquisition functions were tested and compared. Li et al. developed the MAOSIC (materials acceleration operating system in cloud) in order to look at chiral perovskite nanocrystals.119 This class of optoelectronics have shown strong optical activity, and have possible applications in spintronics, sensing, or optical communications.634 However, the controlling the chirality of such semiconductors is nontrivial. Li et al. utilized a microfluidics SDL with a cloud server for data storage and communication. A robotic arm is used for automated transfer of the synthesized NPs into a spectrometer, returning data on the absorbance and circular dichroism (CD) spectra (Figure 43). The SNOBFIT algorithm was used for experimental design, varying the temperature and the precursor concentrations. Synthesized NPs with strong CD intensities were extracted for further analysis via XRD and transmission electron microscopy (TEM). Vikram et al. applied the automated microfluidics approach to optimization of indium phosphide nanocrystals.635 In order to understand the kinetics of the growth and nucleation of the InP NPs, the SDL had a growth stage that spatially separates the various stages of NP synthesis for sampling and characterization. The experimental design of the SDL uses an ensemble of 25 neural networks for uncertainty estimation, predicting the polydispersity, and bandgap of the NPs from the synthesis conditions. And while the kinetics were not involved in the SDL optimization, the additional data on the stages of InP growth were analyzed afterwards. Additional work on lead halide perovskite NPs was conducted by Bateni et al., 636 doping the nanocrystals with cations in a flow reactor similar to those discussed prior (Figure 42).630,632 Similar to the halide exchange mechanism, cation doping reactions were performed in the microfluidics platform through the introduction of manganese acetate, dissolved in 1-octadecene and activated with oleic acid. Spectroscopy data from the synthesized NPs were then fed into an ensemble of 100 neural networks, and the next experiments were suggested based on a greedy BO strategy. The closed-loop optimization campaigns optimized for the peak energy, and the Mn:exciton emission peak area from the photoluminescence spectra, producing on-demand bandgaps and doping levels of the lead halide perovskite NPs. In a recent study from the same authors, Bateni et al. demonstrated Smart Dope, an SDL for multi-cation doping of the lead halide perovskite NP system.637 CsPbCl3 quantum dots were doped with both Mn and Yb cations; the successful doping was confirmed through off-line characterization. Varying the reaction temperature and the precursor flow rates, the optical features in the absorbance spectra, measured using in situ spectrometry, were optimized as proxies for the reaction yield, and Mn and Yb emission. Experimental design was done using BO with a similar ensemble of neural networks, first pretrained on an unbiased dataset of 150 NP synthesis experiments. The optimized Mn-Yb doped NP resulted in an impressive PLQY of 158%. Zhao et al. studied colloidal perovskite NPs in a highthroughput platform consisting of a robotic arm, and a series of modules for pipetting, and UV-Vis spectroscopic analysis.638 While the authors still use liquid precursors, the robot is capable of selecting solutions based on the suggestions of a ML algorithm without human intervention, and has the potential to perform more complex chemical tasks. The authors considered two different systems, AuNPs, and lead-free double-perovskite NPs (Cs2AgIn1−xBixCl6). To start, a literature search was performed to determine the best starting concentrations for AuNP synthesis, and the best surfactants and solvents to use for perovskite NP synthesis. Based on these results, a series of NPs were synthesized while systematically varying the experimental parameters, generating a database of absorption and photoluminescence data for AuNPs and double-perovskite NPs, respectively, along with data on the aspect ratio of the NPs from TEM and SEM images. The resulting datasets were then used to train a sure independence screening and sparsifying operator (SISSO) model, which identifies correlations between the target and compressed input descriptors.639 Based on the prediction of the models, the authors ran an additional iteration experiment, varying the concentrations and volumes of precursors to verify the predictions of the model. For AuNPs, the aspect ratios were measured, and for the double-perovskite NPs, the sizes of the crystals were measured; the created NPs matched the predictions provided by the SISSO model. However, no additional model training with the new results were performed, and no additional iterations were done. With further development of ML algorithms, more sophisticated methods of optimization were studied in the context of optimizing reaction conditions. Deep RL utilizes a neural network agent to decide the next experiments based on some policy. This policy is refined with each experiment, as the agent receives feedback from the environment, in this case the experimental result, in the form of rewards or punishments. Zhou et al. applied this optimization algorithm to finding the optimal conditions for organic reactions in a microdroplet reactor, as discussed in a previous section.336 The agent is a modified long short-term memory network (LSTM) capable of recursively learning from time-series data, such as data acquired over each iteration of experimentation. To overcome overfitting in the low-data regime, the authors pretrained the network on simulated data. Not only was the pretrained neural network based optimizer capable of optimizing the yield of the reactions, the model was able to successfully optimize the SDL synthesis of silver NPs for maximal absorbance at a particular wavelength. In a more recent study, Volk et al. presented AlphaFlow, an RL-driven SDL capable of optimizing CdSe/CdS core-shell NPs with a modular microfluidics platform, optimizing the optoelectronic material over 40 experimental parameters.640 Experimental planning was done using an LSTM agent over 20 steps, with a belief model comprised of an ensemble neural network regressor and a decision tree classifier. The regressor maps the action-state pair to the corresponding reward (based on spectral data), and the classifier determines if the actionstate pair is viable; both models are retrained over each iteration. The authors demonstrate AlphaFlow’s capability to optimize the sequence of injected precursors, and the volume and reaction time at each iteration."
        }
    ],
    "7.1.3. Molecules in Solution": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "In addition to nanoparticles and crystalline materials, optoelectronic molecules are often the precursor to forming thin films and devices. Drawing from a long history of organic chemistry, small organic molecules can be formed from a myriad of organic reactions with careful control of initial organic fragments, much like the precursor solutions in synthesis of nanoparticles. While molecules in solution do not behave exactly the same as when in thin film form or in devices, they are more easily characterized and serve as a proxy to more complex morphologies of optoelectronics. In 2023, Koscher et al. 642 presented an SDL for designing dye molecules integrated with computer-aided synthesis planning, first exploring unknown regions through synthesizing diverse examples to ground the property models, then exploiting the trained models to realize top-performing candidates. The platform leverages automated molecular generation using a graph-completion model trained on existing data to propose new candidate molecules. Viable synthetic routes for these candidates are identified through automated reaction pathway planning with ASKCOS (Autonomous Synthesis Knowledge Cloud Organized System).450,643 Ensembles of message-passing GNNs are employed for automated property prediction, evaluating candidates for specific optoelectronic properties like absorption, lipophilicity, and photostability. Robotic arms, batch reactors, and an automated liquid handler are integrated for automated synthesis to execute the recommended multi-step reaction pathways and isolate products. Crucially, the property prediction models are continually retrained with new experimental data in a closed automation loop, improving their accuracy iteratively. This platform demonstrated both the exploration of unknown parts of chemical space, and the exploitation of important optoelectronic properties in dye-like molecules. Strieth-Kalthoff et al. demonstrated the closed-loop discovery of organic laser molecules across three different SDL platforms, asynchronously.641 The chemical space is defined through the combination of organic fragment building blocks into organic pentamer molecules (Figure 44), similar to previous work done by Wu et al. 644 in which the fragments are joined together through iterative Suzuki-Miyaura couplings. The synthesis was performed through a generalizable two-step one-pot protocol, handled by automated experimental platforms. Absorption and emission spectra were recorded for the in-solution molecules, from which the lasing performance is estimated using the spectral gain factor.645 Results were then uploaded to a database for coordination with the other laboratories. For decision-making, the authors used a GP model, with the molecules represented as embeddings extracted from a GNN. To overcome the issue of low amounts of experimental data, time-dependent density functional theory (TD-DFT) calculations were performed for the enumerated chemical space, and the descriptors generated from the calculations were used to train the GNN. In this transfer learning approach, the embeddings extracted from the GNN provided a stronger set of features for the GP regression task, which informed the subsequent experiments. Ultimately, this work discovered 21 novel gain materials with state-of-the-art lasing performance, of which the top three compounds were successfully tested in devices. The work of Angello et al. demonstrated an SDL focused on discovering organic optoelectronics with good photostability, particularly for organic photovoltaic (OPV) applications.646 Like in the previously discussed works on automated organic molecule synthesis,613,641 the chemical space was predefined through the combination of molecular fragments through iterative Suzuki-Miyaura coupling reactions.431 In this case, the fragments were acceptor and donor complexes connected by a bridge fragment; this is a common design for OPV applications, with light-induced charge separation encouraged by the difference in local electronic energy levels. The platform was capable of synthesis, purification, and structural characterization of the final compounds. The in-solution photostability was then approximated as a product of the spectral decay time, and the spectral overlap of the molecular absorbance and the solar irradiance spectra. In total, the closed-loop synthesis and characterization was repeated 5 times, with the experiments guided by Gryffin, a BNN-based BO algorithm capable of handling categorical parameters (such as the selected fragments).446 After the optimization, the authors further extended the work by using the experimental results from the SDL to perform physics-informed discovery. Whole molecule DFT calculations were performed on the entire space of possible molecules, and the extracted physicochemical descriptors were used to train SVMs. In this way, the experimental results of the SDL campaign were extended to the entire chemical space, and the predicted best and worst 7 molecules were synthesized to confirm the model predictions"
        }
    ],
    "7.2. Thin Films": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Thin films offer another useful proxy for optoelectronic devices without the need for fabricating an entire device. Optoelectronic devices (e.g., light-emitting diodes, LEDs, photodetectors, and photovoltaics, PVs) are based on thin films (nm to μm in thickness) in order to simultaneously balance charge transport, and light absorption or emission. For example, in PV devices a thicker film maximizes the number of photons absorbed by the active layer. On the other hand, it is easier to extract charges from thinner films. Thin films are a useful proxy because they offer the ability to investigate larger length scales and the impact of processing and microstructure on important material properties such as photoluminescence, stability, or charge carrier mobility. Finally, thin films can be fabricated with relative ease through processes including spincoating and thermal evaporation, which can be easily automated and integrated into an SDL. A detailed discussion of data-driven automated synthesis and characterization of thin film optoelectronics and electronic polymers is also provided in other perspective articles.647,648 A study on the SDL synthesis of colloidal and thin film chalcogenide quantum dots handily demonstrates the strong effect the thin film configuration has on measured performance. Chalcogenides are a class of compounds primarily composed of chalcogen elements, such as sulfur, or selenium, combined with various other elements, and are often used in semiconductor technology and materials science. Stroyuk et al. used a novel method of using aqueous precursor solutions of chalcogenide NPs to form multinary quantum dots with composition Cu1‑xAgxInSySe1‑y (CAISSe).649 Previous work from the authors demonstrate that this method produces NPs of similar spectral properties as those directly formed from precursor metal complexes.650 The use of aqueous forms of the precursors allows for automated synthesis using microfluidics platforms. By varying the precursor solutions, the produced NPs vary in Ag/Cu metallic composition (x), and S/Se chalcogen composition (y). By depositing and evaporating the colloidal mixture, solid thin films were formed on glass plates analyzed alongside the colloidal form. Several photoluminescent properties were measured in the experiments, such as photoluminescence lifetime, energy, and rate constant. The experiments were repeated for the quantum dots with a shell composed of ZnS. Due to the relatively small parameter space of the synthesis, the authors simply interpolated between the data points, creating a 2D map of the best CAISSe compositions. In particular, we can see the quenching effects due to the different forms of the NPs, with the photoluminescence lifetime significantly suppressed for the thin film. A second iteration was not performed, however the authors described possible future work involving a ML approach for more complex experimental parameter spaces. MacLeod et al. demonstrate an SDL, named Ada, capable of optimizing thin film fabrication parameters.651 With a robotic arm, Ada is able to move samples between various stations that are responsible for the stages of thin film fabrication. The entire process starts with measuring out appropriate amounts of precursor solution, spin-coating the glass substrate with the material, and then annealing for a specific amount of time. Characterization involves measuring the reflection and transmission spectra in UV-Vis-NIR, and measuring the conductance. The material studied were thin films of spiroOMeTAD, an organic hole transport material used in perovskite solar cells, doped with cobalt (III). By varying the dopant concentration, and the annealing time, Ada maximized the electron hole mobility in the material, approximated as a ratio of the conductance and the absorbance. The results were fed into the BNN-based Phoenics BO algorithm.652 Ada performs subsequent experiments based on suggestions from Phoenics, with the best parameters for global maximum hole mobility identified within 35 experiments. Exploiting the capabilities of Ada, the same group later demonstrated the autonomous optimization of synthesis parameters for the combustion synthesis of Pd thin films. Notably, MacLeod et al. extended Ada with an X-ray fluorescence (XRF) microscope for localizing the Pd in the annealed film before performing the conductance measurements (Figure 45).653 Through the variation of annealing temperature and combustion fuel composition, the authors were able to optimize the Pareto front between annealing temperature and conductivity. Advances in thin film devices often include multinary films, blends of multiple optoelectronic materials that can affect the stability and performance of the devices. Langner et al. developed an SDL capable of fabricating up to 6048 organic polymer films a day, with the experimental planning done by the Phoenics algorithm.654,655 Two quaternary systems with different compositions were explored (Figure 46). The first was composed of P3HT, PBQ-QF, PCBM, and oIDTBr, while the second replaced PBQ-QF with the more common PTB7- Th (i.e., P3HT, PTB7-Th, PCBM, and oIDTBr). The liquid handling robotics platform drop-casts the organic semiconductors onto glass substrates, with variation in the four components that make up the thin films. The films were then exposed to metal halide lamps; absorbance spectra taken before and after the exposure were used to determine the photostability of the quaternary thin film blends. The authors performed a grid-search method in addition to the BO experiments. They found that the full SDL with ML based experiment planning was able to find the blends that were as stable as the grid-search in 27 samples, on average, showing the efficiency of combining HTE with data-driven experimental design. Sanchez et al. 323 proposes a workflow that combines structured GP models with custom physics-motivated mean functions and automated synthesis methods for the optimization of hybrid perovskite thin films with tunable bandgaps. The approach aims to accelerate optimization of properties like bandgap, photoluminescence, and absorption spectra by guiding experiments and reducing the required number of thin film preparations. By incorporating domain knowledge through custom mean functions, the structure GP converges more rapidly to the underlying ground truth compared to classical GP. The article demonstrates the application of this approach to study the bandgap evolution, photoluminescence peak shifts, and absorption spectra changes of MA1‑xGAxPb- (I1‑xBrx)3, a mixed-halide perovskite system relevant for tandem solar cells and tunable light emission. Experimental characterization included measuring bandgaps from absorption onsets, tracking photoluminescence peak positions and intensities, and monitoring absorption spectral features over a range of compositions. The workflow's adaptability to automated synthesis platforms is highlighted, enabling the exploration of higher-dimensional compositional spaces. The authors suggest that this approach could facilitate the discovery and optimization of advanced materials for optoelectronic applications in self-driving laboratories."
        }
    ],
    "7.3. Devices": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "SDLs that can optimize whole devices are incredibly complex because they require integrating and automating multiple workflows with many highly complex experimental systems. However, this also makes it possible to directly test the performance of a device and control aspects from composition all the way to device architecture, which are rarely optimized simultaneously. As a result of the high degree of complexity of SDLs that optimize optoelectronic devices, there are only a few groups in the world with the resources to conduct such research. Despite this limitation, significant progress has been attained in recent years. Du et al. in 2021 developed AMANDA Line One, a robotic platform capable of automated multi-layer device fabrication and characterization.656 Rather than exploring chemical space, the device parameters were varied, similar to the group’s previous work in quaternary systems, described above. The platform used a robotic arm to move the sample between stations on AMANDA Line One for deposition of layers, thermal treatment, and optoelectronic measurements. The active compounds were PM6 and Y6, acting as donor (D) and acceptor (A) organic semiconductors, respectively. Various layers were deposited via spin-coating, with the PM6:Y6 active layer sandwiched between electron and hole conducting layers to form the device, shown in Figure 47. In total, 10 different processing parameters were varied in the device fabrication, optimizing for four figures of merit: open current voltage (Voc), short circuit current (Jsc), fill factor (FF), and the PCE. Due to the parallelized high-throughput nature of the platform, ∼100 process conditions were systematically explored; without an experimental planning algorithm, the best fabrication parameters were identified within these experiments, producing a device with a PCE of ∼14% in ambient conditions, aligning with the results from the literature.657 The authors utilized the data from the automated platform to train a GP regression model, correlating spectral features obtained from the absorption spectra to the figures of merit, which gave some physical insight for the differences in performance. However, the platform is not yet integrated with an automated experiment planner, nor was a second iteration performed based on the feedback from the GP model predictions. The following year, Liu et al. published their work on BO of perovskite solar cell devices fabrication parameters.658 Motivated by the rapid spray plasma processing (RSPP)659 technique for high-throughput fabrication of open-air perovskite cells, the authors aimed to find the best process parameters, varying the substrate temperature (°C), speed of the spray and plasma nozzles (cm/s), flow rate of precursor liquid (mL/min), gas flow rate into plasma nozzle (L/min), height of plasma nozzle (cm), and plasma duty cycle (ratio of time plasma receives DC power). A Guassian process was trained on batches of experiments, with the next iterations informed by the upper-confidence bound (UCB) acquisition function, in a BO setting. In all, 5 rounds with 20 devices each were performed, with significant manual work involved in the manufacturing and testing of the devices. The authors were able to more quickly identify parameters for higher PCE devices using their ML-guided experiment planning than previous experiments led by human decisions. While both works demonstrate significant advancements in the automation of the hardware and software for device optimization, there is still a lack of a fully closed-loop SDL for process optimization or material discovery for optoelectronic devices. However, the pieces to the puzzle show great potential, and an optoelectronic device SDL may only be a few years away."
        }
    ],
    "7.4. Outlook and Perspectives": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "SDLs for characterizing optoelectronic materials in solution and in crystals are the most developed because of the ease of carrying out the requisite experiments. In-solution experiments, in particular, are the simplest to set up and execute, relying only on pumps and well-established spectroscopic equipment. SDLs for thin film characterization or device fabrication, on the other hand, require substantially more complex systems such as robotic arms for transporting samples and vacuum chambers for depositing interlayers and contacts. At the same time, these highly complex SDLs possess the greatest potential for accelerating design and discovery in optoelectronics. There are a number of challenges that stand in the way of fully realized SDLs for optoelectronic devices. Optimizing optoelectronic materials and devices requires taking into account numerous processes that occur at length scales from Å to μm. While this is challenging, larger quantities of data and improved ML or DL models can potentially overcome it. Next, the sheer number of possible variables�for example, selecting a material with appropriate properties, depositing a thin film and fabricating a device�can easily reach a design space that becomes challenging for BO. At the same time, the cost of the experiments is simply untenable for optimization algorithms such as RL or evolutionary algorithms. While a simple solution might be to optimize devices based on a handful of accessible materials, simultaneously optimizing material synthesis and processing would be transformative. Synthesizing a small amount of a new material in order to tweak how it responds to processing conditions has the potential to significantly accelerate the development of optoelectronics. However, this remains a long term vision due to the many challenges enumerated in the Reaction Optimization section on top of those discussed here."
        }
    ],
    "8. Energy Storage Materials": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Efficient energy storage systems are imperative to exploit the full potential of renewable energy sources, such as solar and wind, to reduce reliance on fossil fuels. The substantial amount of solar energy accessible on Earth could theoretically satisfy all human energy demands by powering photovoltaics and solar thermal systems. However, the intermittent nature of sunlight significantly limits the growth of solar power. For example, in California, peak solar power production during the day drives down the price of electricity, sometimes to negative territories, reducing the incentives for more installations.660 Efficient and powerful energy storage technologies can ensure a stable power supply by capturing the excess energy during the day and releasing it at night, mitigating reliance on fossil fuels. While optimization of battery designs and devices are possible, here we focus on the SDL development of new materials for improved energy storage. Electrochemical energy storage can be roughly divided into two broad categories based on the length of intended storage and speed of power delivery. Short duration energy storage technologies include capacitors and supercapacitors which charge and discharge within seconds to deliver high power. There are industrial efforts to automate the manufacturing of supercapacitors,661 however, SDL-driven discovery of new chemistry and materials is limited. Long-duration electrochemical energy storage is possible in batteries and redox flow batteries, as well as by converting the energy to liquid fuels such as alcohols or ammonia.662 Batteries store energy in the form of reversible chemical reactions in static and enclosed cells. They are relatively compact and inexpensive, suitable for mobile applications such as consumer products and electric vehicles. To scale up, cells can be assembled into battery systems, which require a dedicated battery management system. redox flow batteries (RFBs) also use reversible chemical reactions. However, the redox-active materials (RAMs) are solutions that can be stored in tanks and circulated through electrochemical reactors to generate power. RFBs can scale energy and power independently with higher tank volume and larger flow reactors, respectively, making them more scalable and cost-effective than batteries for grid applications, such as offsetting energy production and demand peaks.663 H2 gas and liquid fuels can store electrochemical energy off the grid, typically generated through non-reversible chemistry in flow reactors such as fuel cells and electrolyzers. Research in this area mainly focuses on cost-effective electrocatalysts that interconvert chemical energy and electricity. The major challenge in developing the aforementioned energy storage technologies lies in the need to develop specific materials and systems for different use cases. Compromises often have to be made to strike a balance between requirements. For example, lithium iron phosphate (LFP) batteries, a type of LIB, are widely used in electrical vehicles due to low cost, high thermal stability and long cycling life. But they are not suitable for cell phones because of lower energy density than most other LIBs. Even with the same choice of electrode materials, there is still a large and high-dimensional parameter space to explore to optimize the performance of batteries, including electrolyte formulation, cell configurations and assembly methods. SDLs can effectively address these complex problems, and offer large time and resource savings compared to traditional manual or high-throughput experimentations. An ideal SDL for energy storage should be able to automatically design, make, assemble, and test energy storage technologies at different scales. An end-to-end platform for battery or flow battery development without human intervention is a major challenge. The industrial production of batteries has undergone significant automation to achieve highthroughput and capital efficiency. These processes aim to carry out precise actions to maximize consistency and productivity in large-scale manufacturing processes, at the cost of flexibility for research and development. SDLs for battery research should focus on the ability to test new materials and optimize electrochemical processes in batteries of standardized form factors. In comparison, flow batteries operate on a large variety of redox chemistries, many of which have not been scaled up to industrial relevant levels. Therefore, SDL for flow batteries should focus on the screening and scaling of molecules and materials, and the optimization of these materials in realistic flow reactors."
        }
    ],
    "8.1. Materials Synthesis and Characterization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "A major focus of energy storage technologies is to develop materials that can improve device performance and longevity. The synthesis of materials for energy storage requires low cost and high scalability because they are aimed to be produced at massive scales. Therefore, low-cost feedstocks such as metals, metal oxides, and products and wastes of petrochemical processes are greatly preferred. There is also a drive to simplify preparation steps, and minimizing the need for solvents and purifications. In comparison, there is a stronger motivation to characterize materials as detailed as possible, whether as synthesized, in situ or even operando, to fully understand underlying processes. SDLs for energy storage will likely have a relatively small synthesis component, but relatively complex characterization capabilities. The common characterization methods include XRD, SEM, and solid-state NMR for solid state materials; thermal analysis for polymers; and most importantly, various electrochemical methods. The ability to conduct electrochemical analysis in a fully automated fashion is the prerequisite of any SDL that studies batteries, flow batteries, fuel cells or electrolyzers. However, this is often a challenge due to the fact that most commercial potentiostats, the instrument that performs electrochemical tests, are expensive and only operate with closed-source softwares, making them difficult to integrate into automated workflows. An example of such effort is the Electrolab, a modular electrochemistry platform by Oh et al. able to automatically formulate redox electrolytes and characterize them using cyclic voltammetry (CV) across various conditions without human intervention (Figure 48).664 Their platform integrates a liquidhandling robot to mix solutions and dispense them onto a series of cells containing an electrode array connected to a potentiostat (Figure 48C). After measurements are done, the robot can also de-gas and clean the cells. Electrolab was able to run a series of 200 CV scans in 2 hours (along with cleaning) on a known redox species under a variety of concentrations and scan rates. They then demonstrated a grid search to find the optimal conditions for a supporting electrolyte when scanning a candidate redox polymer for a nonaqueous RFB. Given the modularity of their setup, it seems possible to extend to smarter data-driven search algorithms to find interesting molecular candidates or remove the need for exhaustive grid search in the future. In recent works initially introduced by Hickman et al., 304 they demonstrated a low-cost SDL platform for electrochemistry discovery. The platform combines a synthesis platform, MEDUSA, and open source potentiostat for endto-end automated complexation and electrochemical characterization. Adapted to the ChemOS 2.0 orchestration framework,111 a closed-loop optimization of the redox potential of metal complexes for flow battery application was demonstrated using the Atlas optimization library.304 The low-cost and opensource features of such a platform make it accessible to a broad scope of researchers, therefore allowing for the democratization of SDL for community-based research. 8.1.1. Lithium-Ion Batteries. Lithium-ion batteries (LIB) are among the most influential technologies today, enabling the establishment of two hundred-billion markets: portable electronics and the electric vehicles.665 Since their commercialization in 1991, the energy density of LIBs has been improved by over 200%, but as the market grow and demand diversify, there is a need to optimize their performance, stability, safety, cost, and environmental footprint. Much of the conventional workflow is centered on trial-and-error approaches to find better materials. Given the enormous space of possible electrolytes, it is difficult and unreliable to screen active materials by manual experimentation. This motivates the need for SDL systems to improve LIBs. Data-driven ML methods for battery design have already been demonstrated on limited experimental dataset, such as for electrolyte formulation,666,667 and battery lifetimes.668 In general, a fully automated workflow is difficult and expensive due to sophisticated engineering requirements, whereas semi-automated platforms that investigate some aspects of the material are more feasible for researchers.669 Electrolyte formulation is one of the key research areas of LIB research. Most LIBs nowadays require a liquid electrolyte to conduct electricity with minimal undesirable chemical reactions. Dave et al. developed a pipeline to automatically measure the electrochemical properties of 10 different solutions in different compositions (251 total) for use in LIBs using a Bayesian optimizer they developed called Dragonfly.670 They later extended their pipeline to nonaqueous LIB electrolytes, which is a larger search space than aqueous electrolytes on account of co-solvents (although not necessarily a harder search space, as finding electrolytes that work in water is difficult).671 Their system could automatically create and characterize different solution compositions, although some human assistance was required when transferring electrolytes into pouch cells for characterization. Their search space consisted of over 1000 points over three axes: solvent mass fraction, co-solvent ratios, and salt molality. In both cases, Dragonfly found electrolyte compositions that were novel or non-intuitive and better than benchmark electrolytes. They also noted that their experiment planner resulted in a sixfold speed increase in finding the optimum compared to random searching by their robotics platform and postulated the same process would take far longer through manual search. Svensson et al. developed an automated screening platform for different electrolyte formulations, consisting of two platforms: a system to formulate and characterize electrolyte solutions and a system for coin cell assembly/disassembly and electrochemistry characterization, which are linked together using a robotic arm.672 While they only screened one electrolyte formulation as a proof-of-concept, their robotic platform was able to obtain similar measurements for assembled batteries compared with batteries assembled by hand, showing that this part of the battery development pipeline can be automated as well. Vogler et al. 132 demonstrate a brokering approach to orchestrate modular and asynchronous research workflows, enabling the integration of multiple laboratories for LIB electrolyte development. They implement a passive brokering server called FINALES to mediate communication between various tenants, which can be physical modules like experimental equipment, or digital modules such as machine learning agents or simulations software. The SDL comprises an experimental setup for automated synthesis and analysis of LIB electrolytes through a pump and valve system with stock solutions, and connected densimeter and viscometer. As another tenant, a simulation orchestrator using Pipeline Pilot for molecular dynamics simulations is used to calculate ionic transport coefficients, radial distribution functions, and other properties critical for electrolyte performance. An AiiDA interface provides ML models to predict low-fidelity conductivity values,673 and also for BO surrogate modeling. As a proof of concept, the authors aimed to maximize viscosity while minimizing density, using a GP optimizer combined with Chimera for multi-objective optimization. The demonstration successfully orchestrated these tenants across five countries, optimizing electrolyte formulations based on lithium hexafluorophosphate salt in carbonate solvents like ethylene carbonate and ethyl methyl carbonate. This SDL approach enables efficient screening and optimization of new electrolyte compositions to improve critical performance metrics like ionic conductivity, essential for developing next-generation LIBs. Another key research direction of solid-state electrolytes (SSE) for LIBs, such as metal oxide and polymer ion conductors, is to avoid the fire hazard and degradation issues caused by organic solvents. SSEs are also more compact than liquid electrolyte giving rise to higher energy density in batteries.674 However, finding solid-state materials with high ionic conductivity, low electrical conductivity, and high electrochemical stability is a major challenge.675 Currently there is no single best solid state electrolyte material for LIBs, partially due to limited understanding of lithium ion transport in bulk solids and at interfaces of different materials.676 Computationally, a number of works have developed frameworks to featurize solid state conductors and train ML models to either predict properties or recommend new conductors.677 He et al. developed a high-throughput screen platform which integrates a large database with modules that calculate iontransport-related properties and a hierarchical search algorithm to propose promising candidates.678 Laskowski et al. reported ML-guided synthesis of Si-doped Li3BS3 using solid-state reactions, reaching superionic conductivity above 10−3 S cm−1 , surpassing most reported SSEs.679 However, their discovery approach is neither automatic nor closed-loop. To our knowledge, fully automated close-loop SDL that discovers/optimizes solid-state electrolyte materials is much needed but very rare. One of the closest examples of an SDL was developed by Shimizu et al. 680 The Connected, Autonomous, Shared and High-throughput (CASH) laboratory integrated several components of an automated SDL, with some human-in-the-loop steps in initializing the synthesis step. The authors minimize the electrical resistance of Nb-doped TiO2 thin films by varying the oxygen partial pressure during the deposition of the film. Human intervention was required to load substrates and prepare the system for sputter deposition, after which the thin-film deposition and resistance characterization were carried out automatically. A robotic arm transferred the sample between the dedicated chambers (Figure 49). For experimental planning, a BO approach was utilized, with a GP regressor as the surrogate. The CASH SDL identified the global minimal resistance within 18 experimental samples for two different sputtering targets. The authors also outlined future plans to expand the characterization platform for multiple physical properties. Active electrode materials store ions over many chargingdischarging cycles and determine the battery's cell voltage. The chemical space of possible active electrode materials is large, ranging from graphite and Si-metal alloys to mixed-metal oxides and phosphate salts. By 2010, there were over 25,000 real and hypothetical negative electrode materials investigated, but experimental verification remains a bottleneck.681 SDL for the discovery of new electrode materials does not yet exist, to our best knowledge. However, the development of highthroughput methods, such as through physical vapor deposition or solution-based methods, have allowed for combinatorial exploration of electrode materials, for example, negative electrode Si-metal alloys,682,683 and positive electrode Li-Ni-Mn-Co-O or Li-M-PO4.684 In addition to the highthroughput synthesis of these materials, there are various highthroughput characterization methods for both structural and electrochemical characterization of electrode materials.685,686 By performing characterization in large batches, combinatorial searches of the material space can generate large datasets for future data-driven applications. McCalla outlined efforts and engineering bottlenecks using automated workflows to accelerate the design of battery materials, including solid-state electrolytes and electrode materials.669 Currently, semi-automated systems might be more feasible for most academic researchers because they balance speed, cost, and adaptability. Nonetheless, a review by Szymanski et al. discussed in detail the challenges and opportunities in close-loop optimization of inorganic materials for batteries, highlighting the importance of future SDLs for not only liberating human researchers from low-level manual tasks but also possessing the ability to explore new materials without being limited by the development of theories.686"
        }
    ],
    "8.1.2. Alternatives to LIBs": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "There has been extensive work on alternatives to LIBs, such as Li-O2 687 and Li-S688 batteries, looking to achieve many folds higher energy density; and sodium-ion batteries, aiming to reduce cost and the reliance on metal resources such as lithium, cobalt, and nickel.689 SDL can help develop crucial materials for these technologies which are still in early phases of research. Matsuda et al. demonstrated an SDL for the automated electrolyte synthesis for Li-O2 batteries.689 These batteries suffer from poor cycle performance due to low reaction efficiencies for the oxygen-generating (positive) and lithiumforming (negative) electrodes. As a result of the highthroughput screening guided by ML, they found multicomponent electrolyte additives for Li-O2 batteries that gave rise to a stable solid electrolyte interface. Their automated experiments covered a total of 14,460 samples, with 4,320 samples allocated to random search, another 4,320 for hill climbing involving the top 10 samples, an additional 4,320 for hill climbing with the top 50 samples, and finally, 1,500 samples for BO (Figure 50). Combination of the hill-climbing method with BO resulted in significantly improved Coulombic efficiency with all top 10 samples exceeding 91%."
        }
    ],
    "8.1.3. Redox Flow Batteries": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": " Since the debut of RFBs in the 1970s, researchers developed a variety of redox chemistries and technologies for RBFs, yet none of them have reached the scale of LIBs. Prior to 2015, RFBs primarily rely on inorganic salt RAMs, such as vanadium flow batteries (VFBs) and zinc bromide batteries. However, these batteries rely on scarce metal resources or highly corrosive operating conditions, which leads to a high cost of energy storage and maintenance. To achieve wide deployment, significant cost-reductions are needed to reach a target installation cost of $100/kWh and a levelized storage cost of $0.05/kWh.690 This is achievable by optimization of the electrolyte solution, and discovery of more cost-effective chemistries. Similar to LIBs, formulation of the electrolyte solution can result in enhanced performance. Gao et al. presented the Solubility of Organic Molecules in Aqueous Solution (SOMAS) dataset for advancing ML algorithms in the exploration of aqueous organic RFBs.691 In the case of nonaqueous flow batteries which use organic solvents instead of water, mixed-solvent and mixed-electrolyte systems can be explored. Deep eutectic solvents (DESs) are an attractive choice of solvent with low toxicity, broad commercial availability, and low costs.692 The properties of DESs can be fine-tuned with their composition. A recent study by Rodriguez et al. demonstrated a high throughput and data-driven search 185 quaternary ammonium salt (QAS) molecules identified as good candidate components for DES and synthesized DESs using liquid handling robots to combine these components. They tested several physical properties, including melting point, electrochemical potential window and ionic conductivity. It is worth noting this work is based on Jubilee,61 an opensource hardware machine based on 3D printing hardware with automatic tool-changing and interchangeable bed plates. Another major research direction is to find low-cost electrolytes made from earth-abundant and widely available resources, and operate in mild aqueous environments. Over the past decade, researchers have explored various small organic molecules,694 polymers,695 and metal coordination compounds696 to address the limitations of inorganic salts. The structural flexibility of organic molecules has facilitated the exploration of a broad spectrum of chemical and physical properties. They also resulted in a massive chemical space that is difficult to navigate with traditional computational and experimental methods. Currently, there is no SDL capable of completing the DMAT cycles of new redox materials. The design step can be achieved based on the computational screening of different molecular classes, such as generating new analogues by combining core structures and making peripheral substitutions,697,698 or using generative models and principles of inverse design.699 For example, Jinich et al. computationally assessed 315,000 metabolic-inspired redox reactions700 while S. V. et al. showcased the de novo design of radical species as both catholytes and anolytes.701,702 The above workflow narrows down the number of candidates that can be practically synthesized. However, the “make” capabilities in SDLs are restricted to producing molecules within the same class that can be synthesized under similar conditions. Additionally, precise electrochemical assessment of RAMs demands samples of high purity. Conducting tests in real batteries necessitates a considerable quantity of samples, prompting the need for scaling up synthesis (see the Reaction Optimization section). Recently, a new class of radical-based organic RAMs showed promise for higher-throughput exploration due to a simple SN2 substitution reaction scheme.703 A low-hanging fruit is the relatively straightforward synthesis of metal-ligand coordination compounds. Porwol et al. reported an autonomous chemical robot that can explore this process and discover the rules of coordination chemistry.477 This is an important step towards generalizable synthesis of different redox-active metal-ligand complexes, which can be used to generate suitable RAMs on demand. The test of new electrolytes focuses on the evaluation of key performance metrics, such as redox potentials, chemical stability and solubility. Liang et al. reported an important work on the high-throughput and automated solubility determination.704 Solubility is important because it determines the highest possible energy density of the electrolyte solution. There is a significant computational challenge in quantitative prediction of the solubility of organic electrolytes. The authors assembled a robotic system in an argon glovebox to experimentally measure solubility of electrolytes. They showcased the effects of additives on solubility in aqueous flow batteries and the development of solubility databases for non-aqueous systems. Most recently, Noh et al. 705 of the same research group presented an integrated high-throughput robotic platform and BO approach for accelerated discovery of optimal electrolyte formulations for non-aqueous RFBs, specifically the RAM 2,1,3-benzothiadiazole (BTZ). The goal of the study was to improve solubility of BTZ in organic solvents. The SDL carried out automated sample preparation through powder and liquid dispensing with a robotic arm. The solubility of BTZ in the solvent was measured via quantitative NMR spectroscopy. The BO component employs a surrogate GP model, operating on molecular features derived from physicochemical properties and DFT calculations of the solvent and solute. The authors identified multiple binary solvent systems with remarkable solubility thresholds exceeding 6.20 M from a comprehensive library of over 2000 potential solvents. Notably, their integrated strategy necessitated solubility assessments for fewer than 10% of these candidates, underscoring the efficiency of their approach."
        }
    ],
    "8.1.4. Hydrogen and Other Fuels": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Other than enclosed LIBs and flow batteries, electrical energy can also be stored in the chemical bonds of fuels. H2 has the highest gravimetric energy density, or specific energy, of any known chemical, although specialized materials and conditions are needed for its safe storage and transportation.706,707 Liquid hydrocarbons offer a high volumetric energy density, 708 making them easy to store and transport indefinitely. Currently, only hydrogen and methanol can be directly converted back to electricity in fuel cells.709 Ammonia is considered a good carbon-free energy carrier for the future,662 although the electrosynthesis of NH3 from N2 is still challenging. Hydrocarbons are challenging for direct fuel cell consumption due to CO poisoning and carbon deposition on catalytic surfaces. They have to be reformed to generate H2 for hydrogen fuel cells. The development of electrocatalysts is central to the development of both fuel cells and electrolyzers. One important topic is the sourcing of hydrogen from water via electrolysis using earth-abundant catalysts. Fatehi et al. outlined the design of an SDL that is designed to find such catalysts to address the sluggish oxygen evolution reaction in water electrolysis.710 They developed a framework for electrocatalyst SDLs consisting of three automated components: liquid handling, electrochemistry, and software that handles data and optimize experiment parameters (Figure 52). They use GP-based BO to find ideal material composition in a closed loop fashion. Their ultimate goal is to discover earthabundant mixed-metal oxide catalysts for OERs in an acidic medium. They demonstrated the optimization of CoFeMnPbOx electrodeposited catalyst materials through multiple campaigns. Within hours, they were able to identify successful formulae for catalyst synthesis and operational conditions that are corroborated with results in scientific literature. One interesting aspect of this work involved developing proxy measurements for target properties since the ideal characterization machinery was difficult to incorporate into the SDL. Fatehi et al. created a proxy measurement for stability by holding the material at an overpotential for an extended period of time, and found that it was helpful for optimizing within a space of materials. Another important work described the accelerated discovery of solid-state material in fuel cells which was discussed in a previous section on Solid state materials synthesis. The goal of the work by Ament et al. 517 was to autonomously design bismuth oxide thin films. The automated fabrication of Bi2O3 films of different phases have possible applications to thin film solid oxide fuel cells.711"
        }
    ],
    "8.2. Device Design, Assembly, and Characterization": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": ""
        }
    ],
    "8.2.1. Cell Batteries": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Another important area of automation in battery development is cell assembly. This is typically done manually in a research setting. Coin cells are relatively easier to prepare than other cell types and have low material costs, which are beneficial for quick battery prototyping.713 Zhang et al. developed AutoBASS, which automatically assembles 64 coin cell batteries per batch (Figure 53).712 They found that their system produces consistent and reproducible results across batches and parameters for a single electrolyte, which is promising both for improving quality control and increasing the speed of lead discovery. Yik et al. created ODACell, a 4-robot system which combines electrolyte formulation with automated coin cell assembly.714 While their batch throughput is smaller than that of AutoBASS (16 vs 64), their system can formulate different electrolyte compositions using a liquid handling robot, potentially allowing for easier integration with optimization algorithms in the future to search for ideal compositions. ODACell was used to test the impact of contaminants (specifically water) in non-aqueous batteries. The degradation of batteries was measured after being contaminated with different water concentrations, and it was found that the variance of the experiments increased at higher water concentrations when replicated multiple times. This illustrates how automation can effectively identify instances of failure or conditions characterized by elevated performance uncertainty"
        }
    ],
    "8.2.2. Flow Reactors": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The flow reactors in flow cells, flow batteries, and fuel cells share some common design elements, which in themselves have an enormous parameter space to explore. An electrochemical flow reactor is intricate, typically composed of a complex stack of multiple layers, including separators (commonly ion-exchange membranes), electrode materials, current collectors, gaskets, flow plates that regulate flow fields, inlets of liquids and gasses, etc. In most cases, they are assembled manually, as is the system demonstrated by Li et al. The electrochemical flow reactor assembly is difficult to automate, but tests can be performed (1) in parallel reactors and (2) sequentially using the sample reactor by cleaning out the reactor before the measurement. The research goal on the device level is often the optimization of performance by searching the parameter space of reactor design (flow field and materials) operating conditions (electrical and flow system management), and exhaustive monitoring of device degradation or failure over time. As shown in Figure 54, the complex configuration of the electrochemical flow reactors often leads to reproducibility issues since slight misalignments and different applied pressures likely lead to varied device performance. Currently, only fuel cells based on proton exchange membranes are known to be assembled automatically in industry, using highly expensive commercial setups.716,717 For instance, Thyssenkrupp Automation Engineering GmbH demonstrated a commercial plant that produces one electrolyzer per second, or at least 50,000 fuel cell stacks yearly. Such manufacturing maturity and scalability has not been realized in the production of flow reactors for flow batteries and electrolyzers. Other than the exploration of the chemical space of RAMs, a secondary discovery process is conducted to explore the electrochemical parameter space of the RAMs, with a primary focus on optimizing battery performance. This exploration encompasses various factors, such as formulating electrolyte solutions, pairing posolyte and negolytes, determining properties of the membrane and electrode materials, designing the flow field, specifying flow rates, and other related considerations. The electrochemical parameter space is extensive and can become complex as more realistic factors are taken into account. While this exploration has been extensively conducted in a few inorganic systems, particularly in strongly acidic vanadium batteries, the realm of emerging organic RAMs operates under distinct conditions. This necessitates the use of new materials and battery designs for both aqueous and nonaqueous systems with different testing conventions.718 Recently, the Aziz group reported an important step toward high-throughput flow battery testing by miniaturizing the flow batteries using a modular design.71"
        }
    ],
    "8.3. Outlook and Perspectives": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Energy storage technologies play a crucial role in achieving sustainability objectives. Generally, there is a strong industry effort on automation which guarantees robustness in production and product reliability. There is also strong academic research on ML for sustainable energy that expands beyond electrified energy storage to more general electrochemical sciences.720,721 It is necessary to combine efforts in both communities, i.e., automation and ML expertise, to supercharge the discovery of advanced energy storage materials. For instance, LIB manufacturing has been highly automated in the industry. However, there is still a large design space for battery material discovery and optimization and battery design improvements. There exist a number of opportunities to advance SDLs in the field of electrochemical energy storage. Integrating operando spectroscopic and electrochemical analysis of materials and devices, especially in flow reactors, will provide additional training data and a better understanding of failure mechanisms. As with optoelectronic materials and devices, simultaneously optimizing (co-designing) the different component materials in a device is an important potential contribution of SDLs. In most cases, RAMs for flow batteries are developed separately from the membrane or electrode materials. This could result in a mismatch of new RAMs and existing reactors, and thus subpar battery performance. Finally, SDLs capable of carrying out large scale or long term experiments on top candidates generated by another SDL could provide most realistic data about materials and devices for large, long-term energy storage systems. We also predict the explosive growth of SDLs due to the rapid development of ML, automation, and significant investments from both private and government-led initiatives. The latter includes campaign to automate the discovery of new batteries and flow battery materials, including the European Battery2030 initiative,722 the Department of Energy’s efforts at Argonne National Laboratory723 and Lawrence Berkeley National Laboratory,724 and Canada’s CA$200 million investment in the Acceleration Consortium.589 The U.S. government also recently announced a US$7 billion investment from the Department of Energy to build seven hubs of H2 infrastructure in the USA, which can significantly accelerate the implementation of SDLs for fuel-based energy storage."
        }
    ],
    "9. Conclusion and Outlook": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "The evolution of SDLs within chemical and materials science promises to usher in a transformative era of scientific exploration. In this review, we have provided a comprehensive overview of SDL systems, both past and present, for a variety of applications. Early autonomous systems have been enhanced by rapid development of better automated chemistry platforms, improved AI-based experimental planning algorithms, and the availability of large, high-quality datasets fueled by advancements in information technology and computational power. Many Level 3 and 4 SDLs have already demonstrated impact in accelerating reaction process optimization, functional property refinement, and novel chemical and materials discovery. Further development of both custom and general automation systems has also significantly reduced the barrier to entry. Progress towards next-generation SDLs signals a Chemical Reviews Review https://doi.org/10.1021/acs.chemrev.4c00055 Chem. Rev. 2024, 124, 9633−9732 9709 paradigm shift in which we believe SDLs will transition from systems designed and operated by specialists to everyday tools, similar to those brought about by the development of other now ubiquitous tools such as NMR, HPLC, MS, XRD, TEM, and SEM. However, there are also important potential challenges in the future of SDLs. Most immediately, many contemporary SDL systems are very complex and expensive. Realizing the full potential of SDLs requires a concerted effort from the scientific community to embrace open-source software and hardware, democratizing access to these technologies and fostering collaboration. Numerous challenges must be addressed, including the development of robust and user-friendly interfaces, the establishment of standardized protocols and data formats, and adherence to the FAIR principles for data sharing. Effective implementation of FAIR data practices is crucial in enabling researchers worldwide to leverage the wealth of information generated by SDLs, and promoting transparency and reproducibility in chemistry and materials science. Initiatives, such as the creation of low-cost SDL prototypes and educational programs, play a vital role in empowering future scientists to navigate and contribute to this evolving multidisciplinary field. The growing importance of ML, automation, and SDLs is also forcing us to rethink education and workforce development in the physical sciences, where curricula often remain unchanged from the late 20th century. We also envision independent non-profit organizations capable of developing the talent within the community, building the ecosystem for collaboration, and supporting a platform to include private industry in collaborations that respect proprietary concerns. As we continue development of SDLs, a key consideration lies in the role of human researchers in the scientific discovery process. As SDL technologies become more mature and widespread, the role of the researcher may shift toward translating the results from autonomous experimentation into scientific understanding,725 which may be coupled with advances in explainable AI.726−729 We foresee that human ingenuity will remain important in the discovery of new chemical and physical phenomena, novel classes of materials, and advanced laboratory techniques and technologies. Furthermore, while the Level 5 SDL is the pinnacle of autonomous experimentation, the human-in-the-loop Level 4 SDL will continue to be valuable, and perhaps even preferred, due to the adaptable and innovative nature of human problemsolving.730 For such diverse and multidisciplinary fields as chemistry and materials science, the flexibility and modularity provided by semi-autonomous systems will be vital to SDL development. Moreover, as the barrier to chemistry and materials discovery becomes lower, and the process becomes faster, the potential misuse of SDLs for malicious purposes underscores the societal responsibility of researchers, the need for ethical guidelines, and the promotion of responsible implementation in industry. Striking a balance of economic considerations, ethical standards, and societal welfare is imperative to ensure the constructive and beneficial deployment of SDLs. While the challenges are formidable, the potential benefits of a fully realized SDL ecosystem are substantial, and such an ecosystem will be the future of chemical discovery. By fostering a collaborative environment, promoting transparency, and aligning efforts towards a shared objective, the chemistry and materials science communities can accelerate the pace of scientific discovery, explore new frontiers of knowledge, and drive innovation in ways that were previously unattainable."
        }
    ],
    "A 2.1": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 8: Examples of types of automated hardware. The figure categorizes automated hardware for SDLs into three types (a-c) and supporting software (d-e)[cite: 121]. Examples include: (a) the OT-2 platform by OpenTrons (specialized hardware), (b) a robotic arm for chemical operations (general-purpose hardware), (c) the Sidekick liquid dispensing platform (open hardware), (d) a computer vision framework for glassware, and (e) solid weighing simulation software (digital twins)[cite: 121]."
        }
    ],
    "A 2.2": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 9: Summary of the software components of SDLs. A schematic diagram illustrating how the software components interface with the hardware and with each other[cite: 147]. The 'Orchestrator' sits at the center, communicating with 'Automated hardware', 'Public/Private databases', and the 'Exp. planner' (Experimental Planner) [cite: 140-147]. The workflow creates an autonomous closed-loop laboratory where experimental results are analyzed and fed back into the planner to generate new experimental recommendations[cite: 140, 147]."
        }
    ],
    "A 3.1": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 10: Autonomous optimization of chromatographic separation by varying the eluent composition. Top: Schematic visualization of the experimental setup used by Berridge, and structures of four analytes in a sample mixture. Bottom: Visualization of the simplex optimization performance upon variation of eluent composition and flow rate (left). Development of the chromatographic response function (CRF), the objective, throughout the course of the optimization campaign (middle); the optimized chromatogram (right)[cite: 224]."
        }
    ],
    "A 3.2": [
        {
            "paper_title": "Self-Driving Laboratories for Chemistry and Materials Science",
            "source_filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
            "content": "Figure 11: Purity optimization results in multi-step reaction-extraction process. Four parameters are varied: the temperature, solvent ratio (VR), the residence time, and the inlet pH. The size of the dots corresponds to the temperature, and the color represents the purity of N-benzyl-a-methylbenzylamine after the reaction. The star is the optimal purity obtained at experiment 53[cite: 245]."
        }
    ],
    "Performance metrics for autonomous labs": [
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "The features which can define the performance aspects of an SDL and are critical to report include specific information on the SDL's degree of autonomy, operational lifetime, accessible parameter spaces, precision, throughput, sampling cost, and optimization performance – as shown in Fig. 1 and Table 1.\n\nDegree of autonomy\n\nThe degree of autonomy can be defined by the context in which non-robotic experimentalists may interact with the experimental system. Shown in Fig. 2, this feature may be broken down into piecewise, semi-closed loop, closed-loop, or self-motivated operation modules. A piecewise system, which may also be referred to as an algorithm-guided study, has complete separation between platform and algorithm. In this context, a human scientist must collect and transfer experimental data to the experimental selection algorithm. Once the algorithm picks the next experimental conditions, a human researcher must then transfer these to the physical platform to test. This piecewise schema is the simplest to achieve as there is no need for in/online or in-situ measurements, automated data analysis, or programming for robotics interfacing. These systems are particularly useful in informatics-based studies, high-cost experiments, and systems with low operational lifetimes since a human scientist can manually filter out erroneous conditions and correct system issues as they arise. However, this strategy is typically impractical for studies that require dense data spaces, such as high dimensional Bayesian optimization (BO) or reinforcement learning (RL). Next in degree of autonomy are semi-closed-loop systems. In these systems, a human scientist must interfere with some steps in the process loop, but there is still direct communication between the physical platform and the experiment-selection algorithm. Typically, the researcher must either collect measurements after the experiment or reset some aspect of the experimental system before experimental studies can continue. This technique is most applicable to batch or parallel processing of experimental conditions, studies that require detailed offline measurement techniques, and high complexity systems that cannot conduct experiments continuously in series. These systems are generally more efficient than a piecewise strategy while still accommodating measurement techniques that are not amenable to inline integration. However, they are often ineffective in generating very large data sets. Then, there are closed-loop systems, which further improves the degree of autonomy. A closed-loop system requires no human interference to carry out experiments. The entirety of the experimental conduction, system resetting, data collection and analysis, and experiment-selection, are carried out without any human intervention or interfacing. These systems are typically challenging to create; however, they offer extremely high data generation rates and enable otherwise inaccessible data-greedy algorithms (such as RL and BO). Finally, at the highest level of autonomy, will be self-motivated experimental systems which are able to define and pursue novel scientific objectives without user direction. These platforms merge the capabilities of closed-loop tools while achieving autonomous identification of novel synthetic goals, thereby removing the influence of a human researcher. No platform to date has achieved this level of autonomy, but it represents the complete replacement of human guided scientific discovery.\n\nOperational lifetime\n\nIn conjunction with the degree of autonomy, it is also important to consider the operational lifetime of an SDL. Quantification of this value enables researchers to understand when platforms are suited to their data, labor, and platform generation budgets. Operational lifetime can be divided into four categories: demonstrated unassisted lifetime, demonstrated assisted lifetime, theoretical unassisted lifetime, and theoretical assisted lifetime. The distinction between theoretical and demonstrated lifetimes allows researchers to showcase the full potential of an SDL without misrepresenting the work that was carried out. For example, the operational lifetime of a microfluidic reactor is constrained to the volume of source chemicals provided as well as additional factors such as precursor degradation or reactor fouling. In practice, most microfluidic studies feature demonstrated lifetimes on the scale of hours. However, without source chemical limitations, many of these systems may reach functionally indefinite theoretical lifetimes. Even with these theoretical indefinite lifetimes, reporting demonstrated lifetimes and their context is critical to communicating the potential application of a platform. For example, demonstrated lifetime should be specified as the maximum achieved lifetime or, more importantly, the average demonstrated lifetime across trials. In addition, assisted and unassisted demonstrated lifetimes should be clarified to help identify labor requirements and therefore scalability of an SDL. For example, in recent work by the authors, a microdroplet reactor was used to conduct colloidal atomic layer deposition reactions over multiple cycles. One precursor used would degrade within two days of synthesis, and a fresh precursor was needed to be prepared once every two days. Beyond this limitation, the SDL could run continuously for one month without stopping or needing to be cleaned. In this study, the demonstrated unassisted lifetime is two days, and the demonstrated assisted lifetime is up to one month.\n\nThroughput\n\nLike operational lifetime, throughput is a critical component in specifying the capability of an automated system. Throughput is often referenced as the primary metric with which to compare technologies, as it is the most common bottleneck in achieving dense data spaces. As such, many techniques and fields distinguish themselves through this metric. However, throughput is often heavily dependent on the experimental system being studied as well as the technique being used to measure the material. For example, a platform can be highly efficient in conducting experiments, but if it is studying a synthesis with a long reaction time and does not have parallelization capability, the throughput is significantly throttled. Alternatively, if an experimental space includes a rapid reaction time, but the characterization method is too slow to sufficiently capture early time scales, then a large portion of the parameter space is neglected. Furthermore, if a characterization method is non-destructive, a single sample can generate multiple measurements, thereby enabling a significantly higher data generation rate. Consequently, the throughput is best reported as both theoretical and demonstrated values, which encompasses both the platform material preparation rate and the analyses. As an example, from work published by the authors, in a microfluidic rapid spectral sampling system presented previously, the platform could generate over 1,200 measurements per hour while running at maximum throughput, but for the longer reaction times studied, the actual sampling rate was closer to 100 measurements per hour. Therefore, this work showed a demonstrated throughput of 100 samples per hour and a theoretical throughput of 1,200 measurements per hour. The combination of these two values provides context on both the maximum potential limit and the actual stress tested limit.\n\nExperimental precision\n\nExperimental precision represents the unavoidable spread of data points around a \"ground truth\" mean value. Precision can be quantified by the standard deviation of replicates of a single condition, conducted in an unbiased manner. Recently, there has been increased focus on the significance of this metric in SDLs, particularly through the use of simulated experimentation through surrogate benchmarking. Surrogate benchmarking is used to evaluate algorithm performance on different parameter spaces without requiring operation of a full experimental system. Instead of conducting physical experiments, the algorithm samples from a simple function digitally, thereby significantly increasing the throughput and offering direct comparisons between algorithms through the evaluation of standardized, n-dimensional functions. Shown in Fig. 3, sampling precision has a significant impact on the rate at which a black-box optimization algorithm can navigate a parameter space, a finding that is supported by prior literature. In many cases, high data generation throughput cannot compensate for the effects of imprecise experiment conduction and sampling. Therefore, it is critical to develop SDL hardware that can generate both large and precise data sets. Characterization of the precision component is, therefore, critical for evaluating the efficacy of an experimental system. The ideal protocol for acquiring this metric is to conduct unbiased replicates of a single experimental condition set. There are many ways to conduct these replicates, and the exact methods for preventing bias will vary from system to system. However, the most common bias to avoid is through sequential sampling of the same conditions. As shown in prior literature, the test condition can be alternated with a random condition set before each replicate. This sampling strategy helps to position the test condition in an environment more similar to the setting used for optimization.\n\nMaterial usage\n\nWhen working with the number of experiments necessary for algorithm-guided research and navigation of large, complex parameter spaces, the quantity of materials used in each trial becomes a consideration. This consideration can be broken down into safety, monetary costs, and environmental impacts. Lower working volumes of hazardous materials in a platform means that critical failures can be more easily contained, which expands the parameter space of exploration to unforeseen results and a larger library of reaction candidates. Therefore, it is important to report the total active quantity of particularly hazardous materials. Furthermore, low material usage reduces the overall cost and environmental impacts of experimentation. For research involving expensive or environmentally harmful materials, it is important to quantify the impacts of the reaction system. As such, experimental costs should be reported in terms of usage of the total materials, high value materials, and environmentally hazardous materials. Total material and environmentally hazardous material generation should be reported with respect to the total quantities used, which includes waste stream materials generated through system washing and measurement references. It should be noted that many processes developed with microscale experimental systems are difficult to scale to functional quantities. Therefore, where applicable, it is important to provide data quantifying the scalability or generated knowledge of a developed process.\n\nAccessible parameter space\n\nBeyond the baseline characteristics associated with the quantity and quality of the data generated, another important consideration is the possible range of experimental parameters that can be accessed on both the inputs and outputs. Every experiment conduction strategy features its own limitations on the accessible parameter space, and each poses further limitations by the tools used to measure them. Liquid handling robots typically are limited from handling extremely low reaction times, and microfluidic reactors typically require solution phase precursors and are constrained to by injection ratios. Precise reporting of the demonstrated and theoretical parameter space along with details of the characterization techniques is critical for communicating the capabilities and limitations of an SDL. Each of the parameters used in a study should be reported alongside their minimum and maximum bounds and how they are parameterized in the optimization algorithms. Furthermore, considerable effort should be made to include qualitative constraints on the accessible list of parameters that may be used by an SDL.\n\nOptimization efficiency\n\nFinally, and likely most importantly, every SDL study should include a comprehensive evaluation of the overall system performance. Benchmarking with a real-world, experimental platform can be highly challenging, as there is often little data available for direct comparison, and it is typically too costly to conduct replicates with alternative systems or algorithms. Moreover, two seemingly similar experimental systems can feature reaction spaces of differing complexity, resulting in a more challenging optimization for one than the other. Shown in Fig. 4, many aspects of surface response features can influence the rate of optimization. With these limitations in mind, there are several aspects of a physical platform and the experiment-selection algorithm of SDLs that can serve as reasonable indicators of their performance. First, it is important to specify the optimized feature that was achieved because of the study along with the number of experiments or prior data implemented to reach that outcome. Where relevant, all champion results should be benchmarked with appropriate state-of-the-art literature. Next, the algorithm should be demonstrated to provide basic predictability across the studied data set. In model-driven algorithms, this can be provided through a simple regression validation by splitting all the available data into training and testing sets and predicting the outcome of unknown measurements. Furthermore, there should be a clear discussion of the dimensionality of the parameter space explored along with quantification of each parameter's degree of influence. With increasing interest in explainable AI, there are libraries of simple tools, such as Shapley plots, for quantifying the influence of each parameter on the system response. With model-driven algorithms, extracting these values is as simple as running the model through a prebuilt algorithm. Finally, when there are no apparent benchmarks for a given experimental space, random sampling can serve as a simple and clear standard. By comparing the performance of an experiment-selection algorithm to random sampling, the researcher can demonstrate control over the experimental space. Outside of serendipitous trials, the only way to achieve an experiment-selection algorithm that bypasses the performance of randomly selected conditions is to build a functioning autonomous platform with an effective guiding algorithm."
        }
    ],
    "Self-driving laboratories in literature": [
        {
            "paper_title": "Performance metrics to unleash the power of self-driving labs in chemistry and materials science",
            "source_filename": "Volk and Abolhasani - 2024 - Performance metrics to unleash the power of self-driving labs in chemistry and materials science.pdf",
            "content": "By clearly reporting the parameters detailed in this perspective, research can be guided towards more productive and promising technological areas. Early evaluation of these metrics under a sampling of recent SDL literature – detailed in Table 1 – leads to several technological indicators that can already affect decision-making in SDL studies. First, of the available technologies, microfluidic platforms have demonstrated unassisted generation of larger data sets and at a higher demonstrated throughput, as shown in Fig. 5. Among liquid handling tools, micro-well plate systems were at the top in performance. Second, there is a slight correlation between experimental cost and the total number of trials used to reach the optimum condition. Experimental systems that consume small quantities of materials can generate larger data sets and, therefore, apply more resources toward process optimization. Both indicators suggest that low material consumption technologies are the most effective in black-box optimization environments in the current state of SDL technology. However, these points should be taken with a major caveat. In much of the SDL literature mining performed for this perspective, data generation rates are largely limited by the reaction rates under study. Few SDL papers report system specifications beyond what is necessary for a case study experiment, but in studies that present an SDL as the core of the work, these parameters are just as important as the exact experiments that are conducted. Improved reporting and stress testing of SDLs would help to resolve this deficiency in the available data and direct further research into more effective and productive technologies.\n\nAdditionally, the sampled SDL literature, shown in supporting information Table S.1, does not show a clear correlation between the dimensionality of the studied parameter space and the number of trials required to reach an optimum. Some deviation in the required number of trials is expected, due to varying complexity of the response surfaces and the presence of non-contributing parameters. However, a correlation with dimensionality should be present, particularly when assuming real-world experimental systems tend to exhibit similar levels of complexity. This trend indicates that many of the prior works do not provide the global optimum of the studied experimental space. This is to be expected, as identifying when a global optimum has been reached is a fundamental and largely unsolvable challenge in the optimization of high-cost experimental spaces. With no clear, quantifiable indicator of a comprehensively explored and optimized space available, alternative metrics for demonstrating an SDL efficacy are necessary.\n\nAs previously discussed, it is critical to report SDL's algorithm performance features in formats that demonstrate predictive capabilities, feature analyses, and benchmarking, yet these parameters are not often included in the SDL literature. Among the seventeen surveyed studies shown in supporting information Table S.1, 23% included a real-world benchmarking of any kind, and 12% included simulated benchmarking, leaving 65% of the studies without any form of algorithm comparison. Additionally, only 62% of the thirteen studies that leverage a machine learning model demonstrated any form of model validation, and only 19% conducted any parameter analysis. Furthermore, 71% of the studies reported no data quantifying the precision of the automated experimental system of the built SDL. Finally, no quantitative information on the accessible parameter space was found in the selection of reported literature. With this absence of information on the basic performance metrics of SDLs, it is highly challenging to elucidate a clear direction for the field. A larger effort should be taken by researchers to ensure that these quantitative metrics are included."
        }
    ],
    "Result and discussion": [
        {
            "paper_title": "Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning",
            "source_filename": "Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf",
            "content": "System review: Figure 1 illustrates the overall architecture of the proposed Liquid Lens Microscope System utilizing DRLAF[cite: 1407]. As shown in Fig. 1a, we constructed a microscope system equipped with a liquid lens, which operates on the principle of electrowetting for zoom control[cite: 1408]. The host computer runs the proposed autofocus model, based on deep reinforcement learning, which is trained using the captured image dataset[cite: 1411]. The model is capable of autonomously determining the requisite focus adjustments based on the characteristics of the current image, thereby facilitating expeditious end-to-end autofocus[cite: 1412]. Figure 1b shows the training images from different samples, including micro/nano-structured surfaces with high-contrast regular patterns, resolution test targets with medium-contrast similar patterns, and laser-processed metal substrates with medium-contrast irregular patterns [cite: 1413].\n\nPreparation and performance of the liquid lenses microscope system: The microscope was equipped with a constructed liquid lens module, which was positioned at the diaphragm location[cite: 1469]. The module was driven by a voltage source via flexible electrodes, thereby enabling the host computer to regulate the voltage[cite: 1470, 1471]. Through investigating the influence of different cavity structures and dielectric materials and thicknesses on the performance of liquid lenses, a high-performance liquid lens was successfully fabricated[cite: 1481]. The lens utilizes a 5 µm thick Parylene C layer as the dielectric layer and a 180 nm Teflon AF layer as the hydrophobic coating[cite: 1482]. Figure 2d and e illustrates the response time of the constructed liquid lens and the relationship between its focal length and driving voltage under different magnification levels of the microscope, respectively[cite: 1483]. As can be observed from Fig. 2e, the focal length of the liquid lens varies at different objective lens magnifications under the same driving voltage[cite: 1484]. Therefore, for a microscope system integrated with such a liquid lens, solely relying on constructing a focal length-voltage model and performing autofocus based on this model requires substantial prior calibration work, and accuracy is difficult to guarantee[cite: 1485]. It is necessary to apply intelligent focusing algorithms in the liquid lens microscope system to achieve fast and precise end-to-end autofocus [cite: 1486].\n\nEffect of actions on autofocus performance: The autofocus adjustment actions include forward adjustments, backward adjustments, and stop actions[cite: 1488]. To determine the optimal action space size for autofocus, the performance of the model with different action space sizes was studied[cite: 1491]. Figure 3a shows the distribution of the influence of action space size on the focusing deviation of the autofocusing results[cite: 1492]. It can be observed from the figure that the size of the action space significantly affects the focusing deviation of autofocusing[cite: 1497]. As the number of actions increases, the deviation distribution tends to converge to 0 V, leading to a significant reduction in focusing deviation[cite: 1498]. Figure 3b shows the impact of the action space size on the success rate and accuracy of autofocusing[cite: 1502]. As the action space size increases, both the accuracy and success rate of autofocusing significantly improve[cite: 1503]. Figure 3c illustrates the influence of the action space size on the number of autofocusing time steps and accuracy on the test set[cite: 1504]. As the action space size increases, the average number of autofocusing time steps decreases significantly and stabilizes, while the average accuracy increases significantly[cite: 1505]. Given that an action set with 7 actions achieves better focusing performance, Table 1 compares the accuracy and time steps for different configurations of these 7 actions[cite: 1511]. Overall, the action sets determined by the logarithmic method with smaller bases are more suitable [cite: 1515].\n\nEffect of state random sampling in training on autofocus performance: The proposed method introduces a deep reinforcement learning approach trained by random sampling from multiple state datasets[cite: 1519, 1520]. This approach is aimed at enhancing the model's autofocusing success rate and improving its generalization across diverse samples[cite: 1575]. Figure 4 illustrates the impact of state dataset list size on the model's autofocusing performance[cite: 1580]. It is observed that with an increase in the number of state datasets, the focusing deviation significantly decreases, trending converge to 0 V[cite: 1583]. As the number of state datasets increases, both the accuracy and success rate of autofocusing notably improve[cite: 1584]. Additionally, the model's performance on the test sets gradually surpasses that on the training sets[cite: 1585]. With 50 datasets, the model achieves a 97.2% success rate on the test set, along with a root mean square error (RMSE) of 2.85 x 10^-3 V for the predicted voltage, indicating an improved ability to generalize[cite: 1586, 1593]. This trend may be attributed to the model learning more comprehensive and diverse information as the number of state datasets increases [cite: 1594].\n\nGeneralization experiments: To further evaluate the model's generalization capability on unknown samples, we conducted specific autofocusing experiments[cite: 1664]. Three methods are tested on completely unfamiliar sample datasets[cite: 1665]. Compared to Method 2 (trained on single sample) and Method 1 (deep learning), Method 3 (DRLAF trained by random sampling from different samples) demonstrates significantly higher autofocusing success rates on unfamiliar datasets[cite: 1670]. This result further validates the effectiveness of the random sampling training strategy in enhancing model generalization [cite: 1671].\n\nAblation experiments on the reward function: To thoroughly analyze the impact of the proposed hybrid reward function design and the contribution of each reward component to the algorithm's performance, a series of ablation experiments were conducted[cite: 1682]. Reward 5 (the proposed reward): Sharpness + Time Step + Stop + Additional Reward (Table 3)[cite: 1684]. The figures illustrate that during training, Reward 5 exhibits the best convergence across all three samples[cite: 1687]. The results presented in the figures indicate that Reward 5 is the most effective reward function formulation investigated, as it exhibits consistent convergence across different samples while achieving a significant reduction in the number of time steps required [cite: 1904].\n\nAutofocus experiment: Figure 7 illustrates the imaging performance of the proposed autofocus algorithm under different samples and random initial defocus states[cite: 1906]. Following autofocusing by the proposed method, the resulting images (Fig. 7g-l) demonstrate rapid adjustment to achieve well-focused images regardless of the initial defocus severity, with clear details and significantly improved corresponding evaluation metric values[cite: 1908, 1909]. To assess algorithm performance, we compare the autofocus results with manually focused results (Fig. 7m-r)[cite: 1913]. Although distinguishing between the two is difficult when observed directly, the precision of the algorithmic output images is comparable to, and in some instances exceeds, the level of manual focusing, as indicated by evaluation metrics[cite: 1914]. To provide a more comprehensive assessment of the proposed autofocus method, two additional approaches, namely the golden section search-based and the Fibonacci search-based autofocus methods, are implemented in the system[cite: 1917]. Experimental results demonstrate that the proposed model achieves autofocus in an average of only 3.15 steps, reducing the time by 79% compared to the golden section search-based method and by 60.63% compared to Fibonacci search-based methods[cite: 1920]."
        }
    ],
    "Materials and methods": [
        {
            "paper_title": "Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning",
            "source_filename": "Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf",
            "content": "Liquid lens: We implemented adjustable focusing functionality using a custom-made liquid lens based on the electrowetting-on-dielectric (EWOD) effect[cite: 1933]. As depicted in Fig. 1a, the liquid lens consists of an upper glass cover plate, a lower ITO cover plate, a chamber, a Parylene C dielectric film coated on the chamber, and a hydrophobic Teflon AF film[cite: 1934]. The Parylene C film, with a thickness of 5 μm, was prepared using chemical vapor deposition, while the 180 nm-thick Teflon film was fabricated via spin-coating[cite: 1935]. It is noteworthy that we designed the cavity with a conical structure featuring rounded corners [cite: 1936].\n\nDataset processing: In this work, we utilize the image sequences collected during the autofocus process of the liquid lens as the state input for the reinforcement learning agent, aiming to provide it with sufficient visual information to guide the learning of the autofocus strategy[cite: 1940]. The experimental methodology employs a microscope with a fixed object distance, equipped with a voltage-driven liquid lens[cite: 1941]. The voltage is adjusted in increments of 0.1 V, allowing for changes in focal length to capture the complete focusing process (defocused-focused-defocused) of the sample[cite: 1942]. After processing the images, a batch suitable for input into the DRLAF is obtained, forming a \"state\" dataset denoted as S[cite: 1943]. S is a four-dimensional tensor with dimensions 100 x 1 x 224 x 224, representing 100 frames of single-channel grayscale images with a resolution of 224 x 224 pixels[cite: 1943]. The study proposes a random sampling training method, where multiple state datasets are combined into a list L for training [cite: 1947].\n\nReward function: The reward function plays a crucial role in the reinforcement learning framework by defining task objectives, evaluating actions, shaping policies, and providing feedback[cite: 1956]. Therefore, for the autofocus task, it is necessary to consider multiple factors such as image sharpness, focusing time, and stopping strategy to provide clear behavioral feedback and learning goals for the agent[cite: 1957]. As a result, we design the following reward function involving image sharpness, time step reward, stopping reward, and an additional reward component [cite: 1958, 1963, 1968, 1970].\n\nAction space: In the proposed reinforcement learning autofocus model, we design the agent's executable actions to adjust the voltage applied to the liquid lens with different step sizes[cite: 1973]. To ensure that the number of actions satisfies certain mathematical constraints, we leverage the properties of the logarithmic function, allowing mapping the variations of the number of actions and the step size scaling coefficient to a small range of variations in the base b of the logarithmic function[cite: 1978]."
        }
    ]
}