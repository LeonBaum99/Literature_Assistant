{
  "filename": "Tom et al. - 2024 - Self-Driving Laboratories for Chemistry and Materials Science.pdf",
  "metadata": {
    "title": "Self-Driving Laboratories for Chemistry and Materials Science",
    "authors": [
      "Anqi Tom",
      "Yikun Chen",
      "Atharva Kelkar",
      "Bokai Zhang",
      "Yuchen Zhang",
      "Juner Zhang",
      "Zeqin Li",
      "Shuxuan Zeng",
      "Xiaolin Liu",
      "Junyu Liu",
      "Chonghuan Zhang",
      "Jielin Cui",
      "Zichao Yan",
      "Ekin D. Cubuk",
      "Edward H. Sargent",
      "Alán Aspuru-Guzik",
      "Yuqi Song"
    ],
    "arxiv_id": ""
  },
  "sections": {
    "Abstract": "Self-driving laboratories (SDLs) combine artificial intelligence (AI), robotics and high-throughput experimentation to accelerate scientific discovery. By closing the loop between experimental design, execution and analysis, SDLs can navigate vast chemical and materials spaces more efficiently than traditional methods. In this Review, we present a comprehensive overview of the field, starting with a historical perspective on the evolution of automation in chemistry. We propose a definition of SDLs and a classification system for their levels of autonomy. We then dissect the anatomy of an SDL, discussing the interplay between the 'brain' (software and AI agents) and the 'body' (hardware and robotics). We survey the current state of SDLs, highlighting successful implementations in organic synthesis, materials science and other domains. Finally, we address the technical and non-technical challenges facing the field, such as data standardization, interoperability and the need for educational reform, and offer a perspective on the future of autonomous discovery.",
    "1. Introduction": "In the face of pressing global challenges such as climate change, energy sustainability, and current or emerging healthcare crises, we must seek efficient solutions in the context of a growing global population and increasing resource demands. The accelerated development of materials, technology, and scientific understanding emerges as a potential avenue for tackling these challenges. Traditional research methods, often characterized by gradual progress with limited efficiency, may prove insufficient for the urgency these challenges demand. The integration of laboratory automation and data-driven decision making can potentially facilitate a more rapid and efficient exploration of solutions, while offering multiple advantages over traditional scientific discovery. Notably, automated experimentations can perform experiments faster and with higher precision, while data-driven search algorithms can quickly and efficiently explore experimental space based on feedback from available data (“closed-loop” experimentation). Additionally, issues such as reproducibility challenges and the underrepresentation of negative results in the scientific literature have been identified. At the same time, automation encourages the further digitization of research. The utilization of automated systems enables more precise documentation of experimental protocols, enhancing repeatability and reproducibility, while digitization facilitates data recording and sharing, with particular emphasis on the significance of negative or null results, contributing to a more comprehensive and accurate portrayal of scientific endeavors. High quality large datasets made possible by autonomous experimentation would aid in the development of artificial intelligence (AI) for materials science and chemistry, creating better machine learning (ML) and deep learning (DL) models, and enhancing the decision-making capabilities of data-driven algorithms.",
    "2. Infrastructure": "SDLs encompass three fundamental components: (i) automated laboratory devices proficient in executing complex chemical operations, (ii) software packages designed to seamlessly handle laboratory operations and the resulting data, and (iii) an experimental planner capable of processing acquired data and guiding subsequent laboratory procedures. In this section, we provide an overview of the essential constituents of SDLs and discuss ongoing efforts to harmonize their integration and control.",
    "2.1. Hardware": "Chemical experiments require different types of operations including chemical handling, reaction execution, post-reaction processing/purification, formulation, device fabrication, and chemical property measurements. For certain steps, taskspecific automated hardware systems have existed for decades, and many have been commercialized as standard laboratory instruments. Most of these systems have not been designed for fully automated workflows, but rather for interfacing with a human researcher. Prominent examples stem from the field of analytical chemistry, where automated solutions, often covering multi-step analytical workflows (e.g., chromatography-mass spectrometry from an autosampler), are routinely available in chemistry laboratories. The integration of such platforms with further automated solutions to enable SDLs presents a major challenge, and can be approached through different strategies. Fixed purpose-built automated systems couple multiple platforms in a static fashion, whereas partially automated workflows, requiring a human-in-the-loop, allow for platforms that can be adapted and repurposed for different experiments.39 Development in general-purpose robotic systems that can perform basic chemistry tasks and interface with the modules have allowed for completely automated yet modular SDLs.40 Moreover, open hardware for lab automation has been proposed to lower the financial barrier to building a SDL.41 In this subsection, we will review the various automated hardware modules, and provide a brief discussion on the development of general-purpose chemistry robotics platforms. (Figure 2) A summary of the distinctions between the hardware types are shown in Table 1",
    "2.1.1. Specialized Hardware": "The automation of laboratory operations has come a long way since the highthroughput catalyst screening campaigns performed by Bosch and Mittasch using continuous-flow platforms. As of today, chemistry and materials discovery laboratories host a range ofautomated solutions for routine tasks. At the same time, there are still many operations that are routinely performed by human researchers in traditional laboratories, due to the need for operational flexibility and adaptive decision-making.45 Finding automated solutions for these workflows requires interdisciplinary efforts within chemistry and engineering, prompting a re-conceptualization of central laboratory processes. At the core of most laboratory routines lies a set of fundamental operations that are essential across various types of experiments. These include, most importantly, the handling and transfer of materials (most often as liquids or solids), or the precise control of vessel or reactor conditions like temperature, atmosphere, and pressure. Whereas the latter have largely benefited from technological advances outside of chemistry, the challenge of automated reagent handling remains specific to the chemical laboratory. The most straightforward and widely applied solution to automated liquid dispensing is the use of syringe pumps or peristaltic pumps. Commercial solutions to these technologies are widespread, and have facilitated the transfer and dispensing liquids in numerous SDLs. The automation of positivedisplacement pipettes (PDPs) has also emerged as an alternative for robotic liquid dispensing, particularly in the context of biological experimentation. Gantry-based systems using PDPs (e.g. SPT LabTech Mosquito, or the OpenTrons OT systems), allow for substantial throughput increases of parallelized experiments in multi-well plates. Remarkably, the capabilities of PDPs to dispense microliter quantities have enabled the miniaturization of many experiments, particularly for biological assays. The downsides of PDP usage include a limited measurement range, along with challenges in handling e.g., highly viscous liquids and slurries. In contrast to liquid dispensing, dispensing powders or other types of solids presents a more significant challenge for laboratory automation. While automated liquid dispensing benefits from precise robotic volumetric displacement, automated solid dispensing requires real-time measurements of the dispensed quantities, making it more rare and costly. As a consequence, automated laboratories often resort to working with stock solutions of solid reagents when possible. In any SDL, these basic reagent handling operations are coupled to more problem-specific modules, including reaction execution (in environment-controlled reactors), separation and purification, or device fabrication. Given the large diversity between these modules, they will be discussed in the respective sections of this review. Eventually, the necessary characterization feedback to “close the loop” is provided by the diverse library of analytical instrumentation, which are already used in a (semi-)automated fashion in traditional laboratories, but require dedicated integration into SDL workflows. One popular solution is the static combination of individual modules into a continuous flow sequential workflow connected through tubing. As a particularly prominent example, this strategy has laid the foundation for the field of flow chemistry and microfluidics (as discussed in more detail in section Reaction Optimization). Owing to the simplicity of this hardware setup, it has also found applications in a series of higher level SDLs, as discussed throughout the course of this review. An alternative strategy to statically coupling individual modules is the idea of flexible automation.40 This approach emphasizes dynamic connections between modules using robotic systems for transfer between different workstations. This approach, imitating a human researcher operating the different modules, is particularly evident in the use of robotic arms, and, for example, has been used in foundational SDLs in drug discovery (Adam37 and Eve,38 see section Drug Discovery and Biochemistry), and thin-film material synthesis (Ada, see section Optoelectronics). The Chemputer by Cronin and coworkers connects different modules, including vessels, reactors, pumps and further specialized units, using selection valves, enabling a diverse range of automated synthetic chemistry workflows.46 Recently, Cooper and co-workers have extended the concept of flexible automation to the use of mobile robots, operating multiple workstations which are distributed across the laboratory, mimicking a human researcher.11 The idea of flexible automation has recently spurred commercial solutions, particularly from companies such as Chemspeed Technologies and Unchained Labs, based on gantry systems reminiscent of the HTE platforms discussed above. Despite higher costs, these solutions have garnered considerable interest in both industrial and academic SDLs",
    "2.1.2 General-Purpose Robot Applied for Chemistry": "While specialized chemistry hardware excels in conducting predefined experiments, their limited modularity can prove inconvenient for specific SDL configurations. Therefore, the application of general-purpose robotic arms for chemistry has been investigated due to their flexibility and multi-purpose nature. A well-known example of demonstration of generalpurpose hardware is the mobile robotic chemist by Burger et al. 11 (Figure 2b) In the study, they used a mobile robot arm, capable of moving around a traditional laboratory and operating various instruments, to search for optimal photocatalyst mixtures. They also demonstrated the reconfigurability of the setup, repurposing the system to perform solubility screening and crystallization.47 General-purpose robots have advantages over purpose-built flow platforms in that they can perform experiments that require physical interaction with tools and objects in the laboratory, thereby minimizing the reconfiguration and/or adaption of proprietary equipment or instruments designed for humans. However, major challenges in perception and decision-making limit the robust deployment of general-purpose robotic systems for flexible lab automation. For this reason, many works in the literature address lab automation for specific tasks�for example, mechanical tasks such as retrieving samples of crystals by scraping the wall of a vial48 and grinding powder with a soft jig.49 Pouring liquid using visual feedback50 and weight feedback51 have been studied as an alternative method of transferring liquid. Custom hardware built to assist robots in handling liquids have also been proposed, for example, Lim et al. used a custom syringe pump operated by a robot arm to conduct a molecule synthesis experiment.52 Knobbe et al. developed a robotic finger for operating electronic pipettes,53 and Zhang et al. used a designed end-effector for operating manual pipettes. Solid dispensing has also been demonstrated using a dual-arm robotic manipulator.54 Yoshikawa et al. demonstrated the use of a robotic arm for the more specific task of polishing electrodes used in electrochemical experiments.55 Nevertheless, besides the advantages of generality, multi-purpose robotic arm systems are lower in efficiency and hard to",
    "2.1.3. Open Hardware for Lab Automation": " The cost of hardware automation is a limiting factor for SDLs. As a means of lowering the hardware cost and crowd-sourcing development and testing, various open hardware for lab equipment has been proposed.56 Users typically print the published design files with their own 3D printer and build the equipment. In addition to labware for human use, lab automation devices such as liquid handlers have been developed as open hardware. FINDUS57 is an open-source liquid handling workstation that costs less than US$400. OTTO58 demonstrated qPCR with a 3D printed liquid handler. Both systems benefit from readily accessible parts and sensors for error checking, though space efficiency and generalizability remain as challenges. PHIL59 is a personal pipetting robot that is compatible with microscopes, making it ideal for live cell studies but implementation in chemistry is limited. EvoBot60 is a reconfigurable liquid handling robot that improved modularity by introducing layers and modules. Building upon a well-established 3D printer technology, it is easy to implement, but the fixed tool design makes it challenging for complicated tasks. Jubilee61 is an open-source multi-tool gantry-style motion platform also based on 3D printing technology, which has been used to demonstrate liquid-handling tasks for synthesis of nanoparticles (NPs).62 It can mount/dismount tools automatically to perform multiple tasks, while community contributions are needed to develop more tools for chemistry applications. Sidekick43 is a liquid dispenser that features an armature-based motion system with a fully 3D-printed chassis and home-built syringe-pumps to realize lower costs, and can handle only a limited number of liquid identities at a time. 3D printers have been utilized for producing microfluidic devices63 or building a pipette for a two-finger robot hand to enable accurate liquid handling.64 Open hardware is beneficial to lowering the cost of building SDLs and their customizability is helpful in meeting individual requirements in different experimental settings which are not met by existing commercial hardware.65 However, the technical difficulty of setting up open hardware and the wide variety of similar hardware proposals hinder widespread adoption in laboratories other than the developers of the hardware. Further support by user communities is needed in facilitating the adaptation, and efforts in growing user communities have been made using online communication platforms.",
    "2.1.4. Perception and Computer Vision": "Execution of chemistry experiments autonomously requires several layers of feedback. Mimicking the visual feedback of a chemist’s eyes, a perception system should track the progress of the chemistry experiment and provide information to the robot such that it can achieve the high-level goal or direction of a given experiment. Computer vision can play a key role in this aspect. For example, HeinSight can provide perceptual information about the chemistry experiments.14,67,68 Connected to an experiment planning algorithm, that information could be used to guide the robot throughout the experiment. More recently, Sun et al. presented a vision-guided liquidliquid extraction platform, using image processing and computer vision to identify phase boundaries.69 In another example, authors used visual feedback to train a 3D-CNN model for viscosity estimation of fluids.70 At a lower level, the robot also requires visual and kinesthetic perceptual feedback in order to perform manipulation tasks successfully and robustly. Robots need to be equipped with accurate perception skills to work in unconstrained open workspaces. One of the characteristics in a chemistry laboratory is the use of transparent objects, such as glass containers. Transparent objects have different optical properties from opaque objects that make object detection challenging. Transparent glassware detection algorithms using depth completion13 and multiple images71 have been proposed. Public datasets, such as VectorLabPics dataset,12 have been published in order to accelerate the development of ML models for laboratory related computer vision",
    "2.1.5. Manipulation Skill Learning and Digital Twin" :"In SDLs, general-purpose robots can interact with tools, objects, and materials within the workspace and require a repertoire of many laboratory skills. Those tools and objects can be in different forms, for example rigid objects like glassware, articulated objects with joints like cabinet doors, or soft objects like rubber tubes or powders and liquids. Some skills can be completed with existing heterogeneous instruments and sensors in chemistry laboratories, such as scales, stir plates, and heating instruments. Other skills are currently done either manually by humans in the lab or with expensive specialized instruments. In an SDL, robots should acquire those skills by effectively using different sensory inputs to compute appropriate robot commands. To effectively endow the robots with many skills in a scalable fashion, one approach would be to allow robots to “learn” those skills in a digital twin, a simulated laboratory environment in which the robotic system can interact with, using AI techniques.72 Digital twins can also be used for testing the workflows, algorithms, and scale-up developments.73 For example, ChemGymRL74 was proposed as an interactive framework for reinforcement learning in chemistry. Some examples of physics-based simulators include Gazebo,75 MuJoCo,76 and NVIDIA Isaac Sim.77,78 Recently, NVIDIA Isaac Lab,77 a modular framework on top of Isaac Sim, was introduced to simplify common workflows for robot learning that is pivotal for robot foundation model training. Some examples of robotic simulation environments and benchmarks are iGibson,79 MetaWorld,80 and BEHAVIOR1K.81 Closer to tasks related to laboratory automation, RB2 proposed a robotics simulation benchmark with pouring, scooping, and insertion tasks.82 In another work, a differentiable environment FluidLab83 was proposed for simulating complex fluid manipulation tasks. An example of using digital twin for the SDL is provided in Vescovi et al., where simulated environments have been used to visualize and compare tools, verifying the laboratory operations.74 These simulators leverage different physics engines, such as Bullet,84 FleX85 and PhysX86 and rendering happens via OpenGL87 or Unity.88 Although robot actions can be trained in a simulation environment at low cost, there are gaps between simulation and real-world settings. Multiple efforts have been made to close this sim-to-real gap,89 including for chemistry laboratory robotics. For example, Kadokawa et al. have trained a powder weighing action in a simulator and realized precise weighing in the real world.44 Nevertheless, high-fidelity and performant simulation of deformable objects and particle systems (such as fluids and powders), as well as the simulation of chemical phenomena in the context of SDLs, remain areas for future exploration.",
    "2.1.6. Robotics Safety in Laboratories": "In chemistry labs, several types of safety risks put humans and the environment in danger, including mechanical, electrical, and chemical hazards. Therefore, multiple levels of regulations and guidelines are implemented in chemistry labs for safety and accidents.90,91 The presence of robotic systems in chemistry labs can affect the risk of accidents in several ways, necessitating a diligent focus on safety and risk management. Generally, automated experiments with robotic systems inherently create a safer workplace for humans, as the users are less exposed to hazardous materials. Even when autonomous experimentation is not possible, chemists can tele-operate robotic systems to perform experiments with hazardous materials�which has been pioneered, for example, in the handling of radioactive materials,92 or explosive compounds.93 However, particular attention should be given when humans are in proximity to robots or in the same lab space, especially when employing mobile robots for tasks such as sample transfer. The choice of robotic systems, whether collaborative or industrial, can affect the safety protocols. For example, when using industrial robots, safety fences or laser scanners are commonly used in the robot workspace. Ensuring human safety in shared spaces requires a comprehensive approach that encompasses both physical and psychological aspects. For physical safety, the literature advocates employing control and motion planning techniques to facilitate safe physical interactions and address pre- and post-collision scenarios. In the realm of psychological safety, considerations such as robot motion, speed, adaptability, and appearance play pivotal roles in reducing stress and fostering a sense of safety in humanrobot interactions. More information about robotics safety standards can be found in ISO 1021882, ISO/TS 1506683, and survey papers by Lasota et al. 80 and Zacharaki et al. 81 Additional safety issues may arise from the manipulation and perception capabilities of robotic systems, as these remain open problems in the community. When deploying such robotic systems in chemistry labs, if manipulation policies and the robot's decision-making abilities are not robust enough, it may lead to failures, increasing the risk of accidents. An approach to rectify this shortcoming is to consider constraints on the robot policies. For example, Yoshikawa et al. 94 used constrained motion planning when transferring liquids with a robotic arm to reduce the risk of spillage. Moreover, the ability of robots to detect accidents, take immediate actions, and notify humans are other important considerations. Overall, safety in laboratories with robotic systems is a multifaceted challenge that requires additional research at several levels, from generating potentially hazardous chemicals to experimental planning and automated experiments where robots are used.",
    "2.2. Software": "The software component of an SDL is composed of three distinct parts, which are executed by some orchestration software (Figure 3): (1) the control and communication system of the automated hardware of the laboratory, (2) the data extraction, management, and analysis of experimental output, and (3) the decision-making experimental planner. In recent years, the fields of chemistry and materials sciences have undergone a paradigm shift with the rise of AI. ML algorithms, particularly DL models, have proven to be indispensable tools in deciphering complex patterns, predicting chemical properties, and accelerating the design of novel materials with tailored properties.",
    "2.2.1 Orchestration": "The true potential of individual automated devices in chemistry is most evident when they are interconnected in order to orchestrate some comprehensive chemical tasks. Consider, for instance, the typical course of a chemical analysis, which involves a sequence of actions, including compound synthesis, thorough characterization, and meticulous processing of resulting raw data. The intricate interplay between these sequential steps underscores the indispensable need for standardized protocols to ensure the coherence and reproducibility of chemical experiments. Traditionally, these protocols were conveyed through research articles and manually executed by chemists. However, contemporary practices enable the translation of these protocols into orchestrated workflows executed by computational software, signifying a pivotal departure from historical methods. The integration of automated workflows has flourished within the field of computational chemistry, where the automation of repetitive and error-prone tasks is straightforward due to the programmatic nature of the field. This is evident in the emergence of tools such as AiiDA,95 Fireworks96 and Snakemake97 among others,98−107 which excel in constructing and managing software workflows for ab initio simulations. On the experimental side, autonomous chemical laboratories constitute an emerging field where the adoption of such orchestration techniques has been hampered by the physical challenges inherent to wet laboratories, the lack of an orchestration standard, and the scarcity of resources for automated instrumentation.108 Thus, it is imperative to acknowledge that the development of orchestration software for SDLs faces numerous challenges rooted in the methods used by the majority of researchers. These challenges include: • The absence of standardized application programming interfaces (APIs) provided by instrument manufacturers, often necessitating the development and utilization of workarounds that place a substantial burden on researchers; • The inherent software complexity of managing and orchestrating the transporting of items between chemicals processing stations;41 • Limited exposure to programming in current chemistry and materials science curricula. Addressing these challenges requires collective efforts from various stakeholders involved in current research, implementation, and deployment of SDLs. The widespread adoption of SDLs, particularly in industry, will compel manufacturers to provide user-friendly SDL solutions, efficient transfer systems between devices, and graphical user interfaces (GUIs) that facilitate the seamless integration of these platforms into the workflows of chemists. Numerous tools have surfaced in recent years to bridge this gap, with initiatives like the SiLA2 standard,109 a communication protocol aiming to replicate a robot operating system (ROS) and adapt it for chemical devices. Within this context, various in-house orchestrators have emerged in different laboratories across diverse chemical fields, with notable examples including ChemOS,110,111 Helao,112 and AresOS,113 among others.47,74,114−121 These experimental orchestration platforms have achieved significant advancements in key orchestration features that are standard in computationallyoriented platforms, such as queue management, logging capabilities, data handling, and, more recently, the implementation of asynchronous execution of laboratory tasks and their integration with computational frameworks.122 However, a lack of consensus between these platforms still prevails, and they often remain tailored to specific laboratories, lacking the required level of generalizability to cater to the diverse spectrum of SDLs.",
    "2.2.2. Communication and Protocol Management for SDLs": "In the context of SDLs, where human researchers are the intended users, effective communication between researchers and the orchestration manager is paramount. This communication enables researchers to issue complex commands to the orchestrator that will be transformed into chemical operations, while receiving feedback and real-time updates on the status of laboratory processes in a readable format. In this regard, programming languages serve as the essential communication bridge, allowing users to convey instructions and request information from the orchestrator. Although general programming languages are frequently used to program chemistry hardware,123 for example the MOCCA124 open-source Python package which directly analyzes HPLC raw data and extracts relevant information, or Chemspyd125 open-source Python software for communication with proprietary Chemspeed software, specialized programming languages are proposed to efficiently describe chemistry experiments. Chemical Description Language (χDL)126 is an XML-based language used to describe chemistry experimental procedures, which was demonstrated by translating chemistry literature into χDL, and then synthesizing the described molecules. Chemical Markdown Language127 is another chemistry domain specific language to describe or assist in experimental documentation. While such languages are more tailed for communicating chemistry specific tasks, they will require a low learning barrier to ensure adoption in other SDLs. In recent years, there have been multiple examples of asynchronous workflows, in which SDLs operating in separate regions, with different research teams and equipment, work on the same discovery or optimization task. This requires extensive software infrastructure for the communication and coordination of results between the SDLs and the respective research teams. Multiple studies have demonstrated the use of internet cloud servers to manage and control distributed laboratory equipment.116,128 Decentralized databases can then allow for communication of experimental protocols, experimental results, and coordinated experiment planning over multiple laboratories, which have been demonstrated in some SDL orchestration softwares.110,111,129 Dynamic knowledge graphs have been proposed130 and demonstrated131,132 as an effective way of coordinating distributed SDLs. Ontologies are developed to capture various aspects of chemical research, including reactions, design of experiments, and hardware setups. Software agents are deployed at each lab site and act as executable knowledge components that can query, update, and restructure the knowledge graph autonomously, as the campaign progresses. Given the recent rapid developments in their capabilities, large language models (LLMs) have been investigated to accelerate chemistry research.133−136 In terms of communication, LLMs are able to interface with human users through text and conversation, translating between natural and machine language. For example, Boiko et al. demonstrated that LLMs can design and perform chemical experiments with a liquid handler based on natural language input from a user.137 The ability of LLMs can be expanded for specialized use cases by collaborating with external programs. ChemCrow138 is an LLM specially designed for chemical tasks, being able to observe, plan, and execute actions with integrated chemistry tools. CLAIRify139 introduced an iterative prompting strategy using automated verifiers to generate χDL, and demonstrated chemical experiments with a general-purpose robot. Likewise, ORGANA140 is an experimental planner that uses a LLM to communicate with chemists, and then plans and interfaces with a robotic arm to perform parallel tasks in an SDL experiment. The role of LLMs is discussed further in the subsequent sections.",
    "2.2.3. Data Management": "Automation accelerates data generation, and the large datasets must be managed efficiently in order to process and disseminate the generated data, particularly for the downstream use in data-driven techniques such as ML. Data management can be categorized into private databases, adept at housing all laboratory-generated data, and public databases designed to share curated and processed data for widespread use. Individual research laboratories often use private databases to facilitate record-keeping of chemical processes within the laboratory, and track chemical inventory and equipment availability. Traditionally, researchers have relied on laboratory notebooks and inventory software for these purposes, manually recording and annotating changes in experimental procedures. Annotated data would then be transferred for curation and processing, although inconsistent information tracking, and missing or biased data due to human error remain as issues. However, improvements in information technologies have changed data collection practices, with electronic laboratory notebooks emerging as modern alternatives to traditional notebooks.141,142 Efforts have been made in integrating private databases with SDL orchestration frameworks to keep track of the status of the laboratory47,110−112 or the status of simulations.95 However, it's worth noting that the adoption of these tools is not standardized across the chemistry community. Conversely, public databases play an indispensable role in the open science paradigm, adhering to the FAIR (findable, accessible, interoperable, and reusable) data principles143 by providing transparent access to experimental data for other scientists, thereby enhancing reproducibility. Computational chemists hold a long tradition of publishing standalone computational databases hosted on cloud platforms like Zenodo.144 These encompass a broad spectrum of materials, including MOFs, organic molecules, and heterogeneous catalysis. More advanced platforms such as the Harvard Clean Energy Project,145 IoChem-BD,146 Materials Project,147 NOMAD,148 The Protein Databank,149 Materials Cloud,150,151 Open Quantum Materials Database (OQMD)152 and Catalysis-Hub153 serve as noteworthy examples of public materials databases. These are typically built on generalpurpose database frameworks; for example, Materials Project147 uses MongoDB and OQMD152 uses SQL. The variety of public materials databases for computational data typically provide supplementary tools for data parsing, querying, and publishing. However, in the realm of experimental chemistry, there is a lack of tradition in publishing chemical results in structured and open databases; reaction and characterization data are commonly published as standalone datasets or in commercial databases. Notable examples include the Spectral Database for Organic Compounds,154 Reaxys,155 SpectraBase,156 SciFinder,157 and the chemical reaction patents from the United States Patent and Trademark Office.158 However, substantial efforts have been made to establish dedicated databases for storing experimental reactions and characterization data, with platforms such as Pubchem,159 Open Reaction Database,160 GNPS,161 Mass Bank of North America, 162 Crystallography Open Database, 163 MNRShiftDB164 and Molar.165 Due to the automation capabilities of SDLs, these databases are poised to play a critical role in the expansion of SDLs as they serve as a common interface bridging diverse research laboratories, facilitating seamless collaboration and data sharing among geographically dispersed research teams. Before concluding this subsection, it is worth mentioning the recent emergence of HuggingFace Hub,166 introducing an open database focused on collecting datasets and ML models. While the effort is currently focused on DL research, the future standardization of SDLs will likely require the adoption of similar solutions. This will enable laboratories to share components of research workflows more effectively. For instance, a synthesis, characterization, and ab initio simulation workflow could be assembled for an SDL setup by downloading independent parts from the repository, connecting them, and subsequently customizing them to align with the specific laboratory needs. This collaborative approach facilitates the sharing and improvement of research components among different laboratories, fostering innovation and efficiency within the scientific community, as has been demonstrated for the AI community.",
    "2.2.4. Role of Artificial Intelligence in Cheminal Discovery": " As large datasets of experimental and computational chemical data became accessible, data-driven statistical methods became more relevant to chemical discovery.167 Cheminformatics have been developed since the 1960s, particularly driven by advances in computing technology, and the development of ab initio techniques such as density functional theory (DFT).168 Early work focused on prediction of chemical properties, identifying quantitative structureactivity relationships (QSAR), for virtual screening of large libraries of pharmaceutical compounds.169,170 Statistical analysis of feature importance, such as through Shapley additive explanation (SHAP) values,171 have been used to provide intuition into the effect of certain chemical structures, properties, or experimental parameters in the model performance.172 ML methods such as (1) tree-based methods: random forests (RF)173 or gradient-boosted trees;174 (2) kernel-based methods:175 Gaussian process (GP)176 or support vector machine (SVM);177,178 and (3) clustering algorithms: knearest neighbors (kNN)179 or k-means clustering,180 were used to capture complex QSAR in chemical and material space. Chemical compounds can be described by machine-readable chemical descriptors (Figure 4), represented by vectors of physicochemical descriptors,181,182 unique fingerprints183,184 (e.g. extended-connectivity, path-based fingerprints),185,186 graph representations,187 and structured strings (e.g. SMILES,188 SELFIES,189 and group SELFIES190).191,192 More complex forms of chemical representations include 3D information, such as through Z-matrix or cartesian XYZ coordinates.149 Additionally, chemical transformations can be represented as SMIRKS193 and SMARTS,194 which extend beyond SMILES to facilitate the textual representation of chemical reactions, while graph encoding has emerged as a powerful approach for capturing the complexity of reaction networks.195−198 More recently, DL methods using neural networks have had successes in chemical applications, with the downside of sacrificing interpretability and requiring large amounts of training data.199,200 Neural networks are highly expressive nonlinear models that can be fit to complex data through backpropagation, capturing complex relationships in highdimensional input data. DL methods are now state-of-the-art for many chemical prediction and classification tasks, for example, graph neural networks (GNN) on molecular chemical data.201−204 Additionally, successes in natural language processing have led to LLMs which are able to extract meaning and context from natural language, and generate coherent responses.205 Molecular language of string representations have been incorporated with language models for property prediction.206 Various applications of language models to SDLs include allowing for orchestrator-to-human interactions through language, or translating natural language to robotic commands.139 Language models have also been used to gather data from the scientific literature, generating datasets in an automated way.207,208 Additionally, DL allows for data-driven generative modeling and ideation, reaching category 3 in software automation (Figure 1). Generative models incorporating neural networks have been used to generate novel chemical compounds and materials without human intervention, through the use of architectures like variational autoencoders (VAEs),209,210 generative-adversarial networks (GANs),211,212 gradient flow (i.e., diffusion) models,213,214 deep genetic algorithms,215,216 language models for chemical strings,217,218 and deep reinforcement learning (RL).219−221 By directly learning the chemical space of a dataset, the model can interpolate and extrapolate new compounds, and even directly optimize within the latent space through inverse design.222 Various in silico campaigns in generative inverse design and benchmarking have already been demonstrated,217,218,223−225 but issues with synthesizability and chemical stability of generative compounds remain a barrier to automated empirical validation. DL methods are capable of transfer learning, a technique commonly used in low-data settings, in which the model is pretrained with more readily available data that provides the model with implicit information about the main task.221,226,227 By leveraging the libraries of computational results, and historical empirical results, models can be preconditioned with physicochemical information for SDL campaigns that typically start in the low-data regime. Such models can even be used to encode chemical compounds as task-specific descriptors, compressing the chemical information into expressive abstract representations.187,228 Both traditional ML and DL techniques are now commonly used as part of optimization algorithms and experimental planners, which will be discussed in the next section",
    "2.2.5 Experiment Planning":"The availability of data, coupled with the robust ML and DL models mentioned earlier, has created a demand for tools adept at processing the datasets. For practical examples, we direct the reader to various reviews that illustrate the utility of these techniques.199,229−235 Traditionally, brute force methods like combinatorial gridsearch and random sampling236,237 have been combined with high-throughput techniques to sample systems of interest. For instance, the Haber-Bosch process,238 which involved testing up to 4000 catalysts in 6500 experiments239 to identify suitable catalysts and experimental conditions for ammonia synthesis, required significant human effort and resources to complete. While such methods may work well and be preferable when dealing with a small number of parameters and low experimental costs, they quickly become unfeasible as the number of variables increases. In such cases, a methodical approach in the experimental space becomes necessary, particularly when computational or experimentation costs are a concern. Similarly, to make use of the improved precision of modern chemical apparatuses and sample preparation devices, exploring continuous variables in finer increments necessitates an increased number of experiments. Conventionally, scientists and engineers have used DoE strategies to systematically scan the experimental space, in an effort to reach the optimum, and identify the important parameters.240−243 A naive approach may be the one-factor-ata-time (OFAT) design, which involves manipulating a single parameter, assuming there are no correlated effects between the factors. In response surface methods such as Box-Wilson central composite design (CCD) and Box-Behnken design, the experiment list is populated by equally spaced points in design space, followed by polynomial fitting of the parameters to the response variable(s) for creating a response surface. This response surface can be then used for finding the optimum while the fitting parameters can be evaluated through statistical tests such as t-test,244 or analysis of variance (ANOVA)245,246 to assess the relative importance of variables as well as to confirm validity of the strategy. While amenable to the computation power available at the time, a disadvantage of DoE methods is the rigidity of the list of experiments, which remains the same as the results are collected. Furthermore, the equal spacing of selected points gives a diverse yet course-grained sample of design space, sacrificing precision in identifying the optimum. As the dimensionality of design space increases, DoE strategies become impractical, and likely insufficient. Especially when targeting software autonomy levels 2 and 3, advanced experiment planning algorithms must be capable of realizing closed-loop workflows. Such iterative global optimization algorithms must fulfill the following requirements: (1) the algorithm must take into account experimental observations from previous iterations, and use this knowledge to make more informed experimental recommendations; (2) as experiments are generally expensive and time consuming, optimization should proceed with the minimum number of required experiments; (3) the algorithm must treat the underlying response surface as a black-box the functional form of the optimization surface, or any gradient information, is usually not available from experiment. Some early approaches to the black-box optimization challenges in chemical and material domains is mimicking the successful strategies observed in biology, such as the evolutionary algorithms or genetic algorithms (GA) inspired by natural selection.247,248 In the context of experiment planning, each experimental setting refers to an individual species in a population. The fitness of each individual, which is associated with the quantities the algorithm is maximizing, is then used to assign a chance of producing offspring via crossover operations with other high-fitness species. With each generation, the population evolves to a greater fitness while random variations can be introduced through mutations that help prevent the GA from getting stuck in local minima. Similarly, particle swarm optimization (PSO) is an optimization tool inspired by flying flocks of birds where each particle represents a point in experimental design space and the velocities of the particles are analogous to update rates of the experiment parameters.249−251 The covariance matrix adaptation-evolution strategy (CMA-ES) is another evolutionary strategy, in which the species of a generation are selected by a probability distribution based on high-fitness individuals.252 Many of today’s pressing challenges such as catalyst discovery for sustainable energy applications, drug discovery, and synthesis optimization are analogous to finding a needle in the haystack, where only some narrow areas of the experimental space are highly promising and require detailed exploration. While GAs and PSO have proven suitable for complex problems, such methods tend to require a large number of samples as the design space grows. Simplex optimization, notably the modified simplex,253,254 has been another common heuristic optimization strategy that is also relatively simple and straightforward. Without any mathematical assumptions, simplex optimization gradually and systematically alters a virtual simplex constructed by vertices of previous experiments within the experimental design space, shrinking and expanding towards optimal settings with each subsequent evaluation. Despite multiple successful applications in chemistry255−263 since its first use in analytical chemistry27, simplex optimization’s final performance may be hindered by local optimality traps, noisy data and large number of variables. Another common systematic approach for experiment planning is Stable Noisy Optimization by Branch and Fit (SNOBFIT)35,264−266 which uses branching for exploration and bounding for elimination of irrelevant parts of experiment design space. SNOBFIT further aims to improve optimization efficiency by including local searches and deals with noisy data via robust sampling combined with statistical modeling of the noise. Finally, gradient-based numerical methods for optimization have also been used in chemical process optimization, such as the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm.267,268 These algorithms require more computational power, but typically converge faster than non-gradient methods, like SNOBFIT or simplex optimization.269 We provide a comparison of some common experimental planning algorithms in Table 2. There are a number of aspects to consider when designing or choosing experimental planning algorithms in the context of SDLs. For example, SDLs in chemistry often involve the simultaneous optimization of continuous, discrete (i.e., ordinal discrete parameters), and categorical parameters. For example, the optimization of a reaction may involve parameters which include continuous variation of temperature, discrete increments of reaction times, and categorical selection of reagents. In scenarios where there are small amounts of data, or when experiments are too expensive to perform, multi-fidelity optimization may be used to accelerate optimization. Multifidelity learning leverages information from both low-fidelity (cheaper, faster, but less accurate), such as ab initio calculations, and high-fidelity (expensive, slower, but more accurate) data sources to guide the search for optimal conditions. This can be done, for example, through transfer learning with DL methods,270−273 or delta-learning optimization.274−276 And in many automated experimental platforms, batch experimentation is typically used to parallelize synthesis or characterization, such as through the use of well plates. Experimental planners need to be able to optimally suggest batches of solutions while balancing exploration and exploitation of the search space. This is also known as the multi-armed bandit problem.277−279 Multi-objective optimization is particularly important in the optimization of chemical processes, and the design of new materials or molecules�for example, maximizing the activity of drug molecules, while targeting a specific solubility. Scalarizing functions are one of the simplest ways to incorporate multiple objectives into a single objective, with the simplest function being a weighted average of the various targets. Other more sophisticated scalarizers have been proposed, such as Chimera,280 which allows for user-specified hierarchical optimization of each objective, or the Pareto front hypervolume. The Pareto front represents the set of solutions where no single objective can be improved without degrading at least one other objective. Maximizing the hypervolume of the Pareto front yields a set of solutions that dominates a larger portion of the objective space, indicating better overall performance across all objectives and providing decisionmakers with a diverse range of trade-off solutions. Some such methods include: parallel efficient global optimization (ParEGO),281 which uses the Chebyshev scalarization; nondominant sorting genetic algorithm (NSGA)-II,282 and NSGAIII,283 which are GA approaches that optimizes by considering crowding distances of solutions within the Pareto front; smetric evolutionary multi-objective optimization algorithm (SMS-EMOA),284 which directly optimizes the hypervolume indicator. For more detailed discussion, we refer readers to Sharma and Kumar,285 Rangaiah et al., 286 Vel et al, 287 and Angelo et al. 288. ",
    "2.2.6. Bayesian Optimization and Active Learning":"One of the most common strategies today for SDL experimental planning is Bayesian optimization (BO), which aims to maximize or minimize some black-box function, such as a measurable chemical property, as a function of controllable experimental parameters.289,290 To do this, a surrogate model with some prior distribution is fit, or trained, on the available data, and the predicted posterior distribution is used to create an acquisition function. The acquisition function contains information about the prediction and the uncertainty of the prediction based on the posterior distribution, and can be used to control the exploitative or explorative nature of the subsequent experiments. Some common acquisition functions include the upper confidence bound (UCB), a linear combination of the mean and standard deviation of the posterior, and the expected improvement (EI), the expectation value of the next point that most improves upon the best value. In multi-objective optimization tasks, some commonly used acquisition functions include the expected hypervolume improvement (EHVI),291 the noisy EHVI (NEHVI),292 or ParEGO. BO algorithms have since incorporated more complex nonlinear models using ML and DL models as surrogates, and have found success in more difficult chemical optimization tasks. Similarly, active learning is a sequential optimization algorithm, however the goal is to improve the performance of the surrogate model with each additional data point. The goal of both algorithms is to optimize to the respective goals in as few evaluations as possible, minimizing the number of expensive black-box function calls. This entire loop of experimentation, model training, and decision-making is repeated until a given experimental budget is reached or until the given target is achieved. While any model can serve as a surrogate model, GPs and RFs are typically chosen, as they train quite well with small amounts of data, which is typically the case in the early iterations of an SDL campaign. While GP models learn distributions of functions and intrinsically provide uncertainty estimates, RFs provide an uncertainty based on the ensemble of decision trees. DL surrogate models like multi-layer perceptrons (MLPs), convolutional neural networks (CNN), recurrent neural networks (RNN), or GNNs are less common in SDL applications, and usually require pre-training from larger datasets, (e.g., computational datasets). However, probabilistic DL models such as Bayesian neural networks (BNNs)293 have been shown to work relatively well with lower amounts of data, due to the regularization effect of the neural network weight distributions. Given the versatility and recent success of the BO methods for experiment planning, there has been an intense effort to provide software libraries targeting multi-objective optimization, compatibility with specialized material/chemical optimization tasks, better handling of categorical variables, increased accessibility as well as benchmarking. Shields et al. introduced an open-source Python package Experimental Design via Bayesian Optimization (EDBO)294 and provided multiple benchmarks. The authors showed BO with GP performs statistically better when compared to common DoE techniques suitable for both continuous and discrete variables in maximizing Suzuki and Buchwald-Hartwig reaction yields. While EDBO was capable of recommending experiments in batches with seemingly no performance loss, Tores et al. published EDBO+295 that extended the platform to tackle multi-objective tasks. They further augmented the platform with a cloud powered web-interface to provide accessibility to non-coder scientists as well. Hasë et al. developed the Bayesian optimizer Phoenics296 that addressed the issue of large numbers of samples required for chemical global optimization tasks, particularly where evaluation of a point in chemical design space is costly. By utilizing an autoencoder-like BNN for kernel density estimation from observations, a surrogate function can be constructed with higher efficiency. Phoenics was benchmarked on a reduced Oregonator model for the Belousov− Zhabotinsky reaction, and, when compared to RF, GP, PSO, and CMA-ES, Phoenics outperformed the other methods after only 25 evaluations. Later on, Hasë et al. introduced Gryffin,297 an extension of Phoenics with a particular focus on handling categorical variables. Additionally, Gryffin considers the correlation among the variables through the use of descriptors; for example, the physicochemical descriptors of selectable solvents. Among multiple optimization strategies and packages such as PyEvolve,298 SMAC,299 HyperOpt,300 and GPyOpt,301 the authors reported the best performance with the Gryffin optimizer. Other chemistry specific BO algorithms include Gemini,274 which extends Gryffin to multi-fidelity optimization, Golem,302 which identifies optima that are robust to input or measurement uncertainties, and Anubis,303 which incorporates unknown experimental constraints into the acquisition function. Recently, Hickman et al. introduced Atlas,304 an open-source software library incorporating the functionalities of the aforementioned BO softwares, including mixed parameter BO with a priori known and unknown constraints, as well as multi-objective, multi-fidelity and robust optimization capabilities. With an emphasis on integration with SDLs, the authors showcased the Atlas library embedded in the ChemOS 2.0111 SDL orchestrator for oxidation potential optimization of metal complexes using electrochemical measurements. It is also worth noting that there are many off-the-shelf and general-purpose BO packages available. BoTorch306 is a modern BO library built on top of PyTorch307, offering modular components, Monte Carlo (MC) acquisition functions, and a variety of advanced optimization features such as high-dimensional, multi-fidelity, multi-objective, mixed-variable, and constrained optimization. Notably, the Ax (Adaptive eXperimentation) platform is a high-level wrapper to BoTorch managed by the same developers which significantly reduces the required learning curve and has seen recent usage in materials informatics.308−310 GAUCHE, by Griffiths et al., 311 is an open-source GP framework built atop GPyTorch,312 BoTorch306 and RDKit,184 with a suite of custom kernel functions for GPs, and built-in performance and BO benchmarks for molecular and reaction discovery. Dragonfly313 is an optimization library for handling both multi-fidelity and high-dimensional optimizations, emphasizing adaptability to various domains. SMAC3 (Sequential Modelbased Algorithm Configuration)314,315 combines BO with RFs, and is particularly suited for algorithm configuration tasks. GPyOpt301 utilizes GPs as surrogate models for BO, offering a variety of GP-based acquisition functions. HEBO316 (Hierarchical Evolutionary Bayesian Optimization) integrates evolutionary strategies with BO, presenting a hierarchical approach to enhance search efficiency. HyperOpt300 is a Python library for serial and parallel optimization over challenging search spaces, utilizing techniques like tree-structured Parzen Estimator (TPE)317 rather than traditional BO. Thompson sampling efficient multi-objective optimization (TS-EMO)318 is a general-purpose BO with a GP surrogate allowing both multi-objective optimization and batch suggestions. While off-the-shelf models are convenient, studies have shown that careful selection of surrogate models can greatly influence the performance BO or active learning campaign. For example, for the GP surrogate, Noack and Reyes stress the importance of understanding the physical system in selecting hyperparameters, kernel and mean functions.319 Many studies commonly use standard kernels like the radial basis function, or Materń kernels, without considering the underlying physics of the system, anisotropy in the input features, or stationarity of the data points. Ziatdinov et al. proposed GPax,320 a novel approach that augments GPs with structured probabilistic models to incorporate prior physical knowledge into BO and active learning tasks. Unlike standard GP-based BO, GPax balances the flexibility of non-parametric GPs with the rigidity of parametric models encoding known physical behaviors. The authors demonstrate GPax's capabilities on synthetic test functions, as well as physical lattice models like the 1D and 2D Ising models, where it outperforms classical GPs in discovering optimal regions and reconstructing phase boundaries with fewer observations. Further studies have demonstrated the potential for GPax in improving optimization in highthroughput experimental studies,321−323 increasing explainability of the surrogate model in hypothesis learning.324−327 There can be significant performance variance between experiment planning approaches for different chemical and material optimization/engineering tasks, even for slightly different tasks within the same domain.328 Therefore, the benchmarks have been developed to evaluate the various methods, which can be useful when initially choosing an experiment planning algorithm and the associated hyperparameters. Olympus329,330 and Summit331 are examples of BO benchmarking platforms with realistic chemical tests and experiments. For finding efficient black-box approaches, Tom et al. studied the effect of different chemical featurizations and surrogate models on the predictive performance and uncertainty calibration on different small chemical datasets, and the optimization performance in the context of BO.332 Liang et al. benchmarked different BO flavors specifically on the materials science datasets covering a wide domain ranging from electrical conductivity of drop-casted composite blends to shape scores of 3D printed materials.333 The authors further defined useful metrics for evaluating the acceleration and overall performances of optimization. Rohr et al. compared performances of linear ensembles (LEs), RFs and GPs for active learning based minimization of multi-metal oxide catalyst compositions’ overpotential toward oxygen evolution reaction.334 We summarize some commonly used BO tools for experimental planning in Table 2.",
    "3. Analytical Process Optimization": "The earliest examples of chemistry SDLs that automatically perform a sequence of experimental tasks, planned by a data-driven algorithm in a closed loop, largely stem from the field of analytical chemistry. As early as in the 1960s, the optimization of measurement parameters to maximize e.g., the response of a single instrument, has been addressed in an iterative closed-loop fashion. Whereas these approaches do not fall under the scope of this review, and are routinely implemented into modern (analytical) instruments, it is remarkable that this iterative optimization, taking into account data from previous iterations (in contrast to e.g., PID controllers) has already been achieved more than 50 years ago[cite: 208]. As an example, Ernst et al. demonstrated the autonomous optimization of magnetic field homogeneity in a nuclear magnetic resonance (NMR) spectrometer by adjusting currents along the spinning axis, controlled by a gradient-based and a simplex-based algorithm[cite: 208]. To the best of our knowledge, this work represents the first published example of a simple Level 2 SDL in the field of chemistry.\n\nLevel 3 SDLs are realized when coupling an analytical technique to an upstream operation such as automated sample preparation or separation[cite: 225]. In these setups, the goal definition of the SDL is the identification of those process conditions that optimize the detectability or quantifiability of specific materials. Whilst these tools can be regarded as components of larger SDLs in materials discovery, the development of robust analytical methods has been an active field of research for multiple decades, and has been addressed using SDL approaches early on. In this section, we will review the autonomous optimization of such analytical processes, and further related experimental procedures.",
    "3.1. Composition and Detection Process Optimization": "The identification of ideal measurement parameters for analytical processes is of enormous importance across all chemical industries, and has attained considerable attraction from the standpoint of autonomous optimization[cite: 226]. Beyond simple PID-type controllers, however, due to the limitations of computational power and automated laboratories before the 2000s, experimental planning was done primarily by simplex optimization, and automation typically done through simple step motors and flow systems [cite: 226-227].",
    "3.1.1. Sample Preparation": "The earliest examples of what would be considered a Level 3 SDL as per the definition of this review, have been reported in the 1970s, addressing the optimization of analytical procedures for spectroscopic detection[cite: 227]. Typically, in these works, the optimization would target the automated sample preparation for a subsequent spectroscopic detection, in order to maximize the spectroscopic response for the material of interest.\n\nOne of the first examples of automated optimization of chemical detection was from King and Deming in 1974[cite: 228]. The SDL used a rotating motor with attached pumps to dispense various reagents into a flow system. The chemical system studied was the acid-dependent chromate-dichromate equilibrium. Characteristic peaks in the measured UV-Vis absorption spectra correspond to an equimolar solution. Using simplex optimization, the intensity of the characteristic absorbance peak was maximized as a function of the chromate pump speed over the course of 26 automated experiments[cite: 228]. Following this initial work, a series of similar publications appeared that used automated continuous flow analyzers to perform Simplex optimizations over multiple variables for the detection of glucose [cite: 228-230].\n\nMieling et al. developed a more sophisticated flow system by introducing automated flow-stopping, controlled by a magnetic-tape minicomputer with an analog-to-digital converter[cite: 230]. The flow stopping method allowed for automated solution preparation. The authors demonstrated the capabilities of the platform by creating a series of solutions of varying concentrations. The platform was then used in the detection of titanium through its reaction with hydrogen peroxide in the presence of ethylenediaminetetraacetic acid (EDTA), monitored by absorption spectroscopy and optimized by simplex optimization[cite: 230]. Further advances in analytical laboratory automation led to works involving the Zymark robotic arm, capable of sample preparation, solution addition, and sample transfer into a spectrophotometer[cite: 230]. Lochmüller et al. utilized this SDL to optimize the concentration of MgIn, monitored by UV-Vis absorption spectra, through a reaction that is dependent on Ca2+ ion concentration and pH[cite: 230].",
    "3.1.2. Separation and Chromatography": "The most prominent and widespread class of coupled analytical techniques are chromatographic methods, in particular gas chromatography (GC) and liquid chromatography (LC)[cite: 231]. While, as of today, commercial instruments are sold as integrated solutions, the underlying process is composed of two major operations: first, separation of the analytes occurs on a stationary phase column, and the mobile phase stream is transferred to a downstream detector for measuring the analyte response as a function of time. In this context, identifying the right conditions that enable good and efficient separation of unknown compound mixtures, represents a major challenge [cite: 231].\n\nFoundational work towards the use of data-driven algorithms, particularly the simplex algorithm, for chromatographic separation optimization had been performed in the 1970s[cite: 231]. Shortly after, the first autonomous examples of separation optimization for high-performance liquid chromatography (HPLC) were reported by Berridge in 1982[cite: 231]. Using the simplex algorithm for experiment planning, the authors optimized the eluent composition to maximize the chromatographic resolution for mixtures of 4-5 organic compounds, detected on a single-wavelength UV spectrometer (Figure 5). Building on this foundational work, Berridge and co-workers reported a series of further advancements, including multi-parameter optimization, constrained optimization, and multi-wavelength detection [cite: 231].\n\nVery recently, Boelrijk et al. show the use of BO tools for a fully autonomous optimization of multi-step gradients in HPLC[cite: 234]. Using a multi-objective strategy for simultaneously optimizing resolution and elution time, the authors showcase the autonomous development of separation gradients for complex dye mixtures consisting of up to 50 different analytes while only performing ~30 experiments[cite: 234].",
    "3.2. Other Properties": "Similar to the discussed works on automated sample preparation, other properties such as solution properties of pH or solute concentrations (for crystallization) have been addressed using automated liquid handling systems, instructed by data-driven algorithms in a closed-loop fashion[cite: 235]. Solid state properties such as X-ray diffraction (XRD), and small-angle X-ray scattering (SAXS) signals have also been optimized in a closed-loop manner.\n\nAs a prominent example, Clayton et al. used an automated flow system SDL to determine the solvent volume ratio and pH for liquid-liquid extraction[cite: 237]. The authors studied the separation of a-methylbenzylamine and N-benzyl-a-methylbenzylamine dissolved in toluene, while the solvent and hydrochloric acid flow rates, temperature, and the residence times were varied. The outputs from the separator were analyzed by an on-line HPLC, with the goal of maximizing the amine purity for the two compounds. To extend into multi-step process development, the liquid-liquid extraction was performed in tandem with the reaction of a-methylbenzylamine and benzyl bromide to form N-benzyl-a-methylbenzylamine[cite: 237]. The purity of the product was the optimization objective, with the reaction mixture containing unreacted reactants and various amine-containing impurities. The optimal purity was at 71% which was identified in 53 experimental iterations (Figure 6)[cite: 237].",
    "4. Reaction Optimization": "For over a century, materials discovery has been governed and constrained by the ability to synthesize chemical compounds, and make materials. This applies to molecular discovery (drug discovery, agricultural chemistry, molecular optoelectronics) in particular, where synthesis often represents a tailored sequence of highly specific reaction steps, each of which comes with a set of variable parameters and process conditions. Both the discovery of optimal synthetic routes and the optimization of optimal reaction conditions for each step is therefore critical to all fields of materials discovery, and chemical industries. In fact, the industrial need for economic and ecological synthesis processes has led to the discipline of process and reaction chemistry, targeting route identification, reaction conditions, as well as design and engineering of reactors for synthesis on scale. Due to the importance of reactions in chemistry and chemical applications, a range of closed-loop workflows for reaction optimization have been developed over the last decades, particularly targeting the identification of optimal reaction conditions. This section aims to summarize these literature-known approaches and examples of self-optimizing reactors. In contrast to the following chapters of this review, we will not focus on the optimization of materials properties, but rather on optimizing the ways to synthesize a specific material. Given that the vast majority of studies has focused on organic reactions in solution, this chapter will introduce the major concepts using this class of transformations. In the following subsections, approaches toward other solution-phase reactions, as well as non-solution-phase reactions, will be discussed. Afterwards, this section will provide a comprehensive overview of all works in which the automated integration of robotic reaction execution and data-driven optimization has been demonstrated in multiple iterations for enabling autonomous reaction or process optimization (i.e., Level 4 SDLs, Figure 1). For a more global discussion from the perspective of reaction optimization, we refer the reader to existing reviews and perspectives in the field.375−383 For solution-based reactions, the search space comprises a wide series of categorical (identity of reagents, catalysts, solvents, additives etc.) and continuous (relative stoichiometries, concentration, temperature(s), reaction time(s) etc.) variables. With the goal of automating the optimization process, the choice of the appropriate automation platform for performing the reaction (see Hardware) facilitates or complicates the variation of specific parameters. Generally, the variation of continuous parameters, particularly reaction quantities, can be readily performed on a wide variety of experimental systems (Figure 7). This has led to a large number of studies for optimizing continuous reaction parameters in an automated fashion. On the other hand, optimizing over categorical parameters, such as specific choices of chemicals, poses additional challenges both from the hardware and the software side; the physical availability of reactants and reagents, as well as the storage capacities on the automated platforms, pose physical constraints to the number of available categorical options. As a consequence, most examples of closed-loop reaction condition optimization have operated on comparatively few options (usually < 10) for categorical variables. Additionally, categorical parameters are not readily represented numerically, and lack an unambiguous order or measure of similarity (e.g., between solvents, or catalysts). This requires human decision in selecting an appropriate representation for chemicals for the optimization algorithm. Optimization algorithms that operate on molecular entities, as well as mixed continuous−categorical parameter spaces, are discussed in detail in Software.",
    "4.1. Spezialised Hardware and Software":"The primary objective of optimizing a synthetic reaction is generally the reaction yield, i.e., the quantity of the desired reaction product that is formed. Reaction selectivity, defined as the ratio between the desired product and an undesired side product (e.g., a constitutional isomer or a stereoisomer) can be regarded as an auxiliary measure of product formation. In terms of prediction, this a particularly challenging objective; reaction yields do not only depend on the rates of all steps in the desired reaction sequence, but also on the rates of a multitude of possibly unknown side reaction pathways, which renders the physics-inspired modeling of reaction yields highly difficult. Added complexity stems from the coupling of chemical reaction kinetics with physical transport phenomena (e.g., mass transfer/diffusion, and heat transfer). This dependence on unknown steps and mechanisms can lead to crossdependencies between the assumedly independent variables, and unforeseen activity cliffs upon small variations. In these scenarios, traditional OFAT optimization approaches, which have been the method of choice for reaction optimization in most laboratories, face severe challenges. In this context, the optimization of chemical reaction yields is particularly well− suited for data-driven, system-agnostic optimization approaches (see Software for further details). When it comes to larger-scale reactions and industrial process optimization campaigns, reaction yield or selectivity is no longer the sole optimization objective. Economic and ecological considerations present further constraints to the optimization problem, which can include: reagent and energy costs, atom economy, or environmental impact factors, such as measures of waste formation, or operational and environmental hazards",
    "4.1.1. Reaction Execution":"On a laboratory scale, the execution of solution-phase chemical reactions can be classified into two complementary strategies: batch and continuous-flow operation (Table 3). Both strategies come with distinct chemical (dis)advantages for performing specific types of reactions, which have been thoroughly discussed in the literature, and are outside the scope of this review. Instead, we aim to provide a discussion of these strategies in the context of automation, and developing autonomous self-optimizing reactors. For over a century, solution-phase reactions have been predominantly performed in batch reactors384�a fact well reflected in the practical chemistry education, where synthesis is mainly taught using beakers, flasks and vials as batch reaction vessels. In a teaching laboratory, and in many research laboratories, this approach is highly advantageous, as it allows the execution of a variety of different chemistries with minimal hardware requirements. In fact, most reactions can be executed by a human operator in standard, general-purpose glassware. However, batch reactions face severe challenges when it comes to isolation and purification, or the execution of multi-step reactions. The human-centric approaches to reaction workup and purification, such as extraction, crystallization or chromatography, are often based on specific operations that require large degrees of adaptive, intuition-guided decisionmaking. This has rendered their direct, programmatic translation into automated workflows challenging, and may be one of the reasons why batch reactors have not found widespread application in closed-loop discovery workflows yet. Recent advances in biotechnology and the corresponding liquid handling systems (see section on Hardware for further details) have enabled the miniaturization and parallelization of batch reaction execution in multi-well plates. In such cases, analyzing directly on the crude reaction mixture can significantly increase the automated experimental throughput, particularly when it comes to varying categorical variables such as the identity of reactants, reagents, catalysts or solvents. Therefore, such HTE systems have been primarily used for large condition or substrate screening campaigns for important catalytic reactions. As a complementary approach, the past decades have seen the development of flow reactors, in which reactions are performed in a continuously flowing stream of liquid. Importantly, with the requirement to continuously pump the reagent, reactant and reaction streams, this strategy to reaction execution is inherently automated. Still, flow reactions require highly specialized hardware and software, preventing the widespread adoption of this technology as a tool for chemical synthesis. Reactants and products must also be gaseous or liquid, and the liquid medium cannot be too viscous. Developments in microfabrication have led to miniaturization of flow systems into microfluidic systems (sometimes called lab-on-a-chip).385 The small footprint and low reagent consumption of microfluidic systems make them useful for high-throughput synthesis of compounds at small scales. The operation of chemical reactions in microfluidic systems provides a series of distinct advantages in terms of heat and mass transfer, interphase reactivity or safety. For more detailed discussions on flow and microfluidic reactors, we refer the reader to review articles in the literature.385−387 As such, both academic researchers and industrial teams have focused on developing automated in-flow synthesis platforms, leading to a large variety of specific, custom-built setups, with few standardized solutions on the market. From the perspective of autonomous optimization, the inherently automated nature of flow reactors made them ideal platforms for early explorations of autonomous operation modes, and a vast majority of examples of closed-loop reaction optimization (see below) have been performed on continuousflow platforms. These platforms have allowed for optimization of continuous parameters such as stoichiometries, reaction times, and temperatures, which can be readily varied in sequential experiments, leading to high experimental throughput. The exploration of larger numbers of categorical entities such as reactants or reagents, however, comes with increased hardware requirements and experimental efforts. Recent work has also demonstrated autonomous flow systems in Schlenk lines, allowing for studies involving highly reactive or sensitive compounds.388 Another major advantage of flow chemistry lies in the ability to telescope individual operations (including purification steps) into longer sequences, enabling the automated operation of multi-step sequences. This capacity, which has been reviewed comprehensively in the literature,389,390 has enabled the closed-loop optimization of multi-step reactions in solution, which will be discussed in the Multi-step organic reaction section. For screening and optimization purposes, segmented-flow approaches (often referred to as droplet reactors) provide an attractive approach for HTE in flow systems.386,391,392 Rather than having a continuous flow of liquid in which the reaction occurs, the stream is divided into small separated segments by an inert gas (such as argon) or immiscible liquid (such as perfluorinated oil). Each of these segments can be regarded as an individual batch, and precise operational control can allow for screening distinct reaction conditions (e.g., reagent identities or quantities) in each of these batches. Moreover, the smaller reaction volumes and high surface-to-volume ratios have demonstrated significant acceleration of reactions, leading to micro-droplet approaches such as microfluidic flow droplet reactors, and even free-standing non-flow systems.392",
    "4.1.2. Reaction Analysis": "Irrespective of the reaction execution platform, the second major component of a selfoptimizing reactor is a module to quantify the optimization objective, such as the reaction yield. The most widespread, general-purpose approach to this is the use of an automated chromatographic separation technique, usually LC, or GC, coupled to a quantitative detection technique. In this approach, an aliquot from the reaction mixture is taken and analyzed on the external instrument (Figure 7). Importantly, the required instruments have been commercialized for decades, and offer robust hardware solutions, which are available in most experimental laboratories. As an alternative, in situ monitoring techniques can be used to directly analyze the crude reaction mixture and monitor changes in the reaction composition to quantify possible optimization objectives. Spectroscopic tools such as NMR spectroscopy (implementable through benchtop spectrometers with flow cells) or infrared spectroscopy (flow cells or in-situ probes) can be used to identify and quantify compounds if unique, compound-specific signals exist. Further, in situ probes such as UV-Vis spectroscopy or conductivity cannot, in most cases, be used to quantify specific materials, but allow for the monitoring of global properties of the reaction mixture, which can serve as a valuable proxy for the actual optimization objective. Analytical techniques that enable the quantification of reactants, intermediates or products throughout the course of a reaction can enable decision-making in real time, e.g., for adjusting reaction times, temperatures or reagent quantities. Such adaptive optimization of reaction conditions, however, does not follow the iterative closed-loop definition of SDLs, and therefore exceeds the scope of this review.",
    "4.1.3 Early Examples of Autonomous Condition Optiimization": "Attempts to develop SDLs for aut nomous reaction condition optimization date back to the 1970s, when Winicov et al., from pharmaceutical company Smith, Kline & French, describe a fully automated batch reactor which shows remarkable similarities to modern open-source systems for automating batch reactions such as the Chemputer. 393 The authors describe automated modules for liquid addition (through pumps), stirring, heating and cooling, as well as reaction analysis by coupling to an HPLC-UV system. Remarkably, they even discuss the coupling of their platform with a simplex algorithm for automated experiment planning. However, no actual experiments have ever been publicly reported with this platform, neither in the initial publication from 1978, nor in any follow-up works; this is likely due to the proprietary nature of the research at Smith, Kline & French laboratories. To the best of our knowledge, the first published example of a self-optimizing reactor stems from 1987. Matsuda et al. reported the autonomous optimization of the adduct formation reaction between phosphotungstic acid and basic drug molecules, namely, chlorpromazine hydrochloride and levomepromazine hydrochloride.260 For this purpose, the authors had developed a robotic platform consisting of a Zymark robotic arm (see Figure 8 for a related experimental setup by Frisbee et al. 394) with two exchangeable tools for liquid transfer and vial transport, respectively. The reagents were added as solutions to a batch reactor vial, and the entire vial was first transported to a vortex mixer for stirring, and subsequently to a water bath for heating. Since the desired adducts are strongly colored, they can be detected quantitatively via steady-state absorption spectroscopy. After completion of the reaction, the vial is transferred to a UV-Vis spectrophotometer by the robotic arm, and the absorption at wavelength 538 nm was recorded as a proxy for the reaction yield. The simplex algorithm was used for iteratively planning the next experiment, varying the quantity of phosphotungstic acid and the reaction time. Remarkably, the authors show that the optimal reaction conditions can be found in less than ten iterations for both drug molecules. Using the same experimental setup, the authors demonstrated the optimization of a significantly more complex reaction in 1988 (see Figure 9):259 the conversion of a carboxylic acid to the corresponding hydroxamate using N,N’- dicyclohexylcarbodiimide (DCC) and hydroxylamine, followed by the complexation of the hydroxamate with an iron(III) salt to give a colored, UV-Vis-detectable complex. The authors demonstrate the optimization of up to four continuous parameters (quantities of DCC and hydroxylamine, reaction times of both steps), showing that optimization can be performed in under 30 experiments. The authors benchmarked their optimizer against a grid search strategy, demonstrating a significant reduction (>75%) in the number of required experiments. Inspired by these early results, and the sophisticated automation platforms developed in the 1980s (see e.g., Figure 8), Lindsey and co-workers made a series of contributions to early SDLs in solution-based synthesis. On the hardware side, the design of their “automated chemistry workstation”395 is of note; in parallel with the developments of small-scale pipetting robots in biochemistry that spilled over to chemistry only a decade later, their system enables the miniaturization of chemical reactions (to μL scale), as well as advanced analytical techniques, including, but not limited to, automated thin-layer chromatography. In addition to a series of automated data generation workflows, as a proof-of-concept, they demonstrated the closed-loop optimization of an “optical filter,” creating a specific absorption profile by mixing different dye solutions. Experiments are planned iteratively by the simplex algorithm, and are executed sequentially on the automated platform until the desired absorption profile is reached.396 Showcasing an application in synthetic chemistry, the authors perform the closed-loop optimization of the synthesis of porphyrin dyes from aromatic aldehydes and pyrrole under acidic conditions.397 The concentrations of both reactants (in 1:1 stoichiometry) and of the acid additive were used as independent variables. The yield of the product, obtained by quantitative DDQ oxidation, was determined by UV-Vis spectroscopy. The authors demonstrate accelerated experimentation by comparison with a full factorial design approach (Figure 10). Beyond these works, Lindsey and co-workers made a series of important contributions to advance optimization and decision-making algorithms beyond the native simplex algorithm,375 for example, using decision tree algorithms to handle screening and optimization of categorical variables. While such systems may have been utilized in industrial settings, given the research effort from both academic and industrial researchers,398 publications from the late 1990s for SDL optimization of synthetic reactions are sparse. The cost and reproducibility of the robotic hardware, as well the transferability of software may be influential factors in this regard.",
    "4.2. Single-Step Organic Reactions":"The rise of flow chemistry as a versatile, automated tool for reaction execution, as well as the increased accessibility and distribution of software via the internet, have sparked new interest in the development of self-optimizing reactors in the late 2000s and early 2010s. Since then, a large number of examples focusing on the optimization of organic reactions in solution have been reported in the literature. This section will present the most important concepts and advances using selected examples. A complete, to the best of our knowledge, list of further examples from the literature, including the target reaction, independent optimization variables, hardware, software and optimization objectives is given in Table 4. Detailed discussions of single- and multi-step reactions optimization are provided after.",
    "4.2.1. Self-Optimizing Flow Reactors and Analytical Advances:":"The first modern examples of self-optimizing reactors were reported by Jensen and co-workers in 2010.269,399 In their first work, McMullen et al. show the optimization of reaction conditions for the Heck-coupling between 4-chlorobenzotrifluoride and 2,3-dihydrofuran in a flow microreactor.399 The optimization was carried out to maximize the HPLC-determined yield of the mono-arylated reaction product, which is prone to undergo an undesired second coupling. Categorical parameters such as solvents, phosphine ligands and palladium sources were systematically screened to find conditions under which ammonium salts are soluble and the formation of palladium black is minimized, as this leads to clogging of the microreactor. Subsequently, a closed-loop optimization campaign over the continuous parameters residence time and alkene:aryl chloride ratio was carried out using the Nelder-Mead simplex algorithm. The authors show that the optimal conditions can also be carried out in a meso-reactor while preserving the optimum yield, demonstrating the successful transfer from micro- to mesoscale systems. Notably, even though over 20 years had passed since the initial demonstration of self-optimizing reactors, the selected optimization algorithm is highly similar to the early works discussed above. Using a similar setup, McMullen et al. reported the evaluation of multiple “black-box” optimization algorithms,269 namely the steepest descent algorithm, the Nelder-Mead Simplex algorithm253 and SNOBFIT,335 for the closed-loop optimization of the Knoevenagel condensation of p-anisaldehyde and malonitrile in a flow microreactor (see Figure 11).269 All algorithms converged to essentially the same optimum conditions within 12 hours. The authors optimized a weighted objective function of product yield and flow rate, thereby showcasing the first multi-objective self-optimization. Shortly after, in 2011, Poliakoff and co-workers reported a series of examples in which they employ their SDL for the optimization of reactions using γ-alumina as a heterogeneous catalyst in supercritical CO2 as the solvent.400 Parrott et al. optimized the yield of the dehydration of ethanol, the yield of the carboxymethylation of 1-pentanol with dimethyl carbonate (DMC), and the yield of the methylation of 1-pentanol with DMC. All of these optimization runs used a super-modified simplex algorithm to optimize temperature, pressure and CO2 flow rate as variables (see Figure 12). The latter optimizations were each completed in approximately 1.5 days, whereas a combinatorial search in the condition space would have taken more than 50 times longer, showcasing the efficiency of selfoptimizations. Later, Bourne et al. re-evaluated the methylation of 1-pentanol using different methylating agents in a fourvariable optimization, using the super-modified simplex algorithm.443 In a following study, Jumbam et al. evaluated different objective functions for optimizing this transformation:401 the yield, the space-time yield, the E-factor, E+ (the Efactor including all wastes) and the weighted space-time yield, calculated by the product of the space-time yield and the yield. The different criteria were shown to result in different optimal conditions. Surprisingly, a low E-factor led to a high value of E +. Overall, this shows the importance of designing an appropriate objective function when considering multiple inherently competitive optimization targets. After these initial developments of self-optimizing reactors, a diversification of analytical techniques took place rapidly, allowing researchers to harness the different advantages of each technique (see discussion above). In 2012, Moore and Jensen reported an in-line flow IR cell to optimize the Paal-Knorr synthesis of pyrroles.402 With this IR setup (Figure 13), steadystate conditions can be ensured before objective functions are evaluated. As a first objective function, the authors aimed to maximize the ratio between conversion and residence time. However, this led to poor conversions, and a quadratic loss function was applied to yields lower than 85%. The newly designed objective function resulted in an optimum of 81% conversion, demonstrating the difficulty of selecting combined objectives in multi-objective optimization, and highlights the importance of multi-objective optimization algorithms. This initial development has inspired the adoption of in-line IR techniques in a variety of self-optimizing platforms, such as the optimization of an Appel reaction,117 a [2+2] cycloaddition413 and others,116,265,403,419 pointing out the efficiency improvements, owing to the ability to circumvent time-intensive chromatographic methods. In 2015, Sans et al. reported the use of in-line NMR spectroscopy for the self-optimization of the acid-catalyzed imine formation between 4-fluorobenzaldehyde and aniline.404 Recorded 1 H NMR spectra were automatically phased and baseline-corrected, and the peak integrals were automatically evaluated to optimize an objective function related to the space-time-yield (Figure 14). Even though benchtop NMR spectrometers are commercially available at a reasonable price, their low sensitivity, as well as the difficulty of identifying and resolving characteristic signals, have prevented a more widespread usage in self-optimization platforms. A notable exception is the synthesis of the natural product Carpanone by Felpin and co-workers.416 In the same year, Holmes et al. demonstrated the usage of online quantitative MS for self-optimizing the synthesis of N’- methyl nicotinamide from methyl nicotinate and aqueous methylamine, varying the flow rate of methyl nicotinate, the quantity of methyl amine and the temperature as continuous independent variables (Figure 15).406 Before the selfoptimization experiments, HPLC was used to calibrate a benchtop MS, in order to use the latter for product quantitation without prior purification in the SDL campaign. The authors compared a self-optimization using the SNOBFIT algorithm335 with a classical DoE statistical design approach, finding that both methods found high-yielding conditions. However, while the SNOBFIT algorithm took 12 hours to find the optimum, the DoE approach only took 5.5 hours, due to the human intuition provided in the DoE: heating and cooling of the reactor is time-consuming, so avoid large jumps of temperature in the selected experimental parameters. Despite being commonly used for product identification in conjunction with HPLC, online MS was not widely adopted for quantification in self-optimizing platforms. Other examples are the hydrolysis of 3-Cyanopyridine by Ley and coworkers,117 and the optimization of multiple reactions with RL by Zare and co-workers (vide infra).336 In addition to the integration of more analytical techniques into self-optimizing systems, the used reaction platforms have also seen a significant diversification. On the level of equipment, Fitzpatrick et al. have demonstrated the LeyLab, whose components were designed to communicate via the Internet and are thus accessible through every browser with Internet Access.117 The LeyLab consists of four parts, a graphical user interface (GUI), a database for information storage, an equipment communication module and an equipment command module. Among others, they used this platform to optimize the Appel reaction of 1-phenylethanol, or the hydrolysis of 3-Cyanopyridine over a heterogeneous MnO2 catalyst using a flow reactor setup. The same group further used their internet-based lab in an across-the-world optimization of the syntheses of multiple active pharmaceutical ingredients.116 The optimization was initiated by a researcher residing in Los Angeles (California, USA), directed by remote servers in Japan and carried out in Cambridge (UK). Similarly, Skilton et al. demonstrated remote controlled self-optimizing reactors,128 where collaborators from China, Ethiopia and Brazil directed the optimization of self-etherification of npropanol and methylation of n-butanol and n-propanol through the cloud. In their commentary, the authors note that “watching an optimization in progress can be quite addictive, rather like watching the bids rising during an eBay auction” and further comment on the safety issues, intellectual property and financing of such cloud-based laboratories. A modular flow system was introduced in 2018 by Bedard ́ et al. for autonomous reaction optimization.265 The system consisted of several bays that each could fit a modular unit, e.g., a photo-reactor, a heated reactor, a cooled reactor, a packed bed reactor, a liquid-liquid separator or a bypass (Figure 16). Moreover, the system was connected to in-line analytics such as HPLC, MS, IR and Raman. The authors showcased the modularity of the platform by optimizing the conditions for maximal yield of a multitude of reactions, namely a BuchwaldHartwig Cross coupling, a HWE Olefination, a reductive amination, a Suzuki-Miyaura cross coupling, an SNAr reaction, a photoredox reaction and a multi-step ketene generation followed by a 2+2 cycloaddition. For each reaction, the authors manually designed the appropriate flow system, which was then used to autonomously optimize reagent equivalents, residence times and bay temperatures as independent variables. After each successful optimization campaign, the optimal conditions were examined for different substrates. In one case, where the conditions did not lead to a satisfactory yield for a specific substrate, a re-optimization was conducted with a subset of variables within 6 hours, improving the yield from 67% to 97%, demonstrating the flexibility and efficiency of the flow platform. Despite the prevalence of flow reactors, other reactor types have also been applied recently in self-optimization campaigns.In particular, Clayton et al. demonstrated multiple cascaded CSTR reactors which can provide conditions similar to a flow system while decoupling mixing performance from flow rate, thereby facilitating multiphasic reactions.421 Further, they also allow experiments with reactions involving solids and slurries, for which clogging is often a problem in conventional flow systems. The latter was utilized by Nandiwale et al. in 2022 for the successful self-optimization of a Pd-catalyzed SuzukiMiyaura coupling, as well as two metallaphotoredox-catalyzed Csp3−Csp3 and Csp3−Csp2 couplings of alkyl carboxylic acids and halides, respectively.428 Each of the investigated reactions involved at least one solid reactant, catalyst, additive or product, which could be transferred as a slurry in the CSTR. Leonov et al. report the development of an integrated selfoptimizing programmable chemical synthesis and reaction engine.442 They incorporated various sensors, including those for monitoring color, temperature, conductivity, pH, and liquid transfers, into their previously discussed Chemputer robotic platform. Additionally, they integrated analytical instruments like HPLC, NMR, and Raman spectroscopy, enabling closedloop reaction optimization via feedback control. Adaptive execution of chemical procedures on the Chemputer was made possible by the dynamic χDL programming language. The authors demonstrated the platform's capabilities through temperature-controlled reagent additions, optical endpoint detection, and hardware failure detection. The authors optimized several organic reactions, including the Ugi fourcomponent reaction, Van Leusen oxazole synthesis, manganese-catalyzed epoxidation, and trifluoromethylation reactions, utilizing various optimization algorithms like BO with GP surrogate, Phoenics BO, SNOBFIT, and genetic algorithms. The optimization led to improved product yields of up to 50%. Furthermore, the authors showcased an experimental pipeline for exploring unknown reaction spaces, combining digital discovery and optimization, exemplified by the discovery and optimization of two previously unreported reactions.",
    "4.2.2. Discrete and Categorical Optimization and Batch-Type Reactors":"The previously discussed studies have primarily focused on the optimization of continuous variables such as reagent stoichiometry, temperature or reaction time. Chemical reactions, however, are highly governed not only by the continuous parameters defining the process details, but also, most importantly, by the involved reactants and reagents, which are inherently categorical parameters. As discussed above, such optimization over categorical variables requires specific adaptations both in terms of software and hardware, and are often better suited for parallel batch reactor setups. To the best of our knowledge, the first example of autonomous, closed-loop reaction optimization in batch reactors (since the early examples from the 1980s) was reported by Burger et al. in 2020,11 tackling the homogeneous photocatalytic water splitting reaction. Notably, their work stands out owing to the highly advanced robotic setup used for performing and analyzing reactions (Figure 17). In this work, the authors introduce their “mobile robotic chemist” (for a more detailed discussion, see section on Hardware), a KUKA mobile robot that is designed to operate human-centric workstations. The robot transfers reactors between workstations for solid dispensing, liquid dispensing, inertization, capping and GC analysis. They utilized their robot to investigate the water splitting reaction catalyzed by the photoactive polymer P10. However, to circumvent the need for categorical optimization, the authors treated the quantities of each additive as a continuous variable, enabling the use of an off-the-shelf GP surrogate with an upper confidence bound acquisition function for BO. With their highly advanced experimental setup, the authors demonstrate 43 fully autonomous batches of experiments in approximately 8 days, resulting in an almost 10-fold increase in hydrogen evolution. Similarly, Ha et al. recently reported SynBot,432 a platform for autonomous organic synthesis in batch reactors, which was demonstrated for carbon-coupling reactions. SynBot consists of an AI layer, an AI−Robot layer and a Robot layer. As the AI layer, the authors trained a retrosynthesis model as well as a GNN that proposes suitable reaction conditions in combination with BO on a search space that consists of commonly used catalysts, bases and solvents for multiple reactions: Suzuki coupling, Buchwald amination, and Ullmann reaction. The SDL features an integrated robotic system capable of executing various tasks, including chemical dispensing, reaction handling, sampling, and analysis. The system aims to iteratively refine and optimize synthetic routes and reaction conditions in order to maximize the reaction yields. As an alternative to classical batch reactors, Jensen and coworkers reported a series of SDLs using a segmented-flow system in which each droplet�i.e., each “batch”�contains a specific reaction with a unique set of conditions (Figure 18).405 In their first work from 2015, Reizman et al. use this platform for screening potential solvents and optimizing continuous reaction conditions for the mono-alkylation of trans-1,2- diaminocyclohexane. To address this mixed categorical− continuous optimization, the authors performed an initial fractional factorial DoE for every solvent, followed by another fractional factorial design at experimental conditions close to the predicted optimum, and a feedback DoE search to minimize the uncertainty on the maximum predicted yield for each solvent separately. Subsequently, insufficiently performing solvents were disregarded and an automated gradient-based search around the predicted optimum was carried out for the remaining solvents to optimize the yield. Similar principles for the incorporation of categorical parameters in self-optimizations were subsequently used to select optimal precatalyst scaffolds and ligands for a SuzukiMiyaura coupling,408,412 organic base and Ni precatalyst for a photoredox Ir−Ni dual-catalyzed decarboxylative arylation,411 organic base for several Pd-catalyzed C-N coupling reactions,420 catalyst for a Suzuki-Miyaura coupling, base for a metallaphotoredox-catalyzed sp3 −sp3 cross-coupling of carboxylic acids with alkyl halides or photocatalyst for a metallaphotoredox-catalyzed decarboxylative cross-coupling reaction.428 Slattery et al. made use of readily available internet-of-things phase sensors to detect the relevant reaction slugs in their flow system, dubbed RoboChem.439 The authors integrated off-theshelf hardware and custom software to build a modular platform, which contained a GUI to enable operation by nonexpert chemists. The platform contained a light source with tunable intensity, enabling the autonomous optimization and scale-up of a multitude of photo-catalyzed reactions. The RoboChem platform employs multi-objective BO, as implemented in Dragonfly, to autonomously plan and execute experiments. This autonomous experimentation capability allows the system to explore complex chemical spaces efficiently, identifying optimal reaction conditions tailored to each substrate. The authors demonstrated the platform's versatility by optimizing a diverse set of 19 photocatalytic transformations, including hydrogen atom transfer photocatalysis, photoredox catalysis, and metallaphotocatalysis, which are relevant to pharmaceutical and agrochemical synthesis. An alternate approach integrates the selection of categorical reaction variables directly through a suitable encoding of chemicals into appropriate optimization variables, rather than creating a separate response surface for each categorical reaction variable and comparing the response surfaces. This was done by framing the choice of each categorical variable through one-hot encoding all categorical possibilities or calculating descriptors for each categorical variable. The former method was used to find an optimal base for a regioselective SNAr reaction, a suitable phosphine ligand for a Sonogashira coupling,434,444 optimal solvents and phosphine ligands for multiple C-H activation reactions,436 optimal electrophiles and solvents for a Schotten-Baumann reaction.445 The selection of expert-crafted, physically meaningful descriptors is an important strategy to introduce additional knowledge into the optimization campaign.366 In 2021, Christensen et al. reported on this concept for the optimization of a stereoselective Suzuki−Miyaura coupling.425 Notably, the authors used a batch system for reaction execution, namely a Chemspeed SWING platform coupled to a HPLC-UV system, to run parallel reactions in 96-well plates. The use of this batch reactor system enabled the authors to optimize a wide, representative set of 23 phosphine ligands selected in a fully data-driven fashion. Furthermore, the authors considered several other continuous parameters such as reaction temperature, palladium loading, boronic acid equivalents and phosphine to palladium ratio to optimize the yield of the Ediastereomer, while minimizing the Z-diastereomer yield and the quantities of used reagents. Mixed continuous-categorical optimization was performed using the Gryffin package relying on a BNN surrogate.446 While finding a similar optimum, the descriptor-based optimization campaign converged slower than a reference campaign based on one-hot encoding only, which was attributed to the introduction of unproductive bias through the selected descriptors. A remarkable example of categorical optimization was reported by Angello et al. in 2022, who�rather than optimizing reaction conditions for a single substrate combination�targeted the discovery of general reaction conditions.431 The authors defined the most general conditions of a reaction type as those conditions that provide the highest average yield across the widest range of substrate space. The authors showcase this concept at the example of heteroaryl Suzuki− Miyaura couplings using protected boronic acids. To identify the “widest range of substrate space”, data-driven clustering techniques were employed to identify a set of 11 representative reactions for which general conditions should be identified. Optimization was performed over the identity of solvent, base, catalyst and ligand, and the reaction temperature as independent variables. Experiments are performed on a custom-built automated reaction platform that is capable of performing 36 parallel batch reactions with 20 distinct reagents under inert gas conditions.447 The authors developed a custom BO workflow for maximizing generality, the yield over multiple reactions. Notably, the fully explorative acquisition strategy is designed in a way that it does not require evaluating each of the 11 representative reactions in every iteration. Using this approach, the authors managed to efficiently cover a wide space of conditions and substrates, ultimately identifying conditions that double the average yield compared to benchmark general conditions. Related work in optimization of generality of reaction conditions have also been done, although no automation is involved.448 While not optimizing for general reaction conditions, Schilter et al. recently performed a simultaneous optimization over multiple substrates.440 Using a robotic batch system containing six reactors, the authors performed a single optimization campaign in which the yield and conversion for an alkyne iodination was jointly optimized for multiple substrates. Notably, in the optimization campaign, the substrate was a parameter that could be chosen by the optimization algorithm to optimize the reaction conditions. In order to find the optimal conditions for all of the substrates, a substrate could not be selected by the algorithm after a satisfactory performance (conversion > 80%) was obtained. Remarkably, the optimization campaigns showed high transferability as the optimization run was primarily conducted on one of the substrates, and after a satisfactory performance was obtained for this substrate, the same was achieved for the other substrates of interest, requiring a total of only 23 experiments to find suitable conditions for all substrates.",
    "4.2.3 Pareto Optimizations and Further Algorithmic Advances":"So far, all discussed SDL reaction optimization campaigns were conducted as a single-objective optimization, where the objective is reaction yield in most cases. Optimizing for multiple objectives allows researchers to consider multiple metrics of a reaction, such as yield, conversion, productivity or ecological factors. The most straightforward approach is to scalarize multiple objectives into a single objective value, which, however, requires pre-defining an often unknown tradeoff between different objective values. In an earlier example of this section, we have shown that this can lead to undesirable outcomes of the optimization campaign, particularly if the two objective functions show opposing trends. In such a scenario, impurity yield for a benzylation reaction of a primary amine.417 The elucidation of the entire Pareto front allowed the researchers to identify suitable trade-offs, which was particularly useful in the SNAr reaction, where the space-time yield could be significantly improved while almost not impacting the E-factor. Such a relation would not have been uncovered if only one optimal point was identified. Building on this work, Jeraal et al. evaluated the Pareto front between yield and cost of the mono-aldol-condensation of acetone and benzaldehyde using the TS-EMO algorithm.423 In order to benchmark the algorithm’s performance, the authors ran the campaign twice, once with 20 experiments and once with two low-yield (3% and 5%) experiments as a starting set. Both campaigns converged to the same Pareto front, even though the latter run needed roughly twice as many experiments. In addition, the authors demonstrated the general applicability of their approach by further uncovering the Pareto front between the space-time yield and the E-factor. Similarly, Karan et al. also employed the TS-EMO algorithm for the Pareto optimization of the yield and impurity for an ultra-fast lithium-halogen exchange reaction.438 The authors performed three optimization campaigns with either different initial experiments or different reactant mixing equipment, showing that the algorithm efficiently converges to similar Pareto fronts. Since the TS-EMO algorithm is computationally expensive for categorical parameters, a GP-based BO with the qNEHVI acquisition function291 to find the Pareto front for an optimization with continuous and categorical parameters.445 After demonstrating improved efficiency over TS-EMO in silico, experimental optimization revealed the Pareto front between the space-time yield and E-factor for a SchottenBaumann reaction. Similarly, using their GP-based mixedvariable multi-objective optimization (MVMOO) algorithm,444 Kershaw et al. identified the Pareto front for the yield of orthoand para-products of a SNAr reaction, where the optimization variables included continuous and categorical (solvent) variables.434 Interestingly, their method showed that different solvents are responsible for different regions of the Pareto front, enabling researchers to select the right solvents for the desired product. In a further experiment, the authors uncovered the entire Pareto front between the reaction mass efficiency and space-time yield for a Sonogashira crosscoupling, finding that the Pareto front is almost exclusively dominated by one phosphine ligand. In 2017, Zhou et al. demonstrated the applicability of RL for optimizing chemical reactivity.336 RL was used to learn a policy that determines the next experiment to conduct, where RNNs were used to fit the policy function. Owing to the high cost of experimental data, the algorithm was pre-trained on cheap simulated reaction data, obtained from non-convex mixture Gaussian density functions with multiple local minima. The performance of the RL algorithm was benchmarked against the Nelder-Mead simplex method, the SNOBFIT algorithm and the CMA-ES449 on the simulated data, and was found to outperform all of the established methods on average. However, no benchmarking against standard BO algorithms was performed. The pre-trained policy was then integrated into an SDL using micro-flow reactors with MS quantification, and was used to optimize the conditions of four different reactions: the Pomeranz-Fritsch synthesis of isoquinoline, the Friedlander ̈ Synthesis of a Substituted Quinoline, the synthesis of Ribose phosphate and the reaction between 2,6-Dichlorophenolindophenol and ascorbic acid. Again, the RL algorithm was compared with CMA-ES and a OFAT optimization, showcasing that RL consistently outperforms the other two methods. The optimization campaigns were carried out successively, with the policy improving after each completed optimization campaign, demonstrating the function and generalizability of the pre-trained DL agent model (Figure 20). Recently, Bennett et al. developed Fast-Cat,441 a gas-liquid segmented flow platform suitable for high temperatures and pressures. The platform enables the rapid identification of Pareto fronts for transition-metal catalyzed reactions through BO with the qNEHVI acquisition function. The authors utilized Fast-Cat to identify the Pareto front between the yield and the linear/branched selectivity of the hydroformylation of 1-octene for six different phosphine ligands. Each ligand encompasses different trade-offs between yield and selectivity, demonstrating the importance of efficient automation to uncover optimal conditions. The modular system integrates advanced process automation, in-line reaction characterization using GC, and closed-loop feedback algorithms to dynamically update its belief model and autonomously select new experimental conditions. By leveraging AI approaches, FastCat accelerates reaction space exploration, rapidly identifies optimized conditions, and generates high-quality in-house experimental data to construct digital twins of the catalytic reactions under study. Bai et al. demonstrated a closed-loop distributed SDL within The World Avatar project, aimed at creating a comprehensive digital twin based on a dynamic knowledge graph.131 This architecture utilizes ontologies to capture data and material flows in the design-make-test-analyze cycle, and employs autonomous agents to execute the experimental workflows. The authors demonstrated the framework's application by linking two robotic systems in Cambridge and Singapore for a collaborative optimization of a pharmaceutically relevant aldol condensation reaction, mapping out the Pareto front for costyield optimization within three days. The optimization was done with the TS-EMO algorithm. This setup involved flow chemistry platforms with automated liquid handling and reagent sourcing, showcasing the integration of dynamic ontological knowledge graphs to streamline and coordinate separate SDLs.",
    "4.3 Multi-Step Organic Reactions": "The synthesis of most organic molecules can hardly be achieved in a single step, and can easily require tens of steps for complex natural products. From an SDL standpoint, multi-step reactions can be approached in two distinct ways: (1) each reaction step considered and optimized separately, and the reaction product is purified and isolated before being subjected to the subsequent step; while purification, particularly in batch systems, poses significant hardware challenges, condition optimization can be performed following the approaches discussed in the previous section; (2) alternatively, all steps are run sequentially in the same batch reactor or sequential flow reactors, which is referred to as “one-pot synthesis” or “telescoped synthesis,” respectively. In the latter approach, optimization does not only become a higher-dimensional problem, but the presence of impurities and by-products can complicate the optimization of down-stream steps. For example, Coley et al. demonstrated a system with a robotic arm that can assemble the required unit operations (reactors, separators) into a continuous flow path according to the recipe, connect reagent lines, and carry out the telescoped reactions.450 Furthermore, in telescoped systems, flow rates and reaction times cannot be modified independently, posing an additional optimization constraint. This section will first summarize examples that fall under approach (1) and optimize each step individually, before discussing SDLs that feature selfoptimizing telescoped reactors (approach (2)).",
    "4.3.1.  Sequential Single-Step Optimizations":"To our knowledge, the first published example of autonomous optimization of a multi-step reaction was presented by Cortes-Borda ́ et al. in 2018, where the authors described the synthesis of the natural product Carpanone.416 For the fourstep synthesis, the authors performed four different selfoptimization campaigns, involving allylation, [3,3]-Claisen rearrangement, base-catalyzed isomerization and oxidative dimerization (Figure 21). For each campaign, up to three continuous variables, corresponding to temperature, residence time and stoichiometry/loading of one reactant species were optimized using a modified simplex algorithm. Depending on the reaction, either the HPLC or an in-line benchtop NMR spectrometer was used. Overall, the authors managed to optimize the synthesis to yield 67% of the natural product Carpanone with a total of only 66 experiments. The fact that it was manageable to conduct multiple different reactions resulting in a highly complex product on the same selfoptimizing platform demonstrates the adaptability and efficiency of such systems. A similar example of multi-step synthesis was reported by the same group in 2019, targeting the two-step synthesis of pyridine-oxazoline (PyOX) ligands (Figure 22, top panel).418 Performing two sequential optimization campaigns (three and four continuous variables, respectively) using a custom modification of the Nelder-Mead Simplex algorithm, Wimmer et al. managed to obtain a yield of 75% with only 34 experiments. Notably, the use of the flow system allowed for a significant divergence from the conditions originally reported in batch reactors: Whereas the first step of the original batch route took place at room temperature overnight, due to the thermal instability of the reaction product, the high heat transfer efficiency of flow systems allowed for shorter reaction times under thermal activation, as revealed by the sequential optimization. Transferability of the conditions was further demonstrated through the synthesis of six similar ligands, with yields ranging from 66%−92%. Related examples of multi-step synthesis SDLs were reported by Jensen and co-workers (Figure 22, middle panel), as well as Ley and co-workers (Figure 22, bottom panel). From a practical standpoint, maximizing the yield alone is not a sufficient criterion for successful synthesis�the product needs to be isolated from the crude reaction mixture in high purity, which is usually achieved through phase transfers and phase separations (e.g., extraction, filtration, chromatography). Ley and co-workers, using the LeyLab, reported the autonomous optimization of two two-step syntheses of lidocaine and bupropion, respectively, where each step was optimized separately.116 In the case of bupropion synthesis, after successful optimization of the reaction conditions for both steps, the authors demonstrated the telescoping of both steps into a single, continuous synthesis process (Figure 23). For this, the authors joined the crude product stream of the first step (bromination) with an aqueous sodium bisulfite stream to quench excess bromine. After mixing and subsequent phase separation, the organic phase was joined with the solvent stream for the second reaction (amination) before being transferred to a thin-film evaporation column, in which the dichloromethane from the first reaction step was selectively evaporated. The outflow of this evaporation column, ideally containing the purified product, was then transferred to the reactor in which the amination occurs. This discussion illustrates the hardware considerations required for successfully telescoping individually optimized reactions into a single production workflow�and showcases the existing constraints to a simultaneous optimization of telescoped reaction sequences",
    "4.3.2. Simultaneous Multi-Step Optimization": "Owing to the hardware challenges regarding purification, the first examples of telescoped reactor SDLs did not involve any purification steps, but performed the second step directly using the crude reaction mixture from the previous step. Whilst this enables the use of simpler hardware setups, it not only requires that both reaction steps are compatible with the same solvent, but also necessitates some chemical “cross-compatibility.” In other words, the first reaction step either needs to proceed in a clean fashion without producing major by-products, or the second reaction must be robust and selective enough that side products do not interfere with the desired reaction step. While telescoped syntheses had been reported in the flow chemistry literature for some time, the first examples of autonomous optimization have been described by Bedard ́ et al. in their report on the modular flow platform, as described above. In this work, the authors show the sequential combination of multiple reactor bays to a telescoped reactor system, with the addition of further reagent streams between two reactors. Using this setup, the automation of two two-step sequences is shown: a photoredox-catalyzed oxidative αfunctionalization of amines, and a Lewis-acid-catalyzed [2+2]-cycloaddition of phenylacetic acid chlorides with alkenes. In both cases, the first reaction step consists of the generation of a reactive intermediate (an iminium ion or a ketene, respectively), which is subsequently reacted with an appropriate reaction partner. A further example of a telescoped reaction was shown by Ahn et al., 426 where they conducted an ultrafast lithiumhalogen exchange reaction directly followed by an additioncyclization reaction of phenyl isocyanate. The authors designed an automated microreactor platform, which integrates a microreactor system with syringe pumps, solenoid valves, a thermostat and an in-line FT-IR spectrometer for real-time reaction monitoring. The authors use this platform to optimize the synthesis of a biologically active thioquinazolinone compound. The authors performed optimization campaigns over both only continuous (temperature, flow rate, reactor volume) as well as continuous and categorical (lithiating reagent) parameters. The BO algorithm employed by the authors achieved the same yields within 10 experiments that the authors previously found within 80 experiments of manual planning. Lastly, the authors also optimized the conditions to synthesize a library of S-benzylic thioquinazolinone derivatives. A telescoped Heck coupling of a vinyl ether, followed by selective O-deprotection, was reported by Clayton et al. in 2022 (Figure 24).433 The authors utilized a flow system combined with HPLC for quantifying the reaction yield, and a GP-based BO algorithm for iterative experiment planning. Notably, in order to obtain insights into their reaction, HPLC multi-point sampling, inspired by daisy-chaining from electrical engineering, allowed the sampling and investigating reactor outputs from both reactors separately. With this, the authors were able to uncover an alternative (but preferred) reaction mechanism for the deprotection step, which deviated from the initial working hypothesis, and turned out to be crucial for the identified deprotection conditions. This was only possible since the reaction was optimized as a telescoped process; if all of the three originally assumed steps had been optimized individually, a suboptimal process would have been found. A highly complex example of multi-step synthesis was reported by Nambiar et al. in 2022 for the synthesis of Sonidegib.430 The authors started out with the planning of the synthesis by a Computer-assisted Synthesis Planning (CASP) algorithm, which proposed a two-step route, consisting of an SNAr reaction and an amide coupling reaction. Due to unfavorable electronics in the SNAr step, the authors opted to synthesize the product via a three-step route, consisting of an SNAr reaction, a nitro reduction and an amide coupling (Figure 25). The reactions were carried out in a robotically reconfigurable continuous-flow synthesis platform that allowed for the exchange of different modules by a robotic arm. As analytical modules, FT-IR and LC-MS were integrated to allow for monitoring reactor outputs. In their optimization, the authors considered a series of continuous parameters, including reaction times and stoichiometries, as well as multiple categorical parameters, such as the leaving group for the SNAr reaction, the identity of the amide coupling reagent, or the reactor size for the last reaction step. Their modular platform allowed the robot to exchange the reactor, which in turn enabled the researcher to alleviate constraints on the interdependencies on residence times due to the flow rate of earlier steps. The computer-proposed and human-refined synthesis pathway was subsequently attempted to be optimized in one telescoped reaction. In preliminary experiments, the LC-MS module after the first reaction showed that the SNAr reaction proceeded with > 80% yield, however the FT-IR module after the nitro reduction revealed catalyst deactivation. Further experiments showed that this deactivation was caused by a byproduct of the SNAr reaction, rendering a fully telescoped process without thorough intermediate purification impossible. Thus, the authors decided to run the first reaction separately, and subsequently perform a telescoped reaction for the last two stages. As a consequence, the SNAr reaction was run as a multiobjective optimization campaign, optimizing the yield, productivity and cost with respect to temperature, residence time, stoichiometry of reagent and base, as well as the leaving group as a categorical parameter. Optimal conditions were found in thirty experiments over 10 hours, with the algorithm providing multiple Pareto optimal points. The offline-purified product was subsequently used as a starting material for the telescoped reaction towards Sonidegib, optimizing yield and productivity simultaneously. Optimal conditions were found after fifteen experiments and 13 hours with a total yield of > 90%. The above-mentioned SDL example reflects the challenges in fully autonomous, self-driving systems for organic synthesis particularly well. On the hardware side, telescoping multiple reaction steps offers a highly attractive solution to operating complex multi-step synthesis in a continuous fashion. However, generalizability of this strategy requires the development of advanced purification modules to minimize undesired cross-influences between individual reaction steps, e.g., to remove side products, or to enable solvent exchange. From an analytical standpoint, the introduction of automated reaction monitoring systems at multiple stages of the process provides access to important data that, in turn, can enable invaluable insights into the reaction progress and potential failure modes.451 At the same time, the fully automated interpretation of this data, as well as downstream open-ended decisionmaking, usually require large degrees of expert knowledge, laboratory experience and adaptive decision-making (often referred to as “chemical intuition”). This applies to the integration of automated algorithms for synthesis planning in particular, where, at the current stage, human decision-making is required for ranking routes or identifying reasonable condition search spaces. Integration of these advanced, and often open-ended, decision-making capabilities into AI systems represents an active challenge for the field and leaves room for future developments towards true, reliable SDLs for smallmolecule synthesis.",
    "4.4. Further Solution-Phase Reactions":"The concepts discussed above can readily be translated to synthetic chemistry domains beyond traditional small-molecule synthesis. Importantly, many polymers�with numerous applications in plastics, fibers, electronics, or drug delivery (materials-focused SDLs are discussed in later sections)�are synthesized in solution-phase processes, which makes these amenable to self-optimization. The major distinction to the previous discussions of small-molecule synthesis is the analytical methodology. Whereas for small molecules, a single, well-defined molecular entity needs to be determined in a quantitative fashion, the quantification of a “polymer yield” is less straightforward; in addition to the amount of formed polymer, the targeted size distribution, degree of (co- )polymerization, or other physical properties need to be controlled, which leads to a greater variability in the analytical methods and the resulting optimization objective. One of the earliest examples of polymer SDL was performed in 2002. Vieira et al. demonstrated the closed-loop optimization of molecular weight and composition for copolymer latex.452 Using a series of pumps and agitators, the authors automated emulsion polymerization, in which the monomers are dispersed as tiny droplets in aqueous phase, with emulsifiers and stabilizers to initiate and terminate polymerization, respectively. The copolymers were characterized by a near infrared spectroscopy (NIRS) probe of the solution, detecting the monomer concentration, polymer holdup, and the mean polymer size through the use of the partial least squares (PLS) model.453 The goal was then to minimize the fitness, a weighted sum of differences between the desired and current molecular weights, over the feed rates of precursors, which was done using the iterative dynamic programming (IDP) method.454,455 IDP considers discrete time intervals of previous iterations, and adjusts the flow rates to drive the system toward the desired synthesized polymers. Houben et al. performed similar experiments with the use of multi-objective ML techniques to optimize the recipes of emulsion copolymerization reactions.456 The authors used a setup similar to the one described before, however the analysis of particle sizes and conversion rates were done off-line, using dynamic light scattering and chromatography, respectively. Rather than only considering the flow rates, the 12 other experimental parameters were also varied. After each iteration of experimentation, the results were fed into the multiobjective active learner (MOAL) algorithm, with suggestions produced by a GA, and predictions generated from a GP model.457 Starting with 5 random initial experiments, and 15 additional experiments guided by MOAL, the authors found the conditions needed to produce high conversion polymers with particle sizes of 10 nm. Rubens et al. used continuous flow microreactors, rather than batch reactors, to develop an SDL capable of highthroughput synthesis of reversible addition fragmentation chain transfer (RAFT)458 polymers with precise molecular weights.459 The polymer from the flow reactors were then characterized in situ by size exclusion chromatography (SEC), measuring the molecular weight, and dispersity of the polymers. The results were then fitted using a linear regression model with the results at each iteration. The flow rates with the best predicted results were then used for the next iterations. Most recently, Knox et al. studied the same RAFT polymers with an SDL guided by BO, with the temperature and residence time as continuous optimization parameters.460 Furthermore, the automated characterization techniques included both an in-line chromatography and an online NMR spectrometer. Using TS-EMO with a GP regressor surrogate, the authors were able to map out the Pareto-front for the polymer conversion and molar mass dispersion with higher resolution when compared to DoE. The BO iterations would suggest the next experimental parameters: the temperature and the residence time of the reactor.",
    "4.5. Cataöyst amd Reactopm Discovery":"",
    "4.5.1. New Catalyst Materials":"Beyond the optimization of reaction conditions for a specific synthesis process, the discovery of novel highly active catalysts can allow for novel and more efficient synthetic processes, and can open up new production avenues. While a catalyst is formally defined as a species that accelerates a given reaction, in reality, catalysis enables reactions that would otherwise only occur under impossible conditions. As such, catalysis has an enormous economic value, and it is assumed that >80% of all synthetic consumer products have gone through at least one catalytic process in their production. At the same time, discovering new catalysts is a considerable challenge, since their design requires the knowledge of a series of reaction pathways and modes of action, which also makes it extremely difficult to simulate catalytic efficiency from first principles. As a result, the last century has mainly seen empirically or heuristically driven campaigns for catalyst discovery. One of the most prominent examples is Mittasch’s large-scale screening for heterogeneous catalysts for the Haber-Bosch process,21 where they empirically test over 4000 possible catalysts�yielding an optimal catalyst that is still used as of today in almost unaltered form. More recently, Lai et al. demonstrated a LLM capable of suggesting catalyst synthesis conditions, drawing from the decades of results in the scientific literature.461 Major challenges in automating such a discovery process, and implementing it into an SDL, stem from the requirement to first synthesize and purify the catalyst candidate, which can involve a series of intricate experimental steps, and subsequently evaluate its activity in the catalytic reaction of interest. This challenge is illustrated in a pioneering example from Corma et al., who tackled the challenge of identifying heterogeneous titanium silicate catalysts for olefin epoxidation.462 Here, the catalyst synthesis alone involves gel formation from all involved reagents, followed by hydrothermal crystallization and post-synthesis treatment. The authors use a sophisticated robotic setup to automate these steps. The efficacy of the newly synthesized catalyst in the epoxidation of cyclohexene with a peroxide oxidant is then evaluated in a parallel batch reactor, which is coupled to ultrafast GC for on-line analysis. Even though the authors demonstrate an advanced level of automation (especially given that the work was published in 2005), transfer of samples between the workstations required a human experimentalist. Experiment planning is performed through a GA,463,464 enhanced by a neural network for applying a selection pressure on the newly proposed candidate generation, to optimize the quantities of four catalyst ingredients. The authors demonstrate three generations of 21 experiments each, and show the discovery of two new families of catalysts with improved activity, which are structurally characterized in detail. In 2010, Kreutz et al. reported an SDL for homogeneous catalyst discovery for the partial oxidation of methane with molecular oxygen. 465 The catalytic system, which can be prepared by mixing all ingredients in an aqueous solution, is composed of three components: the active metal, a co-catalyst, and a ligand. These three categorical variables are optimized through a GA. To perform the required experiments, the authors have developed a sophisticated experimental setup based on droplet-flow reactors (Figure 26). Solutions containing the different catalyst compositions are prepared in 96-well plates, and are then injected into a microfluidic reactor. Both methane and oxygen are added by diffusion through the teflon walls of the flow reactor. The formed methanol was quantified by diffusion into neighboring microdroplets that contained a methanol-selective indicator, thereby allowing for semiquantitative analysis using UV-Vis spectroscopy. Per generation, the authors performed 48 experiments (in quadruplicate), and demonstrated that over 8 generations, a significant improvement in methanol formation (up to 3-fold increased catalytic activity) can be obtained. Zhu et al. reported an autonomous system for discovering catalysts for the electrochemical oxygen evolution reaction (OER) from Martian meteorites, simulating the development of an oxygen-generating system on Mars.466 For this purpose, the authors demonstrate a complex synergistic workflow consisting of multiple experimental and computational components. By analyzing the available Martian ores through automated atomic emission spectroscopy, the available elements�and therefore, the accessible materials search space�are defined autonomously by the platform. Within this search space (>3 million combinations of 6 metals in discrete quantity steps), a diverse set of ∼30,000 possible catalyst compositions are first screened computationally, by molecular dynamics and DFT calculations. This data is used to train a surrogate neural network model for the computed catalytic properties as a function of the elemental composition. These computed properties, together with the elemental composition, are then used as inputs to a second neural network for predicting the experimental catalytic activity. The latter network was trained on a small seed dataset of < 300 experiments, which were conducted in a fully automated fashion using a robotic arm operating multiple workstations for dissolving the raw ores, creating reagent stock solutions, precipitating, drying and formulating the catalysts, and determining their catalytic activity in an electrochemical measurement. The authors then performed virtual BO within the entire search space, using the predictions of the trained neural network as their objective, and validated that the identified best candidate indeed outperforms all previously obtained catalysts. Even though the authors do not demonstrate multiple iterations of closed-loop of experiments and data-driven decision-making, the computational definition of the search space, as well as the advanced automation workflows are remarkable�making this work a Level 3 SDL, as by the definition of Figure 1. In 2024, Ramirez et al. demonstrated the optimization of a heterogeneous catalyst for the reduction of CO2 using BO.467 As a catalyst, the authors explored systems containing up to three metals among iron, cobalt, copper, zinc, iridium and cerium with a maximum loading of 5 wt%. Additionally, the algorithm could choose between the presence or absence of potassium as a promoter, the amount of water as solvent as well as having silica, alumina, titania or zirconia as support. The authors synthesized 144 catalysts over six generations. Over the performed experiments, the BO algorithm was able to identify a catalytic system that maximizes the CO2 conversion and MeOH selectivity while minimizing the CH4 selectivity and the cost, where the latter was only considered throughout five generations to demonstrate the adaptability of the algorithm. Even though the algorithm is capable of finding a performant catalyst, the authors point out that this is performed within a well-studied and expert-restricted chemical space, demonstrating the hurdles for autonomous novel catalyst discovery. The discussed works showcase examples of how catalyst discovery could be addressed in a closed-loop fashion� provided that the search space is sufficiently narrow, and the experiments can be automated in a useful manner. Particularly catalyst synthesis poses a major challenge in this regard; the diversity of catalyst space, and the fine nuances that can influence catalytic activity, however, render the development of generalizable automation schemes difficult. In homogeneous catalysis, making new catalysts requires synthesizing new molecular species, which usually require multi-step reaction and purification sequences, which, in turn, we had previously identified as a major challenge for automation. On a purely computational level, this bottleneck can be circumvented, which has led to impressive and experimentally validated examples of closed-loop catalyst design, for example in organocatalysis.468 In heterogeneous catalysis, on the other hand, synthesis requires intricate thermal treatment and annealing steps, which possess inherent automation constraints, and can often lead to structurally ill-defined materials, adding further complexity to the data-driven prediction problem. As a consequence, the last decade has produced rare examples of true SDLs for catalyst discovery, which remains a grand challenge for autonomous discovery, both from the software and the hardware standpoint.",
    "4.5.2. New Reactions and Reaction Types":"While all previous discussions have focused on specific reactions�the product (and reactants) are given, and the goal is to find the catalysts, reactants, reagents or reaction conditions that maximize the product quantity�SDLs can also be used to search for new reactions or products.469 In fact, this problem of discovering new reactivity or catalytic activity has been an active field of research in organic chemistry for more than a century. While the predominant search strategy in this field has been rational design, the importance of “serendipitous” discoveries has been emphasized numerous times.470 As an example from an SDL, Amara et al. reported the detection of an unexpected side product when attempting the selfoptimization of a γ-Al2O3-catalyzed methylation in supercritical CO2 (for a more detailed discussion of these reactions and the self-optimization algorithms used, see the section on Selfoptimizing flow reactors).471 Careful characterization of the side product by a human researcher allowed for its unambiguous identification, and a second closed-loop campaign towards the yield of this side product was carried out, which eventually resulted in the discovery of optimized conditions for a new reaction type. This example demonstrates the possibility of discovering new reactions through SDLs. At the same time, especially from the standpoint of automation and experiment planning, it poses the open-ended analytical challenge of detecting and identifying newly formed, unknown reaction products from a crude reaction mixture, which is often addressed by analyzing changes in the bulk properties of the reaction mixture (UV-Vis spectra, IR spectra, NMR spectra), or through coupled separation−detection techniques (GC- or HPLC-MS). Early examples in the field of “untargeted” reaction discovery have focused on non-iterative screening campaigns using combinatorial chemistry and HTE , which have been reviewed elsewhere.472,473 The first example of a truly closed-loop campaign for discovering new reactivity was reported by Cronin and co-workers in 2018, who developed an SDL for finding new two- or three-component reactions in a pool of reactants (Figure 27).474 In a proof-of-concept work, Granda et al. selected a set of 18 reactants with diverse functional groups, which can be reacted under fixed reaction conditions in a fully automated fashion. Crude reaction mixtures were analyzed by automated IR and 1 H-NMR spectroscopy, and the spectra, along with the spectra of the starting materials, were processed by a pre-trained SVM classifier to label the reaction as “reactive” or “non-reactive”. Based on this data, a linear discriminant analysis (LDA) model was trained to predict reactivity across the entire search space, and new experiments were selected in a fully exploitative fashion. With this search strategy, the authors demonstrate a significantly improved hit rate compared to trivial random search algorithms, and report a series of nontrivial reactions which had not been published before. Later work from the same group, Caramelli et al. used a similar platform to discover new unreported reactions in an automated fashion:475 the photochemical reaction of phenyl hydrazine and bromoacetonitrile, and the reaction of ptoluenesulfonylmethyl isocyanide (TosMIC) and diethyl bromomalonate. For decision-making, Reactify is a CNN that is trained on the NMR spectral data of 440 reactions with reactivity classified by a chemist. A neural network, using the junction-tree VAE embedding of the molecules as features, is trained to then suggest new reactants for the SDL platform. Both the Reactify and the surrogate neural networks were retrained at each iteration. The reaction mechanisms of the novel reactions were further studied by the authors. In related work, Mehr et al. demonstrated a probabilistic approach to reaction discovery, both in silico and as part of an SDL.476 Reactants were assigned prior distributions which were then combined to form a joint probability prediction of the reactivity between them. Following Bayes’ theorem, the distributions were updated based on the feedback results of an automated HTE platform. The experiments were carried out in a flow-based system, with on-line NMR, HPLC, and MS characterization. The authors were able to rediscover known reactions such as the Buchwald-Hartwig amination, and the Wittig-Horner reactions. The question of whether an identified reaction can be considered “novel” has been subject of an ongoing debate. While the previously unknown formation of a reaction product�the definition used by Granda et al. and Caramelli et al.�clearly constitutes a new reaction, the term novelty lacks an unambiguous definition. In both studies, the authors were maximizing the reactivity, or rather, maximizing the number of reactions classified as reactive. Any SDL targeting the discovery of novel reactions therefore requires a series of assumptions and simplifications for defining the optimization objective. Porwol et al. later applied a similar discovery strategy for finding new polyoxometalate clusters composed of metal ions and bridging ligands.477 Following in situ assembly of the ligands in a threecomponent coupling, a metal precursor is added to form potential polyoxometalates. By using a series of characterization techniques including UV-Vis spectroscopy, MS, and pH measurements, novelty is measured as the cumulative difference between the data of starting materials and products, respectively. As independent variables, the authors selected the ligand precursor identities, metal ion, reagent volumes, reaction temperature and reaction time. To maximize the novelty, the authors used a custom surrogate-free search algorithm, which samples each experiment in a given distance from the previous experiment, depending on the novelty of the previous experiment. Following this strategy, the authors discovered a range of new polyoxometalate clusters. This is discussed further in the section on state materials synthesis optimization.",
    "4.6. Determination of Reaction Kinetics":"Especially on the process chemistry level, knowledge about the reaction conditions that lead to optimized reaction yields is not sufficient for safe and reliable reactor operation. In these contexts, detailed information about the kinetics of a reaction is required in order to predict and adjust the behavior of a reactor system. At the same time, kinetic knowledge can enable important insights into the mechanism of a reaction�which is of high relevance for informed decision-making, both at the discovery and at the process stage. SDLs can, and have been, used to iteratively acquire kinetic data, refine kinetic hypotheses, and eventually obtain reliable kinetic models. This has, in a simple proof-of-concept study, already been demonstrated in the late 1970s in the context of derivatization reactions for analytical chemistry.478 Decades later, in 2011, McMullen and Jensen utilized a microfluidic system to optimize the parameters of a kinetics model for the Diels-Alder reaction of isoprene with maleic anhydride.479 Following the Box and Hill method, the probability of a particular rate model describing an experiment can be formulated in a Bayesian context by a posterior probability function based on the experimental conditions and the outcome concentration.480 Using an in-line HPLC, the microfluidic system returns the output concentration of isoprene, which is used to update the distribution until a predefined probability threshold is met. After deciding the rate law, the microreactor was then used to optimize the parameters of the rate constant through plug-flow reactor kinetics. Finally, for validation, the authors performed 4 additional experiments and found good agreement with predictions from the optimized rate law. In a related study, Reizman and Jensen presented a continuous-flow SDL for studying multi-step reaction kinetics.481 Using high-throughput synthesis methods enabled by flow reactors, the authors studied the conversion of 2,4- dichloropyrimidine to 4,4′-(2,4-pyrimidinediyl)bis-morpholine. There are two reaction pathways, each with two reactions, which are all modeled as second-order bi-molecular reactions. The product concentrations were measured after the reaction by online HPLC, and the kinetic model parameters were leastsquares fit to the results. The sensitivity coefficients, a measure of how sensitive the predicted concentrations are to the synthesis parameters, were then calculated for the optimal parameters. By minimizing the sensitivity coefficient, the next experimental conditions were generated, and the reaction kinetic models’ parameters were iteratively optimized. Most recently, Sheng et al. applied a closed-loop SDL to study the electrochemical reaction of cobalt tetraphenylporphyrin (CoTPP) with organohalides.482 The electrochemical platform uses a flow system to control the flow of reactants into a 3-electrode cell, which is monitored by a potentiostat for cyclic voltammetry (CV). The platform first identified reactions which can be modeled by the EC mechanism, which consists of an electron transfer step followed by a solution reaction. This was done by analyzing the CV data with a ResNet CNN previously trained to extract relevant electrochemical quantities.483 In the second stage, the EC mechanism is probed by optimizing the rate constant (k0) of the solution reaction step as a function of the voltammetric scan rate, and the organohalide concentration. Both stages were guided by a Bayesian optimizer from Dragonfly.",
    "4.7. Solid State Materials Synthesis":"Solid state materials, such as molecular crystals, zeolites, metalorganic frameworks (MOFs), covalent organic frameworks (COFs), polyoxometalates and alloys, have a variety of applications, particularly in catalysis of reactions. Porous materials like molecular crystals, MOFs, COFs, and zeolites are characterized by voids in the crystalline structure, typically on the nanometer to micrometer scale, and high surface areas, giving the material the ability to adsorb molecules for storage or catalysis. This has applications in gas storage and separation (i.e., methane, hydrogen gas), filtration, and drug delivery.484,485 While there are many SDLs focused on optimizing the function of solid state materials, the SDLs discussed in this section are focused on finding optimal synthesis conditions for the structure and crystallinity of the material. For SDLs related to energy storage and optoelectronic applications, we refer the reader to the respective sections. The primary advantage of solid state materials is their tunability beyond the chemical component; by varying both the composition and synthesis parameters, the material properties and structure can be tuned for specific applications. Considering the space of possible materials and structures is intractably large, traditional approaches based on manual synthesis are insufficient to explore these materials efficiently. A number of high-throughput methods, both computational,373,486−489 and experimental,490−492 have been developed and successfully applied to combinatorially exploring the material space. These have resulted in large datasets of possible structures and materials, as well as their measured or predicted properties, paving the way for data-driven strategies.151,493,494 Conventional data-driven applications of these highthroughput methods are through compound screening: a predefined space of compounds and structures are filtered down based on predictions from statistical models trained on the datasets, or theoretical calculations.495,496 There are extensive works in the literature completely within the computational domain: developing descriptors and models that can predict the properties of interest from the datasets,486,497−499 and extending these models for computational SDLs, performing active learning campaign based on in silico model predictions from models trained on high-throughput experimental or computational results.500,501 In the synthesis of zeolites, Moliner et al. utilized a highthroughput robotic arm platform capable of liquid/solid handling, stirring, and crystallization to generate a combinatorial DoE study of 144 triethylamine:SiO2:Na2O:Al2O3:H2O zeolites.491 Using the MLP model, the authors were able to attain better predictions of the crystallinity of the zeolites from the experimental dataset than the typical multivariable quadratic models. The crystallinity is measured via XRD: the spectral peaks are fitted with Gaussian functions, and the average full-width half-maximum (FWHM) of the peaks are used as a measure of crystallinity. In a related study, Corma et al. performed a similar study for SiO2:GeO2:Al2O3:F−:H2O:4- (2-methane sulfonylphenyl)-1,2,3,6-tetrahydropyridine hydrochloride zeolites, which have demonstrated successful crystallization into ITQ-21 and ITQ-31 zeolites.502 The authors improved the crystallinity predictions by including structural descriptors derived from the XRD spectra, along with the synthesis descriptors, in the MLP neural network input. Nikolaev et al. demonstrated an Autonomous Research System (ARES)503 capable of autonomously conducting iterative materials experiments to study carbon nanotube (CNT) synthesis�a pioneering example of an autonomous SDL for materials research. Experiments were conducted by heating catalyst-coated silicon pillars, which each serve as CNT microreactors, with a laser while varying growth parameters like temperature, pressure, and gas composition. Raman spectroscopy measured the CNT growth rate in real-time. Using linear regression models, the authors were able to map out the effect of experimental conditions on the resulting growth of single-wall or multi-wall CNTs.504 In a later study, the same system was providing feedback to a RF model and genetic algorithm to propose new experimental conditions. Over hundreds of closed-loop iterations with minimal human intervention, ARES successfully learned to grow CNTs at targeted growth rates by optimizing the multi-dimensional parameter space. More recent work from the group modified ARES to use BO with GP surrogates for the maximization of CNT growth rate.505 These demonstrations showcase ARES's ability to autonomously navigate complex experimental domains and obtain insights into growth kinetics, which is valuable for controlled nanotube synthesis. As one of the first implementations of SDL for materials science, this work highlights the potential of autonomous research systems to accelerate the scientific understanding and development of complex functional materials. Similar ML directed discovery have been demonstrated in the synthesis of MOFs, for example Raccuglia et al. further incorporated reactant and reaction descriptors in the prediction of successful synthesis and crystallization of organic templated vanadium selenite materials.506 Training a SVM on experimental results from both failed and successful reactions, and comparing the recommended reactions from a human chemist, the model was shown to have a higher success rate and provide more diverse reactions. More recently, Xie et al. utilized XGBoost, a gradient boosting tree-based model, to determine the reaction parameters for crystallization of metalorganic nano-crystals.507 To test their model, validation experiments not found in the training set were conducted to demonstrate the use of the XGBoost model for extrapolating to new MOF nano-crystals. Luo et al. later developed the MOF Synthesis Prediction tool, using natural language processing DL models to extract synthesis conditions of MOFs from the literature and create a dataset and prediction tool for synthesizing new MOFs.508 The experiments in these works were not conducted in an automated fashion. The earliest examples of closed-loop SDLs for solid state materials were by Corma et al. in 2005, previously discussed in greater detail in the section on catalyst discovery. The authors were interested in optimizing the catalysis of olefin using a Ti-based zeolite catalyst.462 Various concentrations of hydroxide, titanium and surfactants were combined in the hydrothermal synthesis of the zeolite using a robotics system. The batches of zeolites were then tested for catalytic activity using ultrafast GC. In the development of MOFs, Moosavi et al. developed an SDL that optimizes the crystallinity of the HKUST-1, first synthesized by Chiu et al. 509 at the Hong Kong University of Science and Technology.510 The synthesis was performed using a high-throughput robotic platform, capable of handling and stirring reactants, transferring the samples into a microwave reactor cavity for synthesis and to a powder X-ray diffractometer for crystallinity measurement. The exploration of the parameter space was done using a GA dubbed the SyCoFinder, over the course of three generations, with 30 synthesis conditions tested in each. Similar to previous work,506 results from successful and failed experiments were collected in order to train a RF model to identify the synthesis parameters of importance. By weighting the 9 dimensional parameter space by the identified importance, the parameter space becomes smaller and more confined, allowing for more efficient exploration guided by chemical intuition. Further optimizations were not performed. Xie et al. performed a similar analysis with the zeolite imidazolate framework (ZIF), ZIF-67.511 They developed a new ZIF synthesis protocol based on a custom low-cost gantrystyle robot SDL platform that injects precursors onto laserinduced graphene microreactors fabricated on a thin film (Figure 28).512 The microreactors were then Joule-heated to create ZIF-67 in a high-throughput manner. The synthesized samples were transferred for XRD characterization. Rather than using a GA in the experiment planning, the authors used BO with a RF surrogate model. For the synthesis, the molar ratio of metal ions to organic molecules, the volume of precursors, the applied DC voltage, and the heating duration was varied. After an initial 12 random samples, three additional generations with 12 samples each were suggested by the BO algorithm using the expected improvement acquisition function. Figure 28 shows the improvement in the crystallinity as a function of BO iterations. Extending into thin films of MOFs, Pilz et al. developed an SDL optimizing surface anchored MOFs that are formed layerby-layer.513 Like previous work,510 the authors used the SyCoFinder GA for synthesis planning, with the goal of optimizing multiple objectives: the crystallinity, the [111]- orientation of the crystal, and the phase purity, all of which are measured from the XRD spectra. The objectives are combined with a summation and then normalized to a fitness between 0 and 1. The parameter space included the metal and linker concentrations, the amount of water, and cleaning time via sonication and spray cleaning. The samples were transferred across the various modules via a 6-axis robotic arm. The SDL started with a diverse random set, and two more generations were carried out, with increasing fitness found with subsequent generations. Harris et al. demonstrate an autonomous synthesis platform for pulsed laser deposition (PLD) of thin films by combining real-time diagnostics, automated synthesis and characterization, and ML algorithms. The platform utilizes GP regression and BO to autonomously explore a 4D parameter space of background pressure, substrate temperature, and laser fluences on two targets, tungsten and selenium, aiming to optimize the crystallinity of WSe2 thin films based on in situ Raman spectroscopy feedback�sharper peaks indicate higher crystallinity. Having only sampled 0.25% of the parameter space, the autonomous workflow discovered two distinct growth windows and mapped the process-property relationships governing film quality. Notably, the automation achieved at least a 10-fold increase in throughput compared to traditional manual PLD workflows. The combination in situ Raman spectroscopy monitoring, and ML driven decisionmaking can be used for PLD fabrication of other solid state thin film systems. Duros et al. studied the crystallization of a new polyoxometalate structure with an SDL driven by active learning, and also provided a comparison with random and human-guided experimental planning.514 A series of syringe pumps fed aqueous precursor solutions into a reactor, and the products were visually inspected for crystallization. The platform was capable of performing batches of 10 crystallization experiments per day, and an initial dataset of 89 points was acquired to start as a training set. A SVM classifier was trained on this dataset to classify successful crystallization experiments. The subsequent experiments were then conducted using an active learning loop, with the goal of maximizing the number of polyoxometalate structures and the explored synthesis parameter space. When compared to human and random exploration of the space, the SDL explored more of crystallization space, while still finding a similar number of crystallization points as human decision (Figure 29). Beyond MOFs and COFs, van der Waals superlattices�i.e., stacks of graphene-like atomic monolayers bound through dispersion interactions�have emerged as an attractive class of 2D crystals with multiple applications in e.g., semi- and superconductors, or topological insulation. The layer-by-layer assembly of these materials could allow precise control over materials properties, but requires delicate physical handling. As an important step towards SDLs for van der Waals superlattices, Masubuchi et al. developed a multi-step robotic workflow: in the first step, pre-synthesized 2D crystals deposited on Si chips are automatically detected and characterized using optical microscopy and computer vision.515 Subsequently, the detected crystals are robotically transferred to a stamping apparatus, aligned and assembled to the desired superlattice. While this work does employ iterative data-driven decision-making, the advanced automation and computer vision approaches can justify the classification as a Level 3 SDL, laying the foundation for autonomous materials discovery for van der Waals superlattices. Kusne et al. developed CAMEO for the self-driven discovery of phase-change memory (PCM) materials.516 These are inorganic materials capable of switching between amorphous and crystalline states, altering the optical and electrical properties of the material. CAMEO uses a physics-guided ML model for BO of Ge-Sb-Te ternary PCM. Synthesis was not part of the design process; rather, a combinatorial library of Ge-Sb-Te material was loaded onto the system, along with data from DFT simulations. Because the target property was dependent on the phase of the material, the first iterations maximize the phase map of the material. After some defined threshold for phase map exploration, the BO algorithm, based the predictions GP models with an UCB acquisition function modified with an additional term based on the distance from the phase boundary, selected the next material for automatic synchrotron XRD characterization and human-in-the-loop evaluation of the optical gap. While not fully automated, the authors were able to discover a new photonic PCM with an optical gap difference between crystalline and amorphous phases of 0.76 ± 0.03 eV, over three times larger than the conventional GST225 material. In the quest to understand the phases of specific solid state inorganic materials, Ament et al. demonstrated a self-driven high-throughput platform for determining the phase boundaries of Bi2O3 system.517 Bi was sputtered in an atmosphere of Ar and O2 onto Si wafers to create thin-films of Bi2O3, which were annealed in stripes using a laser. By varying the annealing temperature and time, different phases of Bi2O3 can be observed. The samples were characterized by optical microscopy and reflectance spectroscopy to determine the phase boundaries, and the next conditions are suggested by GP models with custom kernels based on the physics of the experiment; the algorithm was dubbed Scientific Autonomous Reasoning Agent (SARA). The authors were able to map the phase boundaries of the system two orders of magnitude faster than random or exhaustive search methods. A major obstacle to the development of a fully automated SDL for solid state materials is the need for powder handling and XRD characterization. Lunt et al. developed the PowderBot, an autonomous robot capable system capable of automated Powder XRD.10 Powder-Bot successfully synthesized molecular crystals using a Chemspeed liquid-handling platform. A single-arm mobile robotic manipulator transfers the crystalline material to a grinding station where a dual-arm stationary robot produces the powder, and then takes the powder XRD samples to a diffractometer for analysis, totaling thirteen distinct steps. The manipulator operates the diffractometer as a human chemist would, and the XRD spectra is recorded. While the work is not a true closed-loop SDL due to the lack of intelligent experimental design, the authors demonstrated a landmark single iteration of automated synthesis and powder XRD characterization using conventional processing and characterization equipment. In another notable advancement, Chen et al. present ASTRAL, a robotic platform that seamlessly integrates powder-precursor synthesis including powder dispensing, ball milling and oven-firing into XRD characterization of reaction products.518 Most recently, Szymanski et al. presented A-Lab,519 an SDL for solid state synthesis of metal oxides and phosphate powders, with fully automated sample preparation, heating, and XRD characterization capabilities. Solid state synthesis pathways were selected using the ML-based precursor selecting algorithm ARROWS3 , which incorporates decomposition energies from both ab initio calculations and previous experimental outcomes to find the best reaction pathways.520 Air-stable synthesis targets were identified based on ab initio calculations from the Materials Project, and a dataset from Google DeepMind.521 Recipes obtained from text-mining sources in the literature were used to train ML models to generate recipes for compounds not found in the training dataset. We note that this is an unguided systematic search of the proposed synthesis routes; however, if these recipes fail to produce high enough yields (> 50%), A-Lab defaults to the ARROWS3 algorithm, which utilizes information from prior experimental results. The autonomous platform then carries out the recipe, performing dosing, syntheses, and analysis on three different stations, with a robotic arm transporting the sample between stations. The collected XRD spectra were analyzed using a probabilistic ML model trained on the ICSD, as discussed previously in Analytical Process Optimization. The resulting weight fractions of the synthesis products were fed back into the orchestrator of A-Lab to inform further experimentation. Over the course of 17 days of continuous experimentation and 355 experiments, A-Lab successfully synthesized 41 out of 58 target compounds, of which 9 of the targets were optimized by the data-driven ARROWS3 algorithm for improved yields. The authors further claim the discovery of multiple new compounds and structures, although this has been called into question due to the non-standard analysis of XRD results, and the under-characterization of the compounds.522,523 Still, the A-Lab has demonstrated advancements in the development of inorganic solid state SDLs. These examples represent significant steps towards accelerating the discovery of feasible solid-state materials in a design space that contains a large fraction of unstable and metastable materials, and closing the automation design loop for arguably the most difficult-to-automate piece. We expect these endeavors will set precedents for application-driven, inorganic solid-state SDLs to come.",
    "4.8. Outlook and Perspectives":"Within this chapter, we have provided a comprehensive overview of SDLs for chemical reaction optimization, which has arguably been the most widespread application of SDLs as per definition of this review. While first, foundational examples of autonomous reaction optimization have been laid in the 1980s, the field has seen an enormous boost in the 21st century, owing to advances in digitization, computational resources and software distribution. The largest body of work has focused on the autonomous optimization of single-step reactions in solution. Notable examples include: heterogeneous catalysis, photochemical reactions and photocatalysis, nanoparticle catalysis, the use of supercritical fluids as reaction solvents, and many others. These works have also led to notable automation and advances in related disciplines of modern synthesis, including catalytic technologies like electrocatalysis524,525 and organocatalysis,526 or economically important applications like biomass or waste valorization.527 We expect to see pioneering examples of SDLs in these fields in the years to come, leading to a further diversification of SDLs for chemical reaction optimization. Importantly, optimization campaigns have not been limited to maximize the yield of a chemical reaction, but have been extended to economic considerations (e.g., time, cost, and produced waste), kinetic information, or the information content of the obtained reaction data.528 It is important to note that all of these works have relied on two main pillars. First, the availability of open-source solutions for both automated reaction hardware and optimization software has enabled the implementation of autonomous systems across a variety of labs, and has proven to be a (figurative) catalyst for the spread of SDLs. We highly advocate for such open-source initiatives�accessible solutions (such as EDBO+ platform from the Doyle group295) have shown to serve as inspiration for further groups to adopt important SDL technologies.529 Secondly, domain expertise and laboratory experience has been instrumental to set up the required hardware and, more importantly, define and constrain the experimental search problem. The use of AI for those open-ended decision-making tasks represents an important open challenge to the community, in addition to adaptive decision-making in synthetic laboratory scenarios. These software requirements go hand in hand with the development of flexible, reconfigurable hardware systems that enable such adaptive operations. Addressing these challenges, as discussed in detail throughout this chapter of the review, can build the foundation for the next generation of SDLs for chemical synthesis, and eventually bring us one step closer to the dream of autonomously synthesizing any molecule (or material) on-demand. As such, autonomous synthesis can be an integral component of any autonomous materials discovery initiative, including the efforts detailed in the following sections.",
    "5. Drug Discovery and Biochemistry":"Drug discovery plays a pivotal role in modern society and in the chemical industry, not only as a major consumer of chemical compounds, but also as a driving force behind chemical innovations: indeed, the pharmaceutical industry invests billions in research and development (R&D) every year,530 and some of the first examples of automated and highthroughput experiments were first developed by pharmaceutical companies. The reason behind this huge investment is the high cost associated with drug development: it usually takes US$2.6 billion and 10 years to put a single drug on the market.531 This long and costly pipeline can be roughly split into five main stages: early-stage discovery, preclinical studies, clinical trials, FDA review and approval and finally, post-market monitoring. Early-stage discovery includes disease-related proteins target identification, compound screening against selected target, assay development and compound property optimization. Preclinical studies focus on drug profiling, delivery and dose range finding. However, while the R&D budget increases over the years, the composite average approval rate of drugs keeps falling down.530 Analyses of clinical trial data from 2010 to 2017 show four possible reasons attributed to 90% of the clinical failures of drug development: (i) lack of clinical efficacy (40%−50%), (ii) unmanageable toxicity (30%), (iii) poor drug-like properties (10%−15%), and (iv) lack of commercial needs and poor strategic planning (10%).532 Given those statistics, it is apparent that success in early-stage discovery and preclinical studies stages is key to overcoming the high attrition rate. In those stages, researchers are confronted with multi-objective optimization problems that span the chemical and biological space. Not only are those vast, but the understanding of them is also incomplete. For efficient exploration, the pharmaceutical industry has thought to employ automation relatively early compared to other industries:533 Automation in drug discovery dates back to the 1980s with the advent of high-throughput screening platforms, which leverage robotics to manage the handling of thousands of bioassays.534 Spurred by large investments by pharmaceutical companies, robotic drug discovery platforms have evolved towards a higher level of automation and complexity. A notable example is Eli Lilly’s state-of-the-art automated synthesis laboratory, among others.535 Along with hardware automation, the field has benefited significantly from advances in computational molecular design and synthesis planning, which have proven to be powerful tools for accelerating drug discovery.536,537 Notably, while the idea of applying ML methods to drug discovery dates back to the 1990s, the recent achievements of DL methods sparked a high interest in the field for AI-driven early-stage drug discovery.538,539 Indeed, exploiting the capacity of DL to leverage vast amounts of data to create efficient biochemical representations could transform how early-stage research is conducted. For example, Stokes et al. used a DL GNN to identify new antibiotic compounds, and were able to successfully demonstrate the repurposing of halicin, originally used in the treatment of diabetes, as a lead compound for inhibiting E. coli bacterial growth.540 Additionally, the release of AlphaFold541 has revolutionized the approach to computational protein structure prediction, holding great implications on structure-based high-throughput virtual screening, a routinely used method in early-stage drug discovery.542,543 Generative DL approaches have recently been used to design new small molecules and proteins,224,544,545 with multiple drug discovery companies now progressing AI-driven designed molecules into clinical trials.538 Notably, in 2019, Zhavoronkov et al. showed one of the first examples of generative DL accelerated drug discovery, with the 6 possible lead compounds for DDR1 kinase inhibitors verified by manual biological assays.224 Ren et al. later demonstrated that the same workflow was effective in finding lead compounds for dark proteins�those with no experimentally known structure� using AlphaFold to find the protein structure and binding pocket.546 While automation is now routinely used in the pharmaceutical industry, and AI has made its debut into the pipeline, these components have mostly remained disconnected from each other. Therefore, extensive human input, interface between different steps, and external control is still needed. By combining both into a closed-loop manner, SDLs could help reduce the current bottlenecks and also eliminate human biases in hypothesis generation.547 However, there are two important challenges that drug development does not share with any of the other topics discussed in this review: (i) drug development spans vast length- and time-scales unlike any other SDL system and (ii) biological experiments provide very noisy responses, especially as the complexity of the organism increases. Since the stages of drug discovery typically occur sequentially with target identification, hit discovery, hit-tolead, and lead optimization being distinct stages, it is not surprising that SDLs for drug discovery typically focus on optimizing one stage of the pipeline at a time. Therefore, we will assess the progress in the adoption of SDLs in the pharmaceutical industry by looking at their implementation at different stages of the small-molecule discovery pipeline, mainly focusing on early stage research and preclinical studies. We also dedicate a section to discuss the broader application of SDLs to protein engineering and synthetic biology. We limit our discussion to SDLs applied to biochemistry, such as the development of small molecule drugs, molecules and polymers for biologics, nanomedicines, and production of chemical matter through biological systems.",
    "5.1. Drug Discovery Pipeline":"",
    "5.1.1. Target Identification and Validation":"In modern early-stage drug discovery, identifying a target, a gene or protein that is involved in a disease, is a critical initial step.548 A great demonstration of the benefit of automation in target identification is the robot Adam that was developed by King et al. to perform high-throughput automated microbial batch growth experiments which are individually designed.37 Adam was used to identify which genes encoded locally orphan enzymes in Saccharomyces cerevisiae (i.e. enzymes with unknown encoding genes).549 The stages of Adam’s workflow included generating hypotheses; generating, designing, and performing experiments, collecting optical density (OD) data, forming growth curves from the OD data; recording and analyzing data; relating the data back to the hypotheses. The hypotheses suggested potential encoding genes for locally orphan enzymes. They were generated using bioinformatics software and databases. For the experiments, several modules including a robotic arm, plate slides, plate centrifuges, and plate washers were embedded in the high-level automation workflow, shown in Figure 30. Notably, the hardware did not require human intervention other than replacing materials, and could hypothetically run for a few days without human supervision. However, it was still at risk of encountering problems where a human would be needed to solve them. In addition, its hypotheses were indirect and required additional experiments and literature searches by the authors to verify Adam’s hypotheses. Along with the hardware-enabled acceleration of target discovery, AI has emerged as a powerful engine in finding targets. Recent developments in AI for target identification and validation were reviewed by Pun et al. 548 While the authors suggest that combining AI with automated target validation and screening can potentially increase the efficiency of these stages of early drug discovery, the integration of AI approaches into SDLs has remained elusive. The lack of robotic automation in target identification studies could be due to the fact that biological experiments have inherent challenges including the extrapolation of results from small-scale experiments to emergent behaviors in biological systems, and predicting the phenotype of systems with altered DNA.550",
    "5.1.2. Hit Discovery":"Once a target is identified, the traditional drug discovery pipeline enters the compound screening phase, where compounds are screened to find “hits,” compounds that display interaction with the target or desired activity during screening.551,552 This includes assay development and high-throughput screening to conduct pharmacological, chemical, and genetic tests. In recent years, developments have been made in the automation of various aspects of hit discovery, such as virtual and experimental screening, and assay optimization, which represent important steps towards closed-loop drug discovery.38,543,553−555 In 2015, Williams et al. reported the development of the robot scientist, Eve (Figure 30). Eve was developed to perform high-throughput screening of more than 10000 compounds per day for drug discovery.38 Eve operates in three modes: a library-screening mode which involves grid search testing of a randomly chosen set of compounds from its library, a hit confirmation mode in which Eve re-assays hits, and an “intelligent screening” mode where Eve autonomously hypothesizes and tests QSARS. Figure 31 shows how these three modes fit into the greater early stage drug discovery pipeline. Eve generates QSARs using a GP with a linear kernel.176 In addition, active learning using a greedy strategy is implemented to select batches of 64 compounds to test Eve’s hypotheses. The authors made a semantic data model of the screening assay results. The flexibility of Eve’s design allows for the easy definition and modification of assays, including, e.g., general, standardized assays (such as computational assays), targeted assays (such as biochemical assays), and biologically realistic assays and screens for toxicity (such as a cell-based assay). All three modes are integrated with software that communicates with the robotics within Eve’s framework. For the robotics, Eve uses off-the-shelf automation equipment for laboratories. Examples include robotic arms and linear actuators for plate transfer, liquid handling systems for sample transfer, and shaking incubators for screening reactions. For analysis, Eve can measure fluorescence, absorbance, cell morphology (using microplate readers), and bright-field and fluorescence images, with an automated microscope. Once the assay is created and the QSAR problem is defined, Eve can run with minimal human intervention. Remarkably, Eve can further be used to discover new targets for existing drugs; Eve uncovered a second target for an anti-cancer drug which makes it a potential candidate for treating malaria. Eve was also used to compare its intelligent screening with grid search screening, with the authors concluding that intelligent screening is less expensive than grid search screening for pharmaceutical screening which uses large libraries and expensive compounds. While Eve shows great strides towards an SDL since it can optimize the activity of drug molecules for a particular target in a closed-loop fashion, and is proven to be useful for repositioning drugs, one drawback of the platform is that it is not connected to an automated synthesis platform, and therefore it is limited to only testing compounds in its library. Integrating the automated synthesis of new compounds into the pipeline would greatly expand the capabilities of Eve. More recently, Grisoni et al. developed an automated pipeline for hit discovery of liver X receptor (LXR) agonists.553 They combine a DL generative model and automated synthesis in one platform. This modular system, shown in Figure 32, consists of a design module that uses a RNN based generative model with long-short term memory cells to design new molecules as SMILES strings, a verification module that virtually confirms the synthesizability of the designed molecules, and an automated bench-top microfluidics platform that runs the synthesis. The microfluidics platform retrieves reagents, optimizes reaction conditions, and performs one-step reactions to synthesize compounds. The reactions are monitored using HPLC-MS, and the crude reaction mixtures are collected automatically. The only human intervention needed to operate the entire platform is selecting the compounds for pretraining and fine-tuning the model. The authors demonstrate one “iteration” of their pipeline, and do not feed results back from the reactions to the design module. The platform synthesized 61% of the computationally designed molecules in this study. In addition to the automated experiments, the authors performed batch synthesis and further screening of select compounds to confirm activity. Through this study, 12 novel, active LXR agonists were found. Although this platform is not closed-loop, it is a successful example of automated drug design and synthesis, and shows potential to be incorporated in a closed-loop platform. An enzyme assay is an experimental method which qualitatively or quantitatively assesses the activity of an enzyme. 556 With assays being an important part of highthroughput screening, optimizing assays is an area of research in itself, and therefore, automating the assay optimization process is pertinent to creating an SDL for hit discovery. One demonstration of automated assay optimization comes from Elder et al.. 554 They used a cloud-based BO based algorithm, along with automated experiments to optimize a cell-free papain biochemical enzymatic assay for papain inhibitors. The optimization involves minimizing final enzyme concentration, final substrate concentration, and incubation time, while maximizing the value of K’, which is a statistical parameter that uses control data to assess the quality of assays.557 The automated platform included liquid dispensers, microplate reader for fluorescence measurements, and automated microplate washing. Their platform tested, on average, 21 assay conditions in order to find the best conditions, therefore being more efficient and less expensive than other methods such as grid search which requires testing all 294 conditions. This demonstrates the advantage of a closed-loop experimental platform, where the experimental results are fed into the optimizer to suggest future experiments. In addition, the automated platform allows the optimization process to be controlled remotely. Other assays could be optimized on this platform and the technology can be applied to other areas of drug discovery, such as reaction screening and hit selection. Finally, Kanda et al. reported BO combined with automated experiments studied in another context: optimizing a cell culture to produce induced pluripotent stem cell-derived retinal pigment epithelial (iPSC-RPE) cells.555 In this study, the target protocol (differentiation of iPS cells to RPE cells), seven parameters (one parameter for reagent concentration, four parameters for the duration of certain steps, and two pipetting parameters), and validation function are defined by users. The robot booth included a microscope, dry bath, plate and tube racks, an aspirator, a dust bin, a tip sensor, pipette tips, micropipettes, a CO2 incubator, and a dual arm robot. While the seeding, preconditioning, passage, RPE differentiation, and RPE maintenance steps of the experiments were performed by the robot, there was still a considerable amount of human labour involved in the process: initiating and preparing cell suspensions, preparing various reagents, importing plates into and out of the robot booth, taking images of the samples and analyzing them, further processing and testing the cells and media collected from the experiments. In addition, the conditions used for this study, including the robotic equipment, parameters, and scores, are not necessarily directly transferable to different protocols, and must be reevaluated when designing a new study. This platform was able to improve iPSC-RPE production by 88% in 111 days through testing 143 different cell culture conditions. The authors also found that the robot generated cells which satisfy the criteria for research applications in regenerative medicine. While the work focused on the study of regenerative medicine, the authors’ method is not unlike the other examples shown above for hit discovery, and may be applicable to hit discovery platforms as well. This platform has the advantage of being closed-loop, with three rounds of BO performed with a GP surrogate, however, there is room for improvement. Making the platform more flexible to accommodate different types of experiments, and increasing the amount of automation could reduce the amount of human labour required to run and design the experiments, bringing this platform closer to an ideal SDL",
    "5.1.3. Hit-to-Lead and Lead Optimization":"The goal of the hit-to-lead stage is to evaluate and perform optimization on the “hit” compounds from the previous substage to identify which ones are most susceptible to turn into “lead” compounds. Once a lead is found, it usually undergoes multiple rounds of optimization to improve potency and reduce side effects. The integration of SDLs at this substage would answer one of the core demands of the pharmaceutical industry. In fact, while it is relatively straightforward to identify numerous hit compounds virtually or via HTS, prioritizing those for further stages requires medicinal chemistry intuition and testing those hypotheses more thoroughly. Since this process is iterative, it is well amenable to the DMTA paradigm, and therefore to SDL integration. In 2013, Desai et al. designed a fully integrated flow-based autonomous platform assisted by an algorithm design (CyclOps) to perform hit-to-lead optimization, showcasing its use in the case of AbI Kinase inhibitors (Figure 33).558 Starting from ponatinib as a hit compound, the authors defined a chemical space of 270 molecules that could be synthesized in the automated workflow by structural analysis of potanibbound AbI Kinase. The design algorithm would then select compounds from this space to be synthesized on the platform using Sonogashira reactions in flow, purified by in-line preparative HPLC, and analyzed for kinase activity in realtime. The authors used a RF model for activity prediction that used drug-like molecular descriptors involving the Lipinski rules and molecular fingerprints, initially trained on 36 literature compounds. Three design strategies were set up : (i) “chase potency,” an exploitative strategy selecting topscoring compounds based on predicted activity, (ii) “most active under sampled one,” an explorative strategy accounting for the number of times certain reactants have previously been employed and (iii) a hybrid strategy combining (i) and (ii). Overall, the flow chemistry, purification, and bioassay proceeded with a success rate of 71%. In all, 11 key compounds were identified as potent inhibitors of Abl1/ Abl2, with IC50 values in the low nanomolar range. Those were retested with conventional bioassay methods, and the data generally showed a high level of correlation with data generated via the microfluidic platform. In a subsequent paper, Czechtizky et al. demonstrated the reproducibility and consistency of their platform by applying it to replicate xanthine-based dipeptidyl peptidase 4 (DPP4) inhibitors.559 This time, the compounds were synthesized via a two-step synthetic protocol using a Vapourtec R4 flow chemistry system. Overall, 29 compounds were prepared in high purity and tested in only three days with a chemistry success rate of 93%. Close correlation between the microfluidics platform data and data generated within traditional approaches was observed once again. Recently, the CyclOps platform was used to develop hepsin inhibitors selective against urokinase-type plasminogen activator (uPA).560 Over the course of 9 days, 142 novel compounds were generated and assayed with hepsin and uPA. The algorithm explored a virtual chemical space of 5472 molecules, spanning three types of commercially available reagents�a sulfonylating/acylating agent, an amino acid and an amino amidine. Each closed-loop cycle took approximately 90 min on the platform. The authors alternated between exploitative and explorative strategies, but also conducted several grid-search rounds focused on the variation of a specific reagent. The progression from the initial hit to the lead compound was accompanied by an improvement in inhibitory activity against hepsin from ∼1 μM to 22 nM. The selectivity over uPA was improved from 30-fold to >6000-fold. The lead compound found was also further ADMET-profiled (i.e., absorption, distribution, metabolism, excretion, and toxicity) and tested in oncogenic functional assays. When assayed against a panel of 10 serine proteases, it displayed promising selectivity. The CyclOps platform is a great example of concrete application of SDLs to drug discovery development. Leveraging microfluidics for compound synthesis in a combinatorial fashion and coupling it to the RF algorithm allowed saved experimentation time and chemical resources, while leading to the discovery of compounds with enhanced properties. A weakness of such demonstration was that the RF algorithm was only optimizing the compound activity, so it could not be part of decision-making in the event of any synthesis- or process-related issues, e. g. poor reactivity or solubility. One can find more discussion on SDLs integrating Reaction optimization. Recent work from Novartis Medical Research addresses synthesis optimization within hit-to-lead optimization in their microscale SDL. Brocklehurst et al. developed the MicroCycle561 platform, an integrated workflow that connects the infrastructure of Novartis with software tools and a robotics system to create a closed-loop cycle. Candidates are designed through an RF model trained on in-house data for QSAR of physicochemical and biochemical properties. The RF model is then incorporated in a BO campaign, in some cases along with protein docking results, for selecting molecules in the synthesis step. From acquired building blocks, the robotics platform, equipped with automated solid dispensing, liquid handling, and a robotic arm, autonomously performs optimization of reaction conditions and high-throughput microscale synthesis. For the test stage, the MicroCycle platform includes an integrated plating process to prepare microscale assay-ready plates and can perform many types of assays automatically, including physicochemical assays, ADME (i.e., absorption, distribution, metabolism, and excretion) in vitro assays, and target-specific biochemical assays. Starting from a hit compound with moderate activity, the authors used MicroCycle to generate 13 libraries of compounds from 8 reaction types, showcasing the use of their predictive models, automated synthesis, and purification. Over 440 molecules were made and an average success rate of about 50% was achieved, meaning that about half of the syntheses were sufficient for running an assay. With additional analysis and contributions from medicinal chemists, molecules with improved activities and potency were identified, while maintaining good solubilities, and appropriate molecular weights.",
    "5.1.4. Formulation Optimization and Bioavailability":"Drug formulation is an essential stage in the discovery and development of new medicines, allowing to improve bioavailability and targeted delivery. Traditionally, designing drug formulation relies on iterative trial-and-error, requiring a large number of resource-intensive and time-consuming in vitro and in vivo experiments. However, the field has recently experienced a growing interest in integrating ML and automation approaches into the design process, as described in the review of Bao et al. 562 As optimizing drug formulations implies varying multiple parameters related to the drug, excipients, and manufacturing conditions, SDLs could help navigate this highly dimensional space. One example of formulation optimization using an SDL is the work conducted by Cao et al., 563 although this example is not directly tailored to a pharmaceutical application. In this work, a commercial formulation consisting of a mixture of three different surfactants, a polymer and a thickener was optimized in a closed-loop fashion according the following multi-objective (Figure 34): (i) stability and low turbidity, (ii) high viscosity and (iii) low ingredients costs. The TS-EMO algorithm was chosen to suggest formulation parameters318 and coupled to an SVM classifier (trained on initial experimental runs) that classified its temporary suggestions based on their stability. The algorithm was run until the classifier identified eight stable formulations amongst the suggested ones, which were then synthesized automatically using a first robot, and transferred to a second one that performed pH, turbidity, and stability tests. Unfortunately, the samples had to be taken offline to measure viscosity. In 15 working days and without providing any explicit physical intuition to the system, the authors were able to obtain satisfactory formulations. Another example in the literature of SDL for formulation is the work of Grizou et al. 564 in which a new high-throughput droplet dispensing robot was coupled to a Curiosity Algorithm (CA) to study the behavior of dynamic oil-in-water droplets, which serve as promising protocells models�a synthetic celllike entity that contains non-biologically relevant components. The authors defined their parameter space by choosing mixtures of four oils and set a budget of 1000 experiments to observe how varying the oil mixture impacted the speed of the droplets and their division. This observation space was chosen for its simple life-forms-like behavior, which can move and replicate. To do so, small oil droplets are placed at the surface of an aqueous medium, the droplet movements are then video recorded and analyzed using traditional image processing techniques to deduce the speed and division of the droplets. To select the next oil mixture to be tested, the CA first feeds previous observations to a locally weighted linear regressor that approximates the mapping between input parameters and observations. A random target observation is then selected and fed to the numerical inverse of the regressor to infer the most probable experimental parameters that will lead to the target observation. The fully closed-loop platform can conduct more than 30 experiments per hour by leveraging parallelization, a six-time throughput increase from previously reported ones. By leveraging the CA, the speed observation space was more efficiently explored, with only 128 experiments needed to cover the portion of the observation space that random parameter search covered in 1000 experiments. The number of droplets deemed active (with speed >3 mm/s) was also improved 14-fold, without it being an explicit objective. Moreover, two whitepapers on SDL concepts have recently been reported in the formulation development literature, demonstrating the high interest of this field for automation. Hickman et al. have proposed an SDL named NanoMAP, which focuses on the development of nanomedicines for pharmaceutical formulations.565 Nanomedicines commonly consist of a combination of polymer and/or lipid-based materials or excipients that encapsulate small molecules or biologic-based active agents.566 The authors propose to automate the preparation of nanomedicines for screening using nanoprecipitation via liquid-handling robots, while coupling it with active learning strategies. Importantly, this experimental protocol has previously been successfully implemented and can be scaled up by leveraging a microfluidics platform.567,568 On the characterization side, the drug loading capacity (DLC) and encapsulation efficiency (EE) would be automated with appropriate extraction methods and analysis via HPLC. The authors also plan on automating highthroughput in vitro stability and release assays in biorelevant media using 96-well dialysis plates, as well as particle size measurement using dynamic light scattering (DLS) plate readers. Tamasi et al. proposed the development of BioMAP for biologic formulation design (Figure 35).569 Indeed, while therapeutic proteins and vaccines�commonly called biologics�have proven their therapeutic efficacy, they remain extremely fragile under standard pharmaceutical storage and handling conditions, c.a. -78°C. Therefore, extensive formulation efforts are routinely required to avoid their denaturation, using additives such as small-molecule stabilizers, polymer excipients, or surfactants. This is also observed for monoclonal antibodies (mAbs). The authors’ plan on building on their previous experience of coupling automation and ML (see below section on Engineering the stability of proteins for a detailed discussion) to create a fully autonomous platform for optimization of tailored polymer additives. They also aim at increasing their materials library to generally recognized as safe (GRAS) excipients as well as expanding the platform to liquid NP formulation. This ambitious project necessitates careful design of the automated instrumentation, as it must remain flexible for each biologic type. The authors plan on providing extra supportive modules such as a multimode reagent dispenser, a plate heater/shaker, a plate sealer and a vacuum filtration system on top of a multi-purpose liquid-handling robotic system for cell culture and reagent mixing. For liquid NP production, microfluidics and continuous-flow fluidics would be leveraged. Testing the stability of formulations over a range of storage and handling conditions could be carried out using an automated microplate incubator with humidity and CO2 regulation, which could also be used to support cell-based assays. For characterization UV-visible (UV-vis) and DLS plate readers, size-exclusion chromatography (SEC), and a high content imager (HCI) would be employed. A proof-of-principle synthesis and formulation platform by Adamo et al. showcases technological advances in continuousflow synthesis and formulation of pharmaceuticals that could be incorporated into SDLs for formulation optimization.570 The platform’s capabilities included multistep synthesis, purification, crystallization, real-time process monitoring, and formulation. In addition, it was reconfigurable to produce pharmaceuticals with diverse chemical structures and synthesis routes on-demand. With the entire system being approximately the size of a refrigerator, and total cycle time for synthesis and formulation being up to 48 hours, the authors demonstrated a competitive alternative to the usual batch synthesis of pharmaceuticals which can take up to 12 months and involves multiple synthesis steps and formulations occurring in different locations. The authors showed the successful synthesis and formulation of four common drugs in liquid formulations, and were able to achieve a capacity of up to 4500 doses per day of diphenhydramine hydrochloride. Automated components of the platform include pumps, heating reactors, multichannel valves, and gravity-based separators, as well as precipitation, filtration, and crystallization tanks. Only one user is required to operate the entire system. While the purpose of their platform was to demonstrate the production of liquid formulations of common drugs, the design and technology integrated into the SDL could be advantageous not only for formulation optimization, but also for time and cost effective synthesis of molecules in hit discovery or hit-to-lead workflows. However, one drawback of this system that would make it difficult to use in a high-throughput setting is the turnaround time for reconfiguring and cleaning the system, which could take as long as two hours, or potentially longer depending on the complexity of the synthesis. In addition, it is not equipped for solid formulations. Ortiz-Perez et al. 571 proposed an integrated and semiautomated iterative workflow that combines microfluidicassisted nanoparticle formulation, automated fluorescence imaging and analysis with BO to design poly(lactic-co-glycolic acid)-polyethylene glycol (PLGA-PEG) NPs with high uptake in human breast cancer cells. To maximize the uptake, one process variable�the flow rate ratio between solvent and antisolvent�and 4 polymer components that directly influence physicochemical properties (size, PEGylation and charge) were varied. The polymer mixture can be automatically prepared using a syringe pump, and injected into a microfluidic chip at a constant flow rate while the antisolvent rate is adjusted. This automatically produces NPs with controllable size and composition, which can be labeled in situ by incorporating a fluorescent dye during formulation. To measure uptake in cells, NPs are added to cells in 96 well plates and fluorescence microscopy is used to acquire and process widefield fluorescence images in an automated way. This measured response per NP is used to train a BNN to predict nanoparticle uptake from nanoparticle formulation. The uptake, polydispersity index and size of a virtual library of 100,000 NPs spanning the entire design space homogeneously are then predicted using the BNN and tree-based models respectively. The next formulations are then selected from this pool, based on the models’ predictions. With two 5-day experimental cycles, the authors were able to triple the measured NP uptake. While concrete examples of SDLs for drug formulation optimization are still sparse, the excitement of the field for ML techniques and automation foresee a bright future for SDLs. Nevertheless, several questions are left to be answered; notably, how the diversity of therapeutic compounds should be handled. Indeed, contrary to most of the SDLs mentioned in other sections, one would expect the platform to be adaptable to a wide range of drugs, as proposed in the work of Tamasi et al. This is likely to complicate the coordination of experiments both on the software and hardware side. Finally, while this can also be relevant to other stages of the pipeline� and applies to other fields�Lammers et al. highlighted the lack of standardization in studies conducted to characterize formulations such as nanoparticles.572 Better guidelines and reporting is thus needed to provide the best conditions for ML application to thrive in the field.",
    "5.2. Synthetic Biology":"While so far mostly the discussion has been focused on reviewing SDLs for small molecule therapeutics development, it is important to highlight that the deployment of SDLs has also sparked interest in the fields of protein engineering and synthetic biology,550,573 even inspiring space biologists.574 This naturally extends the potential applications of SDLs to a broader range of therapeutics, but also to the design of new biomaterials or biofuels for instance. The challenge resides in the inherent complexity and non-linearity of biological phenotypes, the high-dimensionality of genomic search spaces, and the error-prone and difficult-to-automate nature of biological experiments.",
    "5.2.1. Biosynthetic Pathways Optimization.":"The microbial synthesis of chemicals offers a viable alternative to widely employed chemical manufacturing methods. Biosynthetic production is appealing due to its ability to leverage a diverse range of organic feedstocks, operate under benign physiological conditions, and circumvent the generation of environmentally harmful byproducts. However, natural cells are rarely fine-tuned to efficiently generate a specific molecule. To attain economically feasible production, significant alterations to the metabolism of host cells are frequently necessary to enhance metabolite titer, production rate, and overall yield.575 Unfortunately, the complexity of biological systems and their multiple components and many unknown interactions among them lead to having to perform many DMTA cycles. Employing automation combined with computational approaches can help expedite this process. In their paper, Carbonell et al. improved the production of flavonoid (2S)-pinocembrin in E.Coli, a natural product, by optimizing a 4-gene pathway (2592 possible configurations) by 500-folds, going through two DMTA cycles.576 First, rulebased pathway and enzyme selection tools were employed to define a synthetic route for (2S)-pinocembrin. A combinatorial approach was then utilized to design 2592 synthetic plasmids, each corresponding to a pathway that varied in terms of gene expression and combination. Assembly recipes and robotics worklists were generated to automate the plasmid assembly� commercial DNA synthesis, part preparation via PCR followed by ligase cycling reaction. After this, growth of microbial production cultures was conducted in a high-throughput manner, products were automatically extracted, and screened via fast-LC and MS. Finally, the data was analyzed using standard linear regression to identify important experimental factors to aid in the design of the next iteration. The pipeline was designed in a modular fashion which would allow other laboratories to replace individual pieces of equipment or protocols to adopt their own methods. The authors tried to demonstrate the adaptability of their pipeline to other compounds production than (2S)-pinocembrin, choosing to optimize the expression of the (S)-reticuline�an alkaloid� pathway in E.Coli. While some of the obtained constructs matched literature titers, experimental difficulties were encountered, suggesting that the proposed arrangement was either unstable or negatively selected against in the cloning host. Further conversion of (S)-reticuline to (S)-scoulerine yielded modest results, although this alkaloid had not been produced in E. Coli previously. Each step of their pipeline leveraged automation, but the entire workflow was not fully integrated to enable autonomous operation. Indeed, PCR clean-up and host−cell transformation were carried out off deck, and plates needed to be manually transferred between certain platforms. Another point that weakens the SDL character of this work is that the “analyzing” component was conducted using a standard least squares linear regression, which could identify trends across the experimental parameters, but did not actively make design suggestions for the next cycle. HamediRad et al. focused the production of lycopene in E.Coli�a food additive and colorant recently proposed as anticarcinogenic�and demonstrated that BO could help optimize gene expression of a 3-gene pathway, outperforming random screening by 77% while only evaluating less than 1% of possible variants (over a total of 13,824 possibilities) through three DMTA cycles.577 The experiments were conducted using the iBioFAB biofoundry, a fully automated and versatile robotic platform consisting of a 6-degree-freedom articulated robotic arm that travels along a 5-meter-long track to transfer microplates among more than 20 instruments installed on the platform and a 3-degree-of-freedom arm moves labware inside a liquid handling station.578 Each instrument is in charge of a unit operation, such as pipetting and incubation. They are linked by the two robotic arms into various process modules, such as DNA assembly and transformation, and then further organized into workflows such as pathway construction and genome engineering. An overall scheduler orchestrates the unit operations and allows hierarchical programming of the workflows. In this work, the iBioFAB platform was used to automate the lycopene pathway DNA assembly with different expression levels of genes using the Golden Gate method,579 as well as the transformation, cell cultivation and lycopene extraction. The BO algorithm proposed DNA assembly designs, which were then converted into robotic commands for iBioFAB to conduct the complex pipetting work using a Tecan liquid handler. The best mutant found produced 1.77- fold higher lycopene titer than the best mutant found using random sampling and the number of evaluations was at least eight times less than the regression-based optimization scheme. This work greatly demonstrates how a flexible platform like iBioFAB and BO can benefit from each other to efficiently explore the gene expression landscape. It is important to mention that the lycopene pathway was chosen as a proof of concept for its straightforward methods of extraction and quantification, which facilitated high-throughput execution using iBioFAB. Indeed, the biofoundry faced limitations linked to compound extraction methods and analytical/quantification methods requiring equipment more complex than a plate reader (e.g., GC-MS or LC-MS instruments). Those challenges could be overcome in the future by the development of largerscale and more sophisticated biofoundries. Another possible improvement highlighted by the authors was that no initial assumptions about the landscape were made prior to using BO. Using the trained model for one system as the starting point for a similar system could potentially result in reducing the number of evaluations to find the optimum.",
    "5.2.2. Engineering Protein Stability":"Proteins are sparking increasing interest in the context of biomedicine and pharmaceutical sciences. However, the moderate stability of proteins, and specially enzymes, is the major drawback hindering the generalized application of these bioactive molecules at the industrial scale. Indeed, the current process conditions may include extreme temperatures, pH values, or presence of organic solvents that are outside the operating stability window of the biomolecule, but that are often necessary to solubilize poorly-water-soluble substrates in high concentration values. Therefore, ensuring protein stability is of tremendous importance to unleash their potential and is a very active field of research.580 Enabling protein stability optimization via SDLs could help to efficiently explore the combinatorial search space that relates protein sequences to their function, and ensure reproducibility of such results. Tamasi et al. investigated the use of active learning for protein-polymer hybrids (PPHs) using copolymers. PPHs are a promising way to address challenges such as solubility and low physical stability of proteins by conjugating them with synthetic polymers.581 They leveraged a GP regression model with BO to identify candidate copolymers for three chemically distinct enzymes, namely HRP, GOx, and Lip, to maximize the retained enzyme activity (REA). While the synthesis of the proposed copolymers was automated, the PPHs formation and REA characterization were undertaken manually. Their discovery process invoked five DMTA cycles and resulted in enhanced thermostability for the three distinct enzymes (46.2%, 31.5%, and 87.6% improvement in comparison to the initial seed batch for HRP, Gox, and Lip, respectively). While this work is not an SDL in itself, the same group has then later proposed the conceptual outline of BioMAP (vide supra), demonstrating the incremental nature of SDL improvements. Recently, Rapp et al. introduced the Self-driving Autonomous Machines for Protein Landscape Exploration (SAMPLE)582 platform for fully autonomous protein engineering and were able to engineer glycoside hydrolase family 1 (GH1) enzymes with an enhanced thermal tolerance (Figure 36). The protein engineering task was framed as a BO problem that was tackled using a multi-output GP, combining GP regression on thermostability and GP classification on protein activity. First, the authors designed a GH1 combinatorial sequence space composed of sequence elements from natural GH1 family members, elements designed using Rosetta,583 and elements designed using evolutionary information. This yielded a space containing 1352 unique GH1 sequences. To pick sequence candidates for the experiments, the authors designed a custom sampling strategy that constrained selection to the subset of sequences predicted as active by GPC. Within this subset, candidates for improved thermostability were then selected. To access the protein sequence space experimentally, SAMPLE relies on combining pre-synthesized DNA fragments using the Golden Gate method579 to produce a gene, which can be amplified using PCR then expressed into the desired protein using T7-based cell-free protein expression reagents. Finally, the expressed protein is characterized using colorimetric/ fluorescent assays to evaluate its biochemical activity and properties. The procedure took approximately one hour for gene assembly, one hour for PCR, three hours for protein expression, three hours to measure thermostability, and overall, nine hours to go from a requested protein design to a physical protein sample to a corresponding data point. The experimental pipeline was fully automated and implemented on the Strateos Cloud Lab.584 To ensure reproducibility, four diverse GH1 enzymes from Streptomyces species were optimized for thermostability, each trial was composed of 20 DMTA cycles. Each resulting enzyme was at least 12 °C more stable than the starting protein sequences while exploring less than 2% of the defined protein sequence-function landscape. This work demonstrates the use of SDLs for improved protein thermostability. Importantly, it confirms the reproducibility of the results across several GH1 enzymes, and reports, to our knowledge, the first example of BO coupled to a fullyautomated Cloud Lab platform. A cloud lab is a fullyautomated decentralized laboratory in which scientists can run multiple experiments simultaneously and remotely, all through a single digital interface. This type of facility allows researchers to have full control over their experiment without having to be physically present in the lab. Moreover, research can be conducted without purchasing costly lab instruments or leasing physical laboratory space.584 Another strength of this work is the exception handling and data quality control mechanism implemented to further increase the reliability of the SAMPLE platform, allowing to flag experiments as inconclusive and add the associated sequence back to the potential experiment queue.",
    "5.3. Outlook and Perspectives":"The use of animal models has played a crucial role in the advancement of modern biomedical research, allowing the exploration of basic pathophysiological mechanisms, but also the development of new medicines. Indeed, because of their role in evaluating new therapeutic approaches, animal models bear the weight of the “go” or “no-go” decision to carry new drug candidates forward into clinical trials. However, the discordances between animal and human studies are frequent, thus drug candidates may be eliminated for lack of efficacy in animals, or discovery of hazards or toxicity in animals that might not be relevant to humans.585 The impressive expansion of the organ on a chip field bears the promise to address this issue by leveraging the latest advances in microfabrication engineering, microfluidics, genome editing and cell culture capabilities.586 Recently, the FDA Modernization Act 2.0 was approved, allowing alternatives to animal testing for drug and biological product applications.587 This change in legislature could potentially improve the adoption of SDLs in the preclinical phase of the drug discovery pipeline. Given the great challenges of automating biotechnology, it is fundamental that laboratories collaborate and collectively develop guidelines and protocols by establishing global alliances588 and consortia.589 For instance, the adoption of the FAIR guidelines143 for data-sharing is crucial for efficient use of DL and ML, especially in a field where many processes are yet to be explained and reproducibility can be an issue. The use of “cloud labs” could also allow researchers to access standardized equipment from anywhere at any time.584 Furthermore, the stellar increase in interest for data-driven approaches applied to drug discovery calls for a reflection upon open science adoption in this industry. Indeed, a lot of success stories in DL stem from data availability and open-source libraries.166 The AI for science excitement could therefore bring us to a long-awaited moment.590,591 While we begin to see success stories where academia and industry join forces in an open science setting,592 it is important to carefully consider and control the risks of misuse associated with sharing biological data and models.593 Throughout this section, we have reviewed the adoption of SDLs across the different stages of the drug discovery pipeline and in the field of protein engineering and synthetic biology. The works we highlighted showcase the great potential of SDLs to transform the current drug discovery process, while shining light upon the challenges ahead. On the algorithmic side, the recent DL achievements have highly increased the community’s excitement and its expectations. Nevertheless, several challenges are already foreseeable.594−596 For example, the ligand-protein binding data from AI predicted protein structures is not as accurate as that which uses experimentally determined protein structures, and it is often not experimentally validated. In addition, more transparency from companies is needed in order to move the field forward as it helps to build trust in their results and allows larger sets of data to become available for ML. Federated learning is a way to overcome this challenge because it can keep company data confidential while using data from multiple companies in one ML model. On the automation side, we expect future advancements in microfluidics technology to greatly improve some of the systems covered above. Moreover, BioFoundries represent sophisticated hardware that is likely to have a great impact in the adoption of SDLs in this context, and could inspire other fields too. Furthermore, while proof-of-concept SDLs exist at various stages, the lack of seamless integration of feedback across these stages poses a potential limitation. Addressing this challenge and fostering collaboration between different phases could enhance the overall efficiency of SDLs. Overall, recent legislative changes, the growing significance of DL and the current advancement in hardware components suggest a favorable landscape for the adoption of SDLs in pharmaceutical science, paving the way for transformative advancements in drug discovery and biotechnology",
    "6. Structural Materials":"This section covers materials geared towards structural applications with a focus on mechanical performance of materials. Naturally, we focus on materials and techniques that involve automated and data-driven methods including robotics, optimization algorithms, software orchestration, experiments, and simulations. We do so for alloy design, concrete formulations, non-alloy additive manufacturing, and adhesives. Many of these materials are inorganic solid state materials. All synthesis, processing, characterization, and testing methods of mechanical properties of solid samples require mechanical motion of some kind�whether through the transfer of solid samples or the movement of experimental apparatus around or in contact with solid samples. To date and to the authors’ best knowledge, there are no published demonstrations of fully autonomous SDLs that use data-driven methods to iteratively explore inorganic solid state materials design spaces for largescale structural applications with mechanical performance measurements. However, there has been a large amount of progress towards automated, and in some cases autonomous, synthesis and characterization methods for inorganic solidstate materials such as with Powder-Bot, A-Lab, and ASTRAL, as described in section on Solid State Material Synthesis optimization. Within each section and where appropriate, we point to the data-driven approaches that accelerate the discovery of these materials and which provide the foundation for structural-focused SDLs to come.",
    "6.1. Alloy Design":"From extreme-temperature Inconel alloys in jet turbines, lightweight aluminum alloys for engine blocks, biocompatible titanium alloys for hip replacement joints, to the classic corrosion-resistant 304 stainless steel found in the kitchen sink, the enabling effects of alloys are immense and often invisible. Events like the fatigue-induced catastrophic failure of “de Havilland Comet” commercial jet airliners in the 1950s, the steel corrosion-induced oil spill of the Alaskan Oil Pipeline in 2006, and the tens of thousands of metallic poisoning cases from cobalt-based hip implants up to 2010, is a reminder of how much the world relies on this class of materials and how the performance can enable (or the lack of performance can hinder) advancements in the transportation, energy, and medical fields. While many alloys are known and in usage, the alloy discovery space is a largely unexplored and high-dimensional search space. In the case of multi-principal element alloys (MPEAs)�i.e., alloys with many constituent components� Miracle et al. estimates that there are nearly 200 million potential MPEAs systems with three-to-six constituent elements. Note that these are individual systems, meaning the tunable parameters of stoichiometry and processing conditions are ignored.597 Over the course of twelve years from their discovery and documentation (2004−2017), the authors estimated that only 122 MPEA systems had been identified. Forecasting the current rate to the year 2117, and using only traditional methods, the risk of missing the best possible MPEAs system for a given application is over 99.999%, pointing to the need for physics-based, data-driven, automated, and high-throughput approaches. The alloy discovery space is not only high-dimensional, it is also multiobjective. See, for example, a spider plot with twelve performance properties for structural applications (Figure 37) such as yield strength, fracture toughness, thermal expansion, and fatigue. While not every structural application incorporates all of these objectives, depending on the application, many of the objectives must be met simultaneously for commercial viability. While fully autonomous setups that iteratively suggest new experiments with automated synthesis, processing, characterization, testing, and sample transfer are rare in materials discovery for structural applications, there has been a large amount of progress towards automating complex and difficult tasks with inorganic solid-state materials. For example, Vecchio et al. demonstrates the use of the FormAlloy tool to automatically mix powder precursors and additively manufacture alloys.598 While characterization and property testing requires manual sample transfer and intervention, the authors developed a unique platform for high-throughput characterization using a turntable-style sample holder with multiple sample positions (Figure 38). The unique benefit of this design is that they integrated it with a variety of characterization tools, which both drastically reduces the amount of sample prep time and simultaneously makes it much easier to correlate multimodal data between instruments with sample batches. For example, scanning electron microscopy (SEM) and electron backscattered diffraction (EBSD) data are spatially correlated with nanoindentation hardness measurements. While no iterative optimization took place, Vecchio et al. used both thermodynamics-based CALPHAD simulations and ML methods for property prediction, and set the stage for an autonomous and robust SDL for alloy discovery. Aside from macroscale structural applications, Kusne et al. demonstrated, in a data-driven and automated synchrotron characterization setup, a large reduction in the number of required experiments to identify optimal epitaxial nanocomposite phase-change memory materials, important for non-volatile data storage applications.516 These materials leverage heat-induced and reversible transitions between amorphous and crystalline phases to mimic the “0” and “1” binary states of conventional transistors but with higher permanence. We note that the materials search was restricted to a single ternary alloy system (Ge-Sb-Te) and the processing parameters were fixed. Additionally, the sequential characterization experiments were carried out on a pool of several hundred pre-synthesized samples on a single silicon wafer via combinatorial sputtering; meaning that only one iteration of synthesis was performed. Their benchmarking results demonstrated that the incorporation of physics-based phase mapping information led to more efficient discovery relative to both random search and BO without phase mapping awareness. In addition to mechanical and phase-change properties, automated methods have been used to search for corrosionresistant alloys. DeCost et al. built an autonomous scanning droplet cell to accelerate the discovery of a novel Al-Ni-Ti alloy composition for corrosion resistance.599 This SDL has automated serial electrodeposition with adjustable solution compositions and online processing characterization (i.e. optical camera and laser reflectance for assessing continuity, coloration, uniformity, and qualitative roughness of electrodeposits; a potentiostat for measured potential, and current; and a pH probe and thermometer for monitoring pH, and temperature). To close the loop in combining all the various process conditions and measured objectives, the authors adopt the active learning strategy using GP to predict and optimize for corrosion resistance (i.e., passivation current, passivation potential, and the slope of the passivation plateau). After several iterations, they successfully found Al-Ni-Ti alloy compositions that were near the Pareto frontier.",
    "6.2. Concrete Formulations":"If all of the artificial materials in the world were to be categorized and placed on a scale, concrete would be the heaviest. Concrete is the most prevalent human-made material and the second most consumed commodity after water (approximately 30 billion tonnes per year600). Concrete is not a trending scientific topic, like one might expect for topics like superconductors, long-range EV batteries, or quantum computers; however, it is one of the most practical materials topics related to environmental sustainability. The energyintensive nature of manufacturing concrete leads to a weightshare of approximately 8% of human-derived CO2 emissions.600 Unfortunately, there are only a few public examples of iterative and accelerated science methods applied to concrete formulations and processing (at least ones that move past property prediction based on classical ML models). While the reason for this may be complex, in addition to the “hype” factor mentioned previously, there are other practical factors that constrain against deviations from existing concrete technologies, such as the safety concerns of new formulations, or the lack of long-term test data. On a related note, perhaps much of the ML and robotics focus in the field has been concentrated on detection rather than discovery�extending the life of existing concrete rather than seeking to replace it, as may be indicated by the large fraction of detection-focused ML manuscripts in the literature.601−605 Despite these considerations, one promising alternative to traditional concrete formulations involves swapping the energy-intensive “Portland cement” with eco-friendly cements based on alkali-activated binders or “geopolymers.” In an effort to validate and identify optimal data-driven routes for optimization of such concrete, Völker et al. used a set of 131 experimental data from the literature to conduct computational benchmarks, exploring the effect of algorithm choice and parallelization on the efficiency with which high compression strength materials can be identified.606 Notably, this is one of the only studies that shows modern adaptive experimentation being used in the context of concrete optimization. Concrete formulation and processing optimization also has a unique challenge relative to many other materials discovery tasks: it is highly affected by locally available materials. Concrete is dense and is used in large quantities for buildings and other structural applications at a relatively cheap cost-perweight, so transporting it large distances is infeasible. What this means is that an optimal concrete in one part of the world does not translate directly to another part of the world. While the considerations for applying accelerated science tools to the field of concrete discovery are complex, the potential positive impacts are large. We anticipate a fully autonomous concrete formulation and processing optimization tool in the near future, which will require awareness, incorporation of AI and robotics into civil engineering repertoires, and a strong understanding of the limitations and opportunities within the field.",
    "6.3. Non-Alloy Additive Manufacturing":"The use of AI and robotics in additive manufacturing settings holds great promise: in terms of synthesis and optical characterization, there is a relatively low barrier to automation; the equipment typically has an API and native programming abilities; the equipment is adaptable and available, and the processing parameters are straightforward. For example, Erps et al. used multi-objective BO to simultaneously maximize toughness, compression modulus, and maximum compression strength as a function of six primary formulations to form a composite formulation for photocurable resins. All processing parameters were held fixed.607 We note that this is a semi-automated platform which requires human intervention for transfer of materials between each step in the sample fabrication pipeline while all of the individual steps of dispensing, mixing, 3D printing, postprocessing, and testing are completed individually without human intervention. Such formulation systems can be adapted to other domains such as surfactants, cosmetics, foods, and paints. Brown and co-workers have also demonstrated SDL studies of mechanical properties of 3D printed structural materials. While the works predominantly focus on the effects of macroscopic structural designs rather than the intrinsic material property or formulation, the studies demonstrate the potential of autonomous BO for searching various design spaces in additive manufacturing. Gongora et al. introduced Bayesian Experimental Autonomous Researcher (BEAR), combining simulations and experimental observations to optimize the twist angle and struts of a 3D printed structure for optimal toughness.608 The authors use a GP regression model to explore the search space, while a novel automated platform provides high-throughput printing and testing of manufactured parts. Simulations through finite element analysis were shown to strengthen the Bayesian prior of BEAR, increasing the speed of optimization in the experiments.609 In their most recent work, Snapp et al. 610 introduce new mechanical designs (i.e., a generalized cylindrical shell with 8 variable parameters) and filament combinations to improve the material’s mechanical properties. Although their approach employs the SDL framework to accelerate toward an optimal mechanical design, there is an absence of chemical synthesis or materials design. While getting the formulation right and automating the exploration of a wide variety of combinations of starting materials is difficult, optimizing processing parameters within a certain material family is much more attainable. One example of this processing parameter optimization is a low-cost example that uses BO with a modified 3D printer to optimize the print characteristics of a silicone material.113",
    "6.4. Adhesives":"Similar to many applications in this review, adhesives are a classic formulation optimization problem. The relative fractional share of constituents and the combinations of precursors can have a large effect on the bond strength exhibited by the adhesive. In this vein, Rooney et al. developed a semiautomated SDL platform for adhesive synthesis and characterization using a SCARA-type N9 (North Robotics) workstation, and substrates (“dollies”) to be coated with the adhesive and used with a custom shear-stress “pull-off” tester (Figure 39).308 Adhesive coating, preparation, and testing were all performed automatically via the robotic platform, while mixing of the adhesive formulations was carried out manually. This system used BO with a GP surrogate to maximize the bond strength as a function of resin to hardener ratio in a two-part epoxy system over four iterations of 5-sample batches (20 experiments). Notably from an algorithm standpoint, the BO algorithm from the Ax package uses a sophisticated batch-aware acquisition function.",
    "6.5. Outlook and Perspectives":"Researchers in structural materials are quickly adopting ML techniques to accelerate the discovery of novel materials, but many have yet to adopt robotics technology to automate the synthesis and characterization of such materials. Specifically in structural materials, hardware automation remains a challenge because of the inherent difficulties in solid-dispensing, extreme temperatures, and complex testing instrumentation. By reimagining our approach to synthesis and characterization, shifting from conventional human-oriented instrumentation to a hardware-centric perspective, we can harness the power of robotics to automate intricate tasks in diverse ways.",
    "7. Optoelectronics":"Optoelectronic materials play a pivotal role in modern technological advancements by enabling the manipulation and control of light-matter interactions. These materials are integral to a wide array of applications spanning from telecommunications and displays to solar cells and medical devices. The significance of optoelectronic materials lies in their ability to absorb, emit, and modulate light, thereby facilitating the conversion of electrical signals into optical signals and vice versa. This functionality underpins the development of high-speed data communication systems, energy-efficient lighting, and sensitive imaging devices, among other innovations. Designing new optoelectronic materials and making them technologically useful requires a comprehensive understanding of the complex relationship between their composition, structure, processing, and physical properties such as electronic structure and optical characteristics. Experimentally characterizing these attributes often requires sophisticated techniques. The synthesis and fabrication of these materials with the desired properties can be difficult and resource-intensive; optoelectronic materials are often part of devices in crystallized form or thin films, with blends and mixes of other optoelectronic materials, which can have dramatic effects on the performance of the device. In the context of SDLs, this introduces additional parameters that require further optimization, and will be dependent on the material-specific properties such as stability, scalability, and cost. Due to the complexity of the design-make-test-analyze cycle for optoelectronics, efforts have been made to combine ML and DoE without an automated laboratory, aiming only to more efficiently search the design space of possible compounds and devices. Cao et al. used SVMs to optimize only device processing parameters for an organic photovoltaic device active layer composed of a PCDTBT donor and PC71BM acceptor.611 Subsequently, Kirkey et al. extended this method to study multiple acceptor compounds.612 Conversely, Wu et al. leveraged high-throughput synthesis and characterization to speed up discovery of organic semiconductor laser materials without ML-guided experiment selection by exploring molecules similar to the prototypical BSBCz.613−615 Although these efforts did not involve SDLs, they served as initial steps toward the development of SDLs for optoelectronic materials and devices. In the following sections, we discuss SDLs for optoelectronics based on the sophistication of the proxy measurements: from solution-state and single crystals to full device fabrication and testing.",
    "7.1. In Solution and Crystals":"Proxy measurements of materials in solution and as single crystals offer two primary advantages. Both proxies are simple enough that they can be used to gain fundamental insights into the materials at the atomic level, especially in conjunction with quantum chemical calculations. Additionally, they are relatively amenable to high-throughput experiments.616 In the case of solution-based testing there are many options for highly parallel and high-throughput experiments to study the relationship between composition and important optoelectronic properties using simple analytical techniques such as optical spectroscopy",
    "7.1.1. Perovskites":"An example of a commonly studied crystal optoelectronic material is the perovskites. In an SDL developed by Higgins et al., the authors utilized a well plate to automatically and parallelly synthesize 96 multi-component perovskites and analyze the photoluminescence of the compounds in a high-throughput manner.617 Four perovskites systems were studied with varying compositions with the goal of maximizing the photoluminescence of the dissolved crystals. The authors also added a temporal axis to the data, looking at the spectra as a function of time, in order to capture the stability of the compounds. The results of the photoluminescence were decomposed using non-negative matrix factorization (NMF) into spectral information dependent only on the material composition, which was then fed into a GP regression model. The GP model interpolated between the low amounts of data to give a predictive map of the best compositions for the relevant perovskite systems. While the authors did not perform a second iteration based on the predictions of the GP model, the model prediction and uncertainty could have been used in a BO scheme to suggest the next round of experiments. There have also been SDL studies of only the crystallization process of perovskites. Crystallization is not only dependent on the composition of the perovskite, but also the process conditions, which will affect the structure and performance of the optoelectronic device. Li et al. studied the perovskite crystallization using a robotics-accelerated micropipetting system.618 The solution was gradually cooled in an inverse temperature crystallization (ITC) reaction,619 and the resulting crystals were visually categorized into 4 levels of crystal quality. The ITC reaction parameters were the same for all reactions, however the concentrations of the inorganic, organic, and formic acid precursors were varied. All reactions contained the lead (II) iodide, and one of 45 structurally diverse organoammonium cations. In total, 8172 reactions were performed, and the most novel structures were further studied in manual experiments, however, no ML was utilized during the crystallization experiment. Instead, the authors performed a retrospective study, and determined that a SVM model with a Pearson VII universal function kernel was most accurate in crystal quality prediction, using the reaction conditions and the organic and inorganic precursor chemical descriptors, however suffers when trying to generalize to different precursors or perform with small amounts of data. This study demonstrates the potential for a second round of high-throughput experiments based on the predictions of the model. In another work studying the crystallization of perovskites, Kirman et al. incorporated a CNN and a kNN regressor model in their SDL for automated metal halide perovskites.620 Repurposing a protein crystallization robot, the authors were able to study 96 experiments with different concentrations of precursors in parallel (Figure 40). The prepared solutions are then sealed in a chamber with an antisolvent to induce the crystallization.621 Initially, studies focused on phenethylammonium lead bromide, (C8H12N)2PbBr4 (PEAPbBr) crystallization, with 7000 images classified from an initial run as either no/bad crystals, or good crystals. This dataset was used to train a CNN, achieving 95% accuracy in crystal detection. Kirman et al. then applied the workflow to a new perovskite system, with 3-picolylammonium (3-PLA) as the ligand, and different lead halides. Additionally, a kNN model was trained to map the experimental conditions, as well as DFT-based descriptors of the precursors, to the success of the crystallization experiment. Another round of experiments was performed based on the predicted reaction parameters most likely to yield crystallization, and the authors were able to increase their rate of crystallization success from ∼1% to ∼10%. While the SDL still required human intervention to perform the crystallization, Kirman et al. demonstrated a fully closed-loop platform for perovskite crystallization, using AI methods to learn from an experiment to suggest the subsequent experiments, and improving upon their original results.",
    "7.1.2. Nanoparticles":"Nanoparticles (NPs) are another type of commonly studied optoelectronic material. By controlling their size, shape, structure, and composition, the absorption/emission intensity and wavelength, and their selfassembly behavior, can be controlled. A variety of methods have been developed to control NP synthesis, many of which are suitable for automated synthesis platforms. One of the first examples of SDL for NP optoelectronics is from Krishnadasan et al. in 2007.35 The authors used a microfluidics platform to control the injection of CdO and Se solutions into a reactor. The goal of the SDL was to optimize the emission of CdSe NP to some target wavelength, and maximize the intensity, combined together in a dissatisfaction utility function. The goal would be to minimize the dissatisfaction as a function of the controllable parameters using the SNOBFIT algorithm,622 which proceeds as follows: (1) the search space is discretized around sampled data points, (2) quadratic models are fit around each point, (3) the next point to sample is the model minima, (4) the model is refit using new experimental data. Krishnadasan et al. demonstrated a closed-loop SDL capable of minimizing the dissatisfaction function in a 3D search space, varying the flow rates of CdO, Se, and the reaction temperature. The optimization trace is seen in Figure 41, with the best experimental conditions identified at evaluation 71. In a related study from the same group, Li et al. studied the ligand-mediated synthesis of nanocrystals in cesium lead bromide NPs.623 Linear ligands of base:acid precursors are used to define the size and shape of the colloidal NPs. Rather than utilizing an automated experimental design, the authors systematically varied the reaction temperature and the ratio of base:acid ligands, and the effect on the photoluminescence spectra. Salley et al. developed a liquid-handling robotics platform for controlled seed-mediated synthesis of Au nanoparticles.624 Three different experiments were performed, manipulating not only the size, but also the shape of the synthesized nanoparticles. The fitness function maximized is dependent on the absorbance at particular wavelengths in the UV-Vis spectra. In order to close the loop, a GA was used to select for the next experimental conditions. In the first experiment, the authors maximized the absorbance of spherical AuNPs at 553nm, corresponding to a NP diameter of ∼80nm, by manipulating the volume of dispensed precursors. Then they moved onto Au nanorods; starting from a randomly selected set of parameters, the platform synthesized 10 generations of nanorods, with 15 experiments each, ultimately maximizing the fitness function, and converging close to the original parameters proposed in the literature.625 In the final experiment, the nanorods from the second experiment were used as the crystallization seed, generating NPs of octahedral shape, a shape with unknown optimal synthesis outcomes. The authors later extended their study to include automated exploration and optimization of multiple levels of seed-mediated AuNP synthesis.626 Jiang et al. extended the previously described automated platform with in-line optical spectroscopy in UVVis-IR, allowing for high-throughput characterization. The various hierarchical levels of seed-mediated synthesis were also automated and controlled via directed graphs. The results are fed into a custom GA based exploration algorithm based on the Multi-dimensional Archive of Phenotypic Elites (MAPElites) algorithm, as well as the sparsity of the synthesis conditions.627 Tao et al. further extended the use of microfluidics in the SDL development of metal nanoparticles by incorporating a machine learned surrogate model.628 Multiple rounds of closed-loop experiments were performed based on suggestions from the surrogate models trained on the results of previous experiments. The authors studied the formation of gold nanoparticles (AuNPs) under various concentrations of aqueous precursor compounds, and different reaction times. Solution concentrations were manually loaded into the platform based on the suggestions of the algorithm. Uniquely, the SDL optimized for multiple objectives calculated from the UV-Vis absorption spectra, such as the position, full-width halfmaximum, and intensity of the peak, through the use of a novel BO algorithm which allows for hierarchical multi-objective optimization.280,446 The authors successfully demonstrate optimization of NP spectral properties for both large and small AuNP, using kernel density regression, with the kernels estimated via a BNNs. Most recently, Low et al. presented an SDL that optimized the synthesis of silver NPs (AgNPs) using a multi-objective optimization algorithm dubbed Evolution-Guided Bayesian Optimization (EGBO).629 They develop a fully automated SDL for seed-mediated AgNP synthesis, integrating microfluidics, inline hyperspectral imaging, and closed-loop optimization. The optimization goals were to target a desired spectral signature for specific optoelectronic applications, maximize the reaction rate for high throughput, and minimize costly seed particle usage. The various objectives were modeled by a GP surrogate. The EGBO algorithm then combines a batched BO with qNEHVI acquisition function with an evolutionary algorithm, leveraging selection pressure to balance exploration and exploitation toward the Pareto front. Applying EGBO to the nanoparticle synthesis and various synthetic test problems, the authors demonstrate improved performance over state-of-the-art methods in terms of hypervolume convergence, uniform coverage of the Pareto front, and constraint handling. They also investigate pre-repair and post-repair strategies for handling input and output constraints, underscoring the importance of careful constraint treatment in self-driving laboratories. Colloidal synthesis of NPs has also been applied to inorganic lead halide perovskite NPs. Epps et al. developed a highthroughput microfluidic reactor platform with an in situ characterization module.630 Starting with CsPbBr3 quantum dots, the bandgap of the NPs was tuned via halide exchange reactions, via introductions of zinc halide precursors.631 The various precursors were varied to optimize for a joint fitness value comprised of the PLQY, emission linewidth, and the emission energy, which is related to the bandgap. The colloidal lead halide perovskite NPs were flowed through a custom inline module capable of absorption and photoluminescence UVVis spectrometry (Figure 42). The results were fed into a boosted ensemble of neural networks and the next synthesis conditions were selected via BO. The authors compared the optimization with other commonly used methods, e.g., SNOBFIT, and CMA-ES, and found superior performance with the neural networks. Additional performance gain was observed after pre-training the networks with supplemental experimental data. In related work from the research group, Abdel-Latif et al. modified the aforementioned platform to include multi-phase reactions (i.e., gas-liquid), allowing for inseries synthesis of CsPbBr3 quantum dots, and expanding the synthesis parameter space for lead halide perovskite NPs.632 To improvement the ensemble of neural networks, an initial round of 200 experiments were performed to pretrain the networks to predict the FWHM and energy of the photoluminescence spectra. Epps et al. further studied optimization of AI guided experimental design agent used in their SDL through a simulated experimentation platform.633 Using 1000 experimental data points on metal halide perovskite NPs, a surrogate model comprised of a series of GP models served as the HTE platform, and the model, fitness functions, and acquisition functions were tested and compared. Li et al. developed the MAOSIC (materials acceleration operating system in cloud) in order to look at chiral perovskite nanocrystals.119 This class of optoelectronics have shown strong optical activity, and have possible applications in spintronics, sensing, or optical communications.634 However, the controlling the chirality of such semiconductors is nontrivial. Li et al. utilized a microfluidics SDL with a cloud server for data storage and communication. A robotic arm is used for automated transfer of the synthesized NPs into a spectrometer, returning data on the absorbance and circular dichroism (CD) spectra (Figure 43). The SNOBFIT algorithm was used for experimental design, varying the temperature and the precursor concentrations. Synthesized NPs with strong CD intensities were extracted for further analysis via XRD and transmission electron microscopy (TEM). Vikram et al. applied the automated microfluidics approach to optimization of indium phosphide nanocrystals.635 In order to understand the kinetics of the growth and nucleation of the InP NPs, the SDL had a growth stage that spatially separates the various stages of NP synthesis for sampling and characterization. The experimental design of the SDL uses an ensemble of 25 neural networks for uncertainty estimation, predicting the polydispersity, and bandgap of the NPs from the synthesis conditions. And while the kinetics were not involved in the SDL optimization, the additional data on the stages of InP growth were analyzed afterwards. Additional work on lead halide perovskite NPs was conducted by Bateni et al., 636 doping the nanocrystals with cations in a flow reactor similar to those discussed prior (Figure 42).630,632 Similar to the halide exchange mechanism, cation doping reactions were performed in the microfluidics platform through the introduction of manganese acetate, dissolved in 1-octadecene and activated with oleic acid. Spectroscopy data from the synthesized NPs were then fed into an ensemble of 100 neural networks, and the next experiments were suggested based on a greedy BO strategy. The closed-loop optimization campaigns optimized for the peak energy, and the Mn:exciton emission peak area from the photoluminescence spectra, producing on-demand bandgaps and doping levels of the lead halide perovskite NPs. In a recent study from the same authors, Bateni et al. demonstrated Smart Dope, an SDL for multi-cation doping of the lead halide perovskite NP system.637 CsPbCl3 quantum dots were doped with both Mn and Yb cations; the successful doping was confirmed through off-line characterization. Varying the reaction temperature and the precursor flow rates, the optical features in the absorbance spectra, measured using in situ spectrometry, were optimized as proxies for the reaction yield, and Mn and Yb emission. Experimental design was done using BO with a similar ensemble of neural networks, first pretrained on an unbiased dataset of 150 NP synthesis experiments. The optimized Mn-Yb doped NP resulted in an impressive PLQY of 158%. Zhao et al. studied colloidal perovskite NPs in a highthroughput platform consisting of a robotic arm, and a series of modules for pipetting, and UV-Vis spectroscopic analysis.638 While the authors still use liquid precursors, the robot is capable of selecting solutions based on the suggestions of a ML algorithm without human intervention, and has the potential to perform more complex chemical tasks. The authors considered two different systems, AuNPs, and lead-free double-perovskite NPs (Cs2AgIn1−xBixCl6). To start, a literature search was performed to determine the best starting concentrations for AuNP synthesis, and the best surfactants and solvents to use for perovskite NP synthesis. Based on these results, a series of NPs were synthesized while systematically varying the experimental parameters, generating a database of absorption and photoluminescence data for AuNPs and double-perovskite NPs, respectively, along with data on the aspect ratio of the NPs from TEM and SEM images. The resulting datasets were then used to train a sure independence screening and sparsifying operator (SISSO) model, which identifies correlations between the target and compressed input descriptors.639 Based on the prediction of the models, the authors ran an additional iteration experiment, varying the concentrations and volumes of precursors to verify the predictions of the model. For AuNPs, the aspect ratios were measured, and for the double-perovskite NPs, the sizes of the crystals were measured; the created NPs matched the predictions provided by the SISSO model. However, no additional model training with the new results were performed, and no additional iterations were done. With further development of ML algorithms, more sophisticated methods of optimization were studied in the context of optimizing reaction conditions. Deep RL utilizes a neural network agent to decide the next experiments based on some policy. This policy is refined with each experiment, as the agent receives feedback from the environment, in this case the experimental result, in the form of rewards or punishments. Zhou et al. applied this optimization algorithm to finding the optimal conditions for organic reactions in a microdroplet reactor, as discussed in a previous section.336 The agent is a modified long short-term memory network (LSTM) capable of recursively learning from time-series data, such as data acquired over each iteration of experimentation. To overcome overfitting in the low-data regime, the authors pretrained the network on simulated data. Not only was the pretrained neural network based optimizer capable of optimizing the yield of the reactions, the model was able to successfully optimize the SDL synthesis of silver NPs for maximal absorbance at a particular wavelength. In a more recent study, Volk et al. presented AlphaFlow, an RL-driven SDL capable of optimizing CdSe/CdS core-shell NPs with a modular microfluidics platform, optimizing the optoelectronic material over 40 experimental parameters.640 Experimental planning was done using an LSTM agent over 20 steps, with a belief model comprised of an ensemble neural network regressor and a decision tree classifier. The regressor maps the action-state pair to the corresponding reward (based on spectral data), and the classifier determines if the actionstate pair is viable; both models are retrained over each iteration. The authors demonstrate AlphaFlow’s capability to optimize the sequence of injected precursors, and the volume and reaction time at each iteration.",
    "7.1.3. Molecules in Solution":"In addition to nanoparticles and crystalline materials, optoelectronic molecules are often the precursor to forming thin films and devices. Drawing from a long history of organic chemistry, small organic molecules can be formed from a myriad of organic reactions with careful control of initial organic fragments, much like the precursor solutions in synthesis of nanoparticles. While molecules in solution do not behave exactly the same as when in thin film form or in devices, they are more easily characterized and serve as a proxy to more complex morphologies of optoelectronics. In 2023, Koscher et al. 642 presented an SDL for designing dye molecules integrated with computer-aided synthesis planning, first exploring unknown regions through synthesizing diverse examples to ground the property models, then exploiting the trained models to realize top-performing candidates. The platform leverages automated molecular generation using a graph-completion model trained on existing data to propose new candidate molecules. Viable synthetic routes for these candidates are identified through automated reaction pathway planning with ASKCOS (Autonomous Synthesis Knowledge Cloud Organized System).450,643 Ensembles of message-passing GNNs are employed for automated property prediction, evaluating candidates for specific optoelectronic properties like absorption, lipophilicity, and photostability. Robotic arms, batch reactors, and an automated liquid handler are integrated for automated synthesis to execute the recommended multi-step reaction pathways and isolate products. Crucially, the property prediction models are continually retrained with new experimental data in a closed automation loop, improving their accuracy iteratively. This platform demonstrated both the exploration of unknown parts of chemical space, and the exploitation of important optoelectronic properties in dye-like molecules. Strieth-Kalthoff et al. demonstrated the closed-loop discovery of organic laser molecules across three different SDL platforms, asynchronously.641 The chemical space is defined through the combination of organic fragment building blocks into organic pentamer molecules (Figure 44), similar to previous work done by Wu et al. 644 in which the fragments are joined together through iterative Suzuki-Miyaura couplings. The synthesis was performed through a generalizable two-step one-pot protocol, handled by automated experimental platforms. Absorption and emission spectra were recorded for the in-solution molecules, from which the lasing performance is estimated using the spectral gain factor.645 Results were then uploaded to a database for coordination with the other laboratories. For decision-making, the authors used a GP model, with the molecules represented as embeddings extracted from a GNN. To overcome the issue of low amounts of experimental data, time-dependent density functional theory (TD-DFT) calculations were performed for the enumerated chemical space, and the descriptors generated from the calculations were used to train the GNN. In this transfer learning approach, the embeddings extracted from the GNN provided a stronger set of features for the GP regression task, which informed the subsequent experiments. Ultimately, this work discovered 21 novel gain materials with state-of-the-art lasing performance, of which the top three compounds were successfully tested in devices. The work of Angello et al. demonstrated an SDL focused on discovering organic optoelectronics with good photostability, particularly for organic photovoltaic (OPV) applications.646 Like in the previously discussed works on automated organic molecule synthesis,613,641 the chemical space was predefined through the combination of molecular fragments through iterative Suzuki-Miyaura coupling reactions.431 In this case, the fragments were acceptor and donor complexes connected by a bridge fragment; this is a common design for OPV applications, with light-induced charge separation encouraged by the difference in local electronic energy levels. The platform was capable of synthesis, purification, and structural characterization of the final compounds. The in-solution photostability was then approximated as a product of the spectral decay time, and the spectral overlap of the molecular absorbance and the solar irradiance spectra. In total, the closed-loop synthesis and characterization was repeated 5 times, with the experiments guided by Gryffin, a BNN-based BO algorithm capable of handling categorical parameters (such as the selected fragments).446 After the optimization, the authors further extended the work by using the experimental results from the SDL to perform physics-informed discovery. Whole molecule DFT calculations were performed on the entire space of possible molecules, and the extracted physicochemical descriptors were used to train SVMs. In this way, the experimental results of the SDL campaign were extended to the entire chemical space, and the predicted best and worst 7 molecules were synthesized to confirm the model predictions",
    "7.2. Thin Films":"Thin films offer another useful proxy for optoelectronic devices without the need for fabricating an entire device. Optoelectronic devices (e.g., light-emitting diodes, LEDs, photodetectors, and photovoltaics, PVs) are based on thin films (nm to μm in thickness) in order to simultaneously balance charge transport, and light absorption or emission. For example, in PV devices a thicker film maximizes the number of photons absorbed by the active layer. On the other hand, it is easier to extract charges from thinner films. Thin films are a useful proxy because they offer the ability to investigate larger length scales and the impact of processing and microstructure on important material properties such as photoluminescence, stability, or charge carrier mobility. Finally, thin films can be fabricated with relative ease through processes including spincoating and thermal evaporation, which can be easily automated and integrated into an SDL. A detailed discussion of data-driven automated synthesis and characterization of thin film optoelectronics and electronic polymers is also provided in other perspective articles.647,648 A study on the SDL synthesis of colloidal and thin film chalcogenide quantum dots handily demonstrates the strong effect the thin film configuration has on measured performance. Chalcogenides are a class of compounds primarily composed of chalcogen elements, such as sulfur, or selenium, combined with various other elements, and are often used in semiconductor technology and materials science. Stroyuk et al. used a novel method of using aqueous precursor solutions of chalcogenide NPs to form multinary quantum dots with composition Cu1‑xAgxInSySe1‑y (CAISSe).649 Previous work from the authors demonstrate that this method produces NPs of similar spectral properties as those directly formed from precursor metal complexes.650 The use of aqueous forms of the precursors allows for automated synthesis using microfluidics platforms. By varying the precursor solutions, the produced NPs vary in Ag/Cu metallic composition (x), and S/Se chalcogen composition (y). By depositing and evaporating the colloidal mixture, solid thin films were formed on glass plates analyzed alongside the colloidal form. Several photoluminescent properties were measured in the experiments, such as photoluminescence lifetime, energy, and rate constant. The experiments were repeated for the quantum dots with a shell composed of ZnS. Due to the relatively small parameter space of the synthesis, the authors simply interpolated between the data points, creating a 2D map of the best CAISSe compositions. In particular, we can see the quenching effects due to the different forms of the NPs, with the photoluminescence lifetime significantly suppressed for the thin film. A second iteration was not performed, however the authors described possible future work involving a ML approach for more complex experimental parameter spaces. MacLeod et al. demonstrate an SDL, named Ada, capable of optimizing thin film fabrication parameters.651 With a robotic arm, Ada is able to move samples between various stations that are responsible for the stages of thin film fabrication. The entire process starts with measuring out appropriate amounts of precursor solution, spin-coating the glass substrate with the material, and then annealing for a specific amount of time. Characterization involves measuring the reflection and transmission spectra in UV-Vis-NIR, and measuring the conductance. The material studied were thin films of spiroOMeTAD, an organic hole transport material used in perovskite solar cells, doped with cobalt (III). By varying the dopant concentration, and the annealing time, Ada maximized the electron hole mobility in the material, approximated as a ratio of the conductance and the absorbance. The results were fed into the BNN-based Phoenics BO algorithm.652 Ada performs subsequent experiments based on suggestions from Phoenics, with the best parameters for global maximum hole mobility identified within 35 experiments. Exploiting the capabilities of Ada, the same group later demonstrated the autonomous optimization of synthesis parameters for the combustion synthesis of Pd thin films. Notably, MacLeod et al. extended Ada with an X-ray fluorescence (XRF) microscope for localizing the Pd in the annealed film before performing the conductance measurements (Figure 45).653 Through the variation of annealing temperature and combustion fuel composition, the authors were able to optimize the Pareto front between annealing temperature and conductivity. Advances in thin film devices often include multinary films, blends of multiple optoelectronic materials that can affect the stability and performance of the devices. Langner et al. developed an SDL capable of fabricating up to 6048 organic polymer films a day, with the experimental planning done by the Phoenics algorithm.654,655 Two quaternary systems with different compositions were explored (Figure 46). The first was composed of P3HT, PBQ-QF, PCBM, and oIDTBr, while the second replaced PBQ-QF with the more common PTB7- Th (i.e., P3HT, PTB7-Th, PCBM, and oIDTBr). The liquid handling robotics platform drop-casts the organic semiconductors onto glass substrates, with variation in the four components that make up the thin films. The films were then exposed to metal halide lamps; absorbance spectra taken before and after the exposure were used to determine the photostability of the quaternary thin film blends. The authors performed a grid-search method in addition to the BO experiments. They found that the full SDL with ML based experiment planning was able to find the blends that were as stable as the grid-search in 27 samples, on average, showing the efficiency of combining HTE with data-driven experimental design. Sanchez et al. 323 proposes a workflow that combines structured GP models with custom physics-motivated mean functions and automated synthesis methods for the optimization of hybrid perovskite thin films with tunable bandgaps. The approach aims to accelerate optimization of properties like bandgap, photoluminescence, and absorption spectra by guiding experiments and reducing the required number of thin film preparations. By incorporating domain knowledge through custom mean functions, the structure GP converges more rapidly to the underlying ground truth compared to classical GP. The article demonstrates the application of this approach to study the bandgap evolution, photoluminescence peak shifts, and absorption spectra changes of MA1‑xGAxPb- (I1‑xBrx)3, a mixed-halide perovskite system relevant for tandem solar cells and tunable light emission. Experimental characterization included measuring bandgaps from absorption onsets, tracking photoluminescence peak positions and intensities, and monitoring absorption spectral features over a range of compositions. The workflow's adaptability to automated synthesis platforms is highlighted, enabling the exploration of higher-dimensional compositional spaces. The authors suggest that this approach could facilitate the discovery and optimization of advanced materials for optoelectronic applications in self-driving laboratories.",
    "7.3. Devices":"SDLs that can optimize whole devices are incredibly complex because they require integrating and automating multiple workflows with many highly complex experimental systems. However, this also makes it possible to directly test the performance of a device and control aspects from composition all the way to device architecture, which are rarely optimized simultaneously. As a result of the high degree of complexity of SDLs that optimize optoelectronic devices, there are only a few groups in the world with the resources to conduct such research. Despite this limitation, significant progress has been attained in recent years. Du et al. in 2021 developed AMANDA Line One, a robotic platform capable of automated multi-layer device fabrication and characterization.656 Rather than exploring chemical space, the device parameters were varied, similar to the group’s previous work in quaternary systems, described above. The platform used a robotic arm to move the sample between stations on AMANDA Line One for deposition of layers, thermal treatment, and optoelectronic measurements. The active compounds were PM6 and Y6, acting as donor (D) and acceptor (A) organic semiconductors, respectively. Various layers were deposited via spin-coating, with the PM6:Y6 active layer sandwiched between electron and hole conducting layers to form the device, shown in Figure 47. In total, 10 different processing parameters were varied in the device fabrication, optimizing for four figures of merit: open current voltage (Voc), short circuit current (Jsc), fill factor (FF), and the PCE. Due to the parallelized high-throughput nature of the platform, ∼100 process conditions were systematically explored; without an experimental planning algorithm, the best fabrication parameters were identified within these experiments, producing a device with a PCE of ∼14% in ambient conditions, aligning with the results from the literature.657 The authors utilized the data from the automated platform to train a GP regression model, correlating spectral features obtained from the absorption spectra to the figures of merit, which gave some physical insight for the differences in performance. However, the platform is not yet integrated with an automated experiment planner, nor was a second iteration performed based on the feedback from the GP model predictions. The following year, Liu et al. published their work on BO of perovskite solar cell devices fabrication parameters.658 Motivated by the rapid spray plasma processing (RSPP)659 technique for high-throughput fabrication of open-air perovskite cells, the authors aimed to find the best process parameters, varying the substrate temperature (°C), speed of the spray and plasma nozzles (cm/s), flow rate of precursor liquid (mL/min), gas flow rate into plasma nozzle (L/min), height of plasma nozzle (cm), and plasma duty cycle (ratio of time plasma receives DC power). A Guassian process was trained on batches of experiments, with the next iterations informed by the upper-confidence bound (UCB) acquisition function, in a BO setting. In all, 5 rounds with 20 devices each were performed, with significant manual work involved in the manufacturing and testing of the devices. The authors were able to more quickly identify parameters for higher PCE devices using their ML-guided experiment planning than previous experiments led by human decisions. While both works demonstrate significant advancements in the automation of the hardware and software for device optimization, there is still a lack of a fully closed-loop SDL for process optimization or material discovery for optoelectronic devices. However, the pieces to the puzzle show great potential, and an optoelectronic device SDL may only be a few years away.",
    "7.4. Outlook and Perspectives":"SDLs for characterizing optoelectronic materials in solution and in crystals are the most developed because of the ease of carrying out the requisite experiments. In-solution experiments, in particular, are the simplest to set up and execute, relying only on pumps and well-established spectroscopic equipment. SDLs for thin film characterization or device fabrication, on the other hand, require substantially more complex systems such as robotic arms for transporting samples and vacuum chambers for depositing interlayers and contacts. At the same time, these highly complex SDLs possess the greatest potential for accelerating design and discovery in optoelectronics. There are a number of challenges that stand in the way of fully realized SDLs for optoelectronic devices. Optimizing optoelectronic materials and devices requires taking into account numerous processes that occur at length scales from Å to μm. While this is challenging, larger quantities of data and improved ML or DL models can potentially overcome it. Next, the sheer number of possible variables�for example, selecting a material with appropriate properties, depositing a thin film and fabricating a device�can easily reach a design space that becomes challenging for BO. At the same time, the cost of the experiments is simply untenable for optimization algorithms such as RL or evolutionary algorithms. While a simple solution might be to optimize devices based on a handful of accessible materials, simultaneously optimizing material synthesis and processing would be transformative. Synthesizing a small amount of a new material in order to tweak how it responds to processing conditions has the potential to significantly accelerate the development of optoelectronics. However, this remains a long term vision due to the many challenges enumerated in the Reaction Optimization section on top of those discussed here.",
    "8. Energy Storage Materials":"Efficient energy storage systems are imperative to exploit the full potential of renewable energy sources, such as solar and wind, to reduce reliance on fossil fuels. The substantial amount of solar energy accessible on Earth could theoretically satisfy all human energy demands by powering photovoltaics and solar thermal systems. However, the intermittent nature of sunlight significantly limits the growth of solar power. For example, in California, peak solar power production during the day drives down the price of electricity, sometimes to negative territories, reducing the incentives for more installations.660 Efficient and powerful energy storage technologies can ensure a stable power supply by capturing the excess energy during the day and releasing it at night, mitigating reliance on fossil fuels. While optimization of battery designs and devices are possible, here we focus on the SDL development of new materials for improved energy storage. Electrochemical energy storage can be roughly divided into two broad categories based on the length of intended storage and speed of power delivery. Short duration energy storage technologies include capacitors and supercapacitors which charge and discharge within seconds to deliver high power. There are industrial efforts to automate the manufacturing of supercapacitors,661 however, SDL-driven discovery of new chemistry and materials is limited. Long-duration electrochemical energy storage is possible in batteries and redox flow batteries, as well as by converting the energy to liquid fuels such as alcohols or ammonia.662 Batteries store energy in the form of reversible chemical reactions in static and enclosed cells. They are relatively compact and inexpensive, suitable for mobile applications such as consumer products and electric vehicles. To scale up, cells can be assembled into battery systems, which require a dedicated battery management system. redox flow batteries (RFBs) also use reversible chemical reactions. However, the redox-active materials (RAMs) are solutions that can be stored in tanks and circulated through electrochemical reactors to generate power. RFBs can scale energy and power independently with higher tank volume and larger flow reactors, respectively, making them more scalable and cost-effective than batteries for grid applications, such as offsetting energy production and demand peaks.663 H2 gas and liquid fuels can store electrochemical energy off the grid, typically generated through non-reversible chemistry in flow reactors such as fuel cells and electrolyzers. Research in this area mainly focuses on cost-effective electrocatalysts that interconvert chemical energy and electricity. The major challenge in developing the aforementioned energy storage technologies lies in the need to develop specific materials and systems for different use cases. Compromises often have to be made to strike a balance between requirements. For example, lithium iron phosphate (LFP) batteries, a type of LIB, are widely used in electrical vehicles due to low cost, high thermal stability and long cycling life. But they are not suitable for cell phones because of lower energy density than most other LIBs. Even with the same choice of electrode materials, there is still a large and high-dimensional parameter space to explore to optimize the performance of batteries, including electrolyte formulation, cell configurations and assembly methods. SDLs can effectively address these complex problems, and offer large time and resource savings compared to traditional manual or high-throughput experimentations. An ideal SDL for energy storage should be able to automatically design, make, assemble, and test energy storage technologies at different scales. An end-to-end platform for battery or flow battery development without human intervention is a major challenge. The industrial production of batteries has undergone significant automation to achieve highthroughput and capital efficiency. These processes aim to carry out precise actions to maximize consistency and productivity in large-scale manufacturing processes, at the cost of flexibility for research and development. SDLs for battery research should focus on the ability to test new materials and optimize electrochemical processes in batteries of standardized form factors. In comparison, flow batteries operate on a large variety of redox chemistries, many of which have not been scaled up to industrial relevant levels. Therefore, SDL for flow batteries should focus on the screening and scaling of molecules and materials, and the optimization of these materials in realistic flow reactors.",
    "8.1. Materials Synthesis and Characterization":"A major focus of energy storage technologies is to develop materials that can improve device performance and longevity. The synthesis of materials for energy storage requires low cost and high scalability because they are aimed to be produced at massive scales. Therefore, low-cost feedstocks such as metals, metal oxides, and products and wastes of petrochemical processes are greatly preferred. There is also a drive to simplify preparation steps, and minimizing the need for solvents and purifications. In comparison, there is a stronger motivation to characterize materials as detailed as possible, whether as synthesized, in situ or even operando, to fully understand underlying processes. SDLs for energy storage will likely have a relatively small synthesis component, but relatively complex characterization capabilities. The common characterization methods include XRD, SEM, and solid-state NMR for solid state materials; thermal analysis for polymers; and most importantly, various electrochemical methods. The ability to conduct electrochemical analysis in a fully automated fashion is the prerequisite of any SDL that studies batteries, flow batteries, fuel cells or electrolyzers. However, this is often a challenge due to the fact that most commercial potentiostats, the instrument that performs electrochemical tests, are expensive and only operate with closed-source softwares, making them difficult to integrate into automated workflows. An example of such effort is the Electrolab, a modular electrochemistry platform by Oh et al. able to automatically formulate redox electrolytes and characterize them using cyclic voltammetry (CV) across various conditions without human intervention (Figure 48).664 Their platform integrates a liquidhandling robot to mix solutions and dispense them onto a series of cells containing an electrode array connected to a potentiostat (Figure 48C). After measurements are done, the robot can also de-gas and clean the cells. Electrolab was able to run a series of 200 CV scans in 2 hours (along with cleaning) on a known redox species under a variety of concentrations and scan rates. They then demonstrated a grid search to find the optimal conditions for a supporting electrolyte when scanning a candidate redox polymer for a nonaqueous RFB. Given the modularity of their setup, it seems possible to extend to smarter data-driven search algorithms to find interesting molecular candidates or remove the need for exhaustive grid search in the future. In recent works initially introduced by Hickman et al., 304 they demonstrated a low-cost SDL platform for electrochemistry discovery. The platform combines a synthesis platform, MEDUSA, and open source potentiostat for endto-end automated complexation and electrochemical characterization. Adapted to the ChemOS 2.0 orchestration framework,111 a closed-loop optimization of the redox potential of metal complexes for flow battery application was demonstrated using the Atlas optimization library.304 The low-cost and opensource features of such a platform make it accessible to a broad scope of researchers, therefore allowing for the democratization of SDL for community-based research. 8.1.1. Lithium-Ion Batteries. Lithium-ion batteries (LIB) are among the most influential technologies today, enabling the establishment of two hundred-billion markets: portable electronics and the electric vehicles.665 Since their commercialization in 1991, the energy density of LIBs has been improved by over 200%, but as the market grow and demand diversify, there is a need to optimize their performance, stability, safety, cost, and environmental footprint. Much of the conventional workflow is centered on trial-and-error approaches to find better materials. Given the enormous space of possible electrolytes, it is difficult and unreliable to screen active materials by manual experimentation. This motivates the need for SDL systems to improve LIBs. Data-driven ML methods for battery design have already been demonstrated on limited experimental dataset, such as for electrolyte formulation,666,667 and battery lifetimes.668 In general, a fully automated workflow is difficult and expensive due to sophisticated engineering requirements, whereas semi-automated platforms that investigate some aspects of the material are more feasible for researchers.669 Electrolyte formulation is one of the key research areas of LIB research. Most LIBs nowadays require a liquid electrolyte to conduct electricity with minimal undesirable chemical reactions. Dave et al. developed a pipeline to automatically measure the electrochemical properties of 10 different solutions in different compositions (251 total) for use in LIBs using a Bayesian optimizer they developed called Dragonfly.670 They later extended their pipeline to nonaqueous LIB electrolytes, which is a larger search space than aqueous electrolytes on account of co-solvents (although not necessarily a harder search space, as finding electrolytes that work in water is difficult).671 Their system could automatically create and characterize different solution compositions, although some human assistance was required when transferring electrolytes into pouch cells for characterization. Their search space consisted of over 1000 points over three axes: solvent mass fraction, co-solvent ratios, and salt molality. In both cases, Dragonfly found electrolyte compositions that were novel or non-intuitive and better than benchmark electrolytes. They also noted that their experiment planner resulted in a sixfold speed increase in finding the optimum compared to random searching by their robotics platform and postulated the same process would take far longer through manual search. Svensson et al. developed an automated screening platform for different electrolyte formulations, consisting of two platforms: a system to formulate and characterize electrolyte solutions and a system for coin cell assembly/disassembly and electrochemistry characterization, which are linked together using a robotic arm.672 While they only screened one electrolyte formulation as a proof-of-concept, their robotic platform was able to obtain similar measurements for assembled batteries compared with batteries assembled by hand, showing that this part of the battery development pipeline can be automated as well. Vogler et al. 132 demonstrate a brokering approach to orchestrate modular and asynchronous research workflows, enabling the integration of multiple laboratories for LIB electrolyte development. They implement a passive brokering server called FINALES to mediate communication between various tenants, which can be physical modules like experimental equipment, or digital modules such as machine learning agents or simulations software. The SDL comprises an experimental setup for automated synthesis and analysis of LIB electrolytes through a pump and valve system with stock solutions, and connected densimeter and viscometer. As another tenant, a simulation orchestrator using Pipeline Pilot for molecular dynamics simulations is used to calculate ionic transport coefficients, radial distribution functions, and other properties critical for electrolyte performance. An AiiDA interface provides ML models to predict low-fidelity conductivity values,673 and also for BO surrogate modeling. As a proof of concept, the authors aimed to maximize viscosity while minimizing density, using a GP optimizer combined with Chimera for multi-objective optimization. The demonstration successfully orchestrated these tenants across five countries, optimizing electrolyte formulations based on lithium hexafluorophosphate salt in carbonate solvents like ethylene carbonate and ethyl methyl carbonate. This SDL approach enables efficient screening and optimization of new electrolyte compositions to improve critical performance metrics like ionic conductivity, essential for developing next-generation LIBs. Another key research direction of solid-state electrolytes (SSE) for LIBs, such as metal oxide and polymer ion conductors, is to avoid the fire hazard and degradation issues caused by organic solvents. SSEs are also more compact than liquid electrolyte giving rise to higher energy density in batteries.674 However, finding solid-state materials with high ionic conductivity, low electrical conductivity, and high electrochemical stability is a major challenge.675 Currently there is no single best solid state electrolyte material for LIBs, partially due to limited understanding of lithium ion transport in bulk solids and at interfaces of different materials.676 Computationally, a number of works have developed frameworks to featurize solid state conductors and train ML models to either predict properties or recommend new conductors.677 He et al. developed a high-throughput screen platform which integrates a large database with modules that calculate iontransport-related properties and a hierarchical search algorithm to propose promising candidates.678 Laskowski et al. reported ML-guided synthesis of Si-doped Li3BS3 using solid-state reactions, reaching superionic conductivity above 10−3 S cm−1 , surpassing most reported SSEs.679 However, their discovery approach is neither automatic nor closed-loop. To our knowledge, fully automated close-loop SDL that discovers/optimizes solid-state electrolyte materials is much needed but very rare. One of the closest examples of an SDL was developed by Shimizu et al. 680 The Connected, Autonomous, Shared and High-throughput (CASH) laboratory integrated several components of an automated SDL, with some human-in-the-loop steps in initializing the synthesis step. The authors minimize the electrical resistance of Nb-doped TiO2 thin films by varying the oxygen partial pressure during the deposition of the film. Human intervention was required to load substrates and prepare the system for sputter deposition, after which the thin-film deposition and resistance characterization were carried out automatically. A robotic arm transferred the sample between the dedicated chambers (Figure 49). For experimental planning, a BO approach was utilized, with a GP regressor as the surrogate. The CASH SDL identified the global minimal resistance within 18 experimental samples for two different sputtering targets. The authors also outlined future plans to expand the characterization platform for multiple physical properties. Active electrode materials store ions over many chargingdischarging cycles and determine the battery's cell voltage. The chemical space of possible active electrode materials is large, ranging from graphite and Si-metal alloys to mixed-metal oxides and phosphate salts. By 2010, there were over 25,000 real and hypothetical negative electrode materials investigated, but experimental verification remains a bottleneck.681 SDL for the discovery of new electrode materials does not yet exist, to our best knowledge. However, the development of highthroughput methods, such as through physical vapor deposition or solution-based methods, have allowed for combinatorial exploration of electrode materials, for example, negative electrode Si-metal alloys,682,683 and positive electrode Li-Ni-Mn-Co-O or Li-M-PO4.684 In addition to the highthroughput synthesis of these materials, there are various highthroughput characterization methods for both structural and electrochemical characterization of electrode materials.685,686 By performing characterization in large batches, combinatorial searches of the material space can generate large datasets for future data-driven applications. McCalla outlined efforts and engineering bottlenecks using automated workflows to accelerate the design of battery materials, including solid-state electrolytes and electrode materials.669 Currently, semi-automated systems might be more feasible for most academic researchers because they balance speed, cost, and adaptability. Nonetheless, a review by Szymanski et al. discussed in detail the challenges and opportunities in close-loop optimization of inorganic materials for batteries, highlighting the importance of future SDLs for not only liberating human researchers from low-level manual tasks but also possessing the ability to explore new materials without being limited by the development of theories.686",
    "8.1.2. Alternatives to LIBs":"There has been extensive work on alternatives to LIBs, such as Li-O2 687 and Li-S688 batteries, looking to achieve many folds higher energy density; and sodium-ion batteries, aiming to reduce cost and the reliance on metal resources such as lithium, cobalt, and nickel.689 SDL can help develop crucial materials for these technologies which are still in early phases of research. Matsuda et al. demonstrated an SDL for the automated electrolyte synthesis for Li-O2 batteries.689 These batteries suffer from poor cycle performance due to low reaction efficiencies for the oxygen-generating (positive) and lithiumforming (negative) electrodes. As a result of the highthroughput screening guided by ML, they found multicomponent electrolyte additives for Li-O2 batteries that gave rise to a stable solid electrolyte interface. Their automated experiments covered a total of 14,460 samples, with 4,320 samples allocated to random search, another 4,320 for hill climbing involving the top 10 samples, an additional 4,320 for hill climbing with the top 50 samples, and finally, 1,500 samples for BO (Figure 50). Combination of the hill-climbing method with BO resulted in significantly improved Coulombic efficiency with all top 10 samples exceeding 91%.",
    "8.1.3. Redox Flow Batteries":" Since the debut of RFBs in the 1970s, researchers developed a variety of redox chemistries and technologies for RBFs, yet none of them have reached the scale of LIBs. Prior to 2015, RFBs primarily rely on inorganic salt RAMs, such as vanadium flow batteries (VFBs) and zinc bromide batteries. However, these batteries rely on scarce metal resources or highly corrosive operating conditions, which leads to a high cost of energy storage and maintenance. To achieve wide deployment, significant cost-reductions are needed to reach a target installation cost of $100/kWh and a levelized storage cost of $0.05/kWh.690 This is achievable by optimization of the electrolyte solution, and discovery of more cost-effective chemistries. Similar to LIBs, formulation of the electrolyte solution can result in enhanced performance. Gao et al. presented the Solubility of Organic Molecules in Aqueous Solution (SOMAS) dataset for advancing ML algorithms in the exploration of aqueous organic RFBs.691 In the case of nonaqueous flow batteries which use organic solvents instead of water, mixed-solvent and mixed-electrolyte systems can be explored. Deep eutectic solvents (DESs) are an attractive choice of solvent with low toxicity, broad commercial availability, and low costs.692 The properties of DESs can be fine-tuned with their composition. A recent study by Rodriguez et al. demonstrated a high throughput and data-driven search 185 quaternary ammonium salt (QAS) molecules identified as good candidate components for DES and synthesized DESs using liquid handling robots to combine these components. They tested several physical properties, including melting point, electrochemical potential window and ionic conductivity. It is worth noting this work is based on Jubilee,61 an opensource hardware machine based on 3D printing hardware with automatic tool-changing and interchangeable bed plates. Another major research direction is to find low-cost electrolytes made from earth-abundant and widely available resources, and operate in mild aqueous environments. Over the past decade, researchers have explored various small organic molecules,694 polymers,695 and metal coordination compounds696 to address the limitations of inorganic salts. The structural flexibility of organic molecules has facilitated the exploration of a broad spectrum of chemical and physical properties. They also resulted in a massive chemical space that is difficult to navigate with traditional computational and experimental methods. Currently, there is no SDL capable of completing the DMAT cycles of new redox materials. The design step can be achieved based on the computational screening of different molecular classes, such as generating new analogues by combining core structures and making peripheral substitutions,697,698 or using generative models and principles of inverse design.699 For example, Jinich et al. computationally assessed 315,000 metabolic-inspired redox reactions700 while S. V. et al. showcased the de novo design of radical species as both catholytes and anolytes.701,702 The above workflow narrows down the number of candidates that can be practically synthesized. However, the “make” capabilities in SDLs are restricted to producing molecules within the same class that can be synthesized under similar conditions. Additionally, precise electrochemical assessment of RAMs demands samples of high purity. Conducting tests in real batteries necessitates a considerable quantity of samples, prompting the need for scaling up synthesis (see the Reaction Optimization section). Recently, a new class of radical-based organic RAMs showed promise for higher-throughput exploration due to a simple SN2 substitution reaction scheme.703 A low-hanging fruit is the relatively straightforward synthesis of metal-ligand coordination compounds. Porwol et al. reported an autonomous chemical robot that can explore this process and discover the rules of coordination chemistry.477 This is an important step towards generalizable synthesis of different redox-active metal-ligand complexes, which can be used to generate suitable RAMs on demand. The test of new electrolytes focuses on the evaluation of key performance metrics, such as redox potentials, chemical stability and solubility. Liang et al. reported an important work on the high-throughput and automated solubility determination.704 Solubility is important because it determines the highest possible energy density of the electrolyte solution. There is a significant computational challenge in quantitative prediction of the solubility of organic electrolytes. The authors assembled a robotic system in an argon glovebox to experimentally measure solubility of electrolytes. They showcased the effects of additives on solubility in aqueous flow batteries and the development of solubility databases for non-aqueous systems. Most recently, Noh et al. 705 of the same research group presented an integrated high-throughput robotic platform and BO approach for accelerated discovery of optimal electrolyte formulations for non-aqueous RFBs, specifically the RAM 2,1,3-benzothiadiazole (BTZ). The goal of the study was to improve solubility of BTZ in organic solvents. The SDL carried out automated sample preparation through powder and liquid dispensing with a robotic arm. The solubility of BTZ in the solvent was measured via quantitative NMR spectroscopy. The BO component employs a surrogate GP model, operating on molecular features derived from physicochemical properties and DFT calculations of the solvent and solute. The authors identified multiple binary solvent systems with remarkable solubility thresholds exceeding 6.20 M from a comprehensive library of over 2000 potential solvents. Notably, their integrated strategy necessitated solubility assessments for fewer than 10% of these candidates, underscoring the efficiency of their approach.",
    "8.1.4. Hydrogen and Other Fuels":"Other than enclosed LIBs and flow batteries, electrical energy can also be stored in the chemical bonds of fuels. H2 has the highest gravimetric energy density, or specific energy, of any known chemical, although specialized materials and conditions are needed for its safe storage and transportation.706,707 Liquid hydrocarbons offer a high volumetric energy density, 708 making them easy to store and transport indefinitely. Currently, only hydrogen and methanol can be directly converted back to electricity in fuel cells.709 Ammonia is considered a good carbon-free energy carrier for the future,662 although the electrosynthesis of NH3 from N2 is still challenging. Hydrocarbons are challenging for direct fuel cell consumption due to CO poisoning and carbon deposition on catalytic surfaces. They have to be reformed to generate H2 for hydrogen fuel cells. The development of electrocatalysts is central to the development of both fuel cells and electrolyzers. One important topic is the sourcing of hydrogen from water via electrolysis using earth-abundant catalysts. Fatehi et al. outlined the design of an SDL that is designed to find such catalysts to address the sluggish oxygen evolution reaction in water electrolysis.710 They developed a framework for electrocatalyst SDLs consisting of three automated components: liquid handling, electrochemistry, and software that handles data and optimize experiment parameters (Figure 52). They use GP-based BO to find ideal material composition in a closed loop fashion. Their ultimate goal is to discover earthabundant mixed-metal oxide catalysts for OERs in an acidic medium. They demonstrated the optimization of CoFeMnPbOx electrodeposited catalyst materials through multiple campaigns. Within hours, they were able to identify successful formulae for catalyst synthesis and operational conditions that are corroborated with results in scientific literature. One interesting aspect of this work involved developing proxy measurements for target properties since the ideal characterization machinery was difficult to incorporate into the SDL. Fatehi et al. created a proxy measurement for stability by holding the material at an overpotential for an extended period of time, and found that it was helpful for optimizing within a space of materials. Another important work described the accelerated discovery of solid-state material in fuel cells which was discussed in a previous section on Solid state materials synthesis. The goal of the work by Ament et al. 517 was to autonomously design bismuth oxide thin films. The automated fabrication of Bi2O3 films of different phases have possible applications to thin film solid oxide fuel cells.711",
    "8.2. Device Design, Assembly, and Characterization":"",
    "8.2.1. Cell Batteries":"Another important area of automation in battery development is cell assembly. This is typically done manually in a research setting. Coin cells are relatively easier to prepare than other cell types and have low material costs, which are beneficial for quick battery prototyping.713 Zhang et al. developed AutoBASS, which automatically assembles 64 coin cell batteries per batch (Figure 53).712 They found that their system produces consistent and reproducible results across batches and parameters for a single electrolyte, which is promising both for improving quality control and increasing the speed of lead discovery. Yik et al. created ODACell, a 4-robot system which combines electrolyte formulation with automated coin cell assembly.714 While their batch throughput is smaller than that of AutoBASS (16 vs 64), their system can formulate different electrolyte compositions using a liquid handling robot, potentially allowing for easier integration with optimization algorithms in the future to search for ideal compositions. ODACell was used to test the impact of contaminants (specifically water) in non-aqueous batteries. The degradation of batteries was measured after being contaminated with different water concentrations, and it was found that the variance of the experiments increased at higher water concentrations when replicated multiple times. This illustrates how automation can effectively identify instances of failure or conditions characterized by elevated performance uncertainty",
    "8.2.2. Flow Reactors":"The flow reactors in flow cells, flow batteries, and fuel cells share some common design elements, which in themselves have an enormous parameter space to explore. An electrochemical flow reactor is intricate, typically composed of a complex stack of multiple layers, including separators (commonly ion-exchange membranes), electrode materials, current collectors, gaskets, flow plates that regulate flow fields, inlets of liquids and gasses, etc. In most cases, they are assembled manually, as is the system demonstrated by Li et al. The electrochemical flow reactor assembly is difficult to automate, but tests can be performed (1) in parallel reactors and (2) sequentially using the sample reactor by cleaning out the reactor before the measurement. The research goal on the device level is often the optimization of performance by searching the parameter space of reactor design (flow field and materials) operating conditions (electrical and flow system management), and exhaustive monitoring of device degradation or failure over time. As shown in Figure 54, the complex configuration of the electrochemical flow reactors often leads to reproducibility issues since slight misalignments and different applied pressures likely lead to varied device performance. Currently, only fuel cells based on proton exchange membranes are known to be assembled automatically in industry, using highly expensive commercial setups.716,717 For instance, Thyssenkrupp Automation Engineering GmbH demonstrated a commercial plant that produces one electrolyzer per second, or at least 50,000 fuel cell stacks yearly. Such manufacturing maturity and scalability has not been realized in the production of flow reactors for flow batteries and electrolyzers. Other than the exploration of the chemical space of RAMs, a secondary discovery process is conducted to explore the electrochemical parameter space of the RAMs, with a primary focus on optimizing battery performance. This exploration encompasses various factors, such as formulating electrolyte solutions, pairing posolyte and negolytes, determining properties of the membrane and electrode materials, designing the flow field, specifying flow rates, and other related considerations. The electrochemical parameter space is extensive and can become complex as more realistic factors are taken into account. While this exploration has been extensively conducted in a few inorganic systems, particularly in strongly acidic vanadium batteries, the realm of emerging organic RAMs operates under distinct conditions. This necessitates the use of new materials and battery designs for both aqueous and nonaqueous systems with different testing conventions.718 Recently, the Aziz group reported an important step toward high-throughput flow battery testing by miniaturizing the flow batteries using a modular design.71",
    "8.3. Outlook and Perspectives":"Energy storage technologies play a crucial role in achieving sustainability objectives. Generally, there is a strong industry effort on automation which guarantees robustness in production and product reliability. There is also strong academic research on ML for sustainable energy that expands beyond electrified energy storage to more general electrochemical sciences.720,721 It is necessary to combine efforts in both communities, i.e., automation and ML expertise, to supercharge the discovery of advanced energy storage materials. For instance, LIB manufacturing has been highly automated in the industry. However, there is still a large design space for battery material discovery and optimization and battery design improvements. There exist a number of opportunities to advance SDLs in the field of electrochemical energy storage. Integrating operando spectroscopic and electrochemical analysis of materials and devices, especially in flow reactors, will provide additional training data and a better understanding of failure mechanisms. As with optoelectronic materials and devices, simultaneously optimizing (co-designing) the different component materials in a device is an important potential contribution of SDLs. In most cases, RAMs for flow batteries are developed separately from the membrane or electrode materials. This could result in a mismatch of new RAMs and existing reactors, and thus subpar battery performance. Finally, SDLs capable of carrying out large scale or long term experiments on top candidates generated by another SDL could provide most realistic data about materials and devices for large, long-term energy storage systems. We also predict the explosive growth of SDLs due to the rapid development of ML, automation, and significant investments from both private and government-led initiatives. The latter includes campaign to automate the discovery of new batteries and flow battery materials, including the European Battery2030 initiative,722 the Department of Energy’s efforts at Argonne National Laboratory723 and Lawrence Berkeley National Laboratory,724 and Canada’s CA$200 million investment in the Acceleration Consortium.589 The U.S. government also recently announced a US$7 billion investment from the Department of Energy to build seven hubs of H2 infrastructure in the USA, which can significantly accelerate the implementation of SDLs for fuel-based energy storage.",
    "9. Conclusion and Outlook":"The evolution of SDLs within chemical and materials science promises to usher in a transformative era of scientific exploration. In this review, we have provided a comprehensive overview of SDL systems, both past and present, for a variety of applications. Early autonomous systems have been enhanced by rapid development of better automated chemistry platforms, improved AI-based experimental planning algorithms, and the availability of large, high-quality datasets fueled by advancements in information technology and computational power. Many Level 3 and 4 SDLs have already demonstrated impact in accelerating reaction process optimization, functional property refinement, and novel chemical and materials discovery. Further development of both custom and general automation systems has also significantly reduced the barrier to entry. Progress towards next-generation SDLs signals a Chemical Reviews Review https://doi.org/10.1021/acs.chemrev.4c00055 Chem. Rev. 2024, 124, 9633−9732 9709 paradigm shift in which we believe SDLs will transition from systems designed and operated by specialists to everyday tools, similar to those brought about by the development of other now ubiquitous tools such as NMR, HPLC, MS, XRD, TEM, and SEM. However, there are also important potential challenges in the future of SDLs. Most immediately, many contemporary SDL systems are very complex and expensive. Realizing the full potential of SDLs requires a concerted effort from the scientific community to embrace open-source software and hardware, democratizing access to these technologies and fostering collaboration. Numerous challenges must be addressed, including the development of robust and user-friendly interfaces, the establishment of standardized protocols and data formats, and adherence to the FAIR principles for data sharing. Effective implementation of FAIR data practices is crucial in enabling researchers worldwide to leverage the wealth of information generated by SDLs, and promoting transparency and reproducibility in chemistry and materials science. Initiatives, such as the creation of low-cost SDL prototypes and educational programs, play a vital role in empowering future scientists to navigate and contribute to this evolving multidisciplinary field. The growing importance of ML, automation, and SDLs is also forcing us to rethink education and workforce development in the physical sciences, where curricula often remain unchanged from the late 20th century. We also envision independent non-profit organizations capable of developing the talent within the community, building the ecosystem for collaboration, and supporting a platform to include private industry in collaborations that respect proprietary concerns. As we continue development of SDLs, a key consideration lies in the role of human researchers in the scientific discovery process. As SDL technologies become more mature and widespread, the role of the researcher may shift toward translating the results from autonomous experimentation into scientific understanding,725 which may be coupled with advances in explainable AI.726−729 We foresee that human ingenuity will remain important in the discovery of new chemical and physical phenomena, novel classes of materials, and advanced laboratory techniques and technologies. Furthermore, while the Level 5 SDL is the pinnacle of autonomous experimentation, the human-in-the-loop Level 4 SDL will continue to be valuable, and perhaps even preferred, due to the adaptable and innovative nature of human problemsolving.730 For such diverse and multidisciplinary fields as chemistry and materials science, the flexibility and modularity provided by semi-autonomous systems will be vital to SDL development. Moreover, as the barrier to chemistry and materials discovery becomes lower, and the process becomes faster, the potential misuse of SDLs for malicious purposes underscores the societal responsibility of researchers, the need for ethical guidelines, and the promotion of responsible implementation in industry. Striking a balance of economic considerations, ethical standards, and societal welfare is imperative to ensure the constructive and beneficial deployment of SDLs. While the challenges are formidable, the potential benefits of a fully realized SDL ecosystem are substantial, and such an ecosystem will be the future of chemical discovery. By fostering a collaborative environment, promoting transparency, and aligning efforts towards a shared objective, the chemistry and materials science communities can accelerate the pace of scientific discovery, explore new frontiers of knowledge, and drive innovation in ways that were previously unattainable.",
    "A 1.1": "Figure 1: The evolution of automation in chemistry and materials science. This timeline illustrates the historical progression from manual experimentation to the modern era of self-driving laboratories (SDLs). The progression is divided into four stages: Manual (Pre-1990s), High-Throughput Experimentation (HTE) (1990s-2000s), Automated Synthesis & Characterization (2010s), and Self-Driving Laboratories (2020s-Present)[cite: 85, 86]. Key milestones are highlighted, such as the development of combinatorial chemistry, the introduction of robotic arms for sample handling, and the integration of AI for closed-loop optimization[cite: 87, 88].",
    "A 1.2": "Figure 2: Levels of autonomy in self-driving laboratories. This diagram defines a hierarchical classification system for laboratory autonomy, ranging from Level 0 to Level 5. Level 0 represents fully manual operations, while Level 1 introduces assisted automation for specific tasks[cite: 142, 143]. Level 2 involves batch automation, and Level 3 represents conditional autonomy where the system can execute complex workflows but requires human intervention for decision making[cite: 144, 145]. Level 4 is high autonomy, where the system operates largely independently with AI-driven decision making, and Level 5 is full autonomy, where the laboratory operates completely without human intervention, capable of self-repair and self-improvement[cite: 146, 147].",
    "A 1.3": "Figure 3: A system-level perspective of SDLs. This schematic illustrates the functional architecture of a self-driving laboratory, divided into the 'Brain' and the 'Body'. The 'Brain' (software) encompasses data management, AI/ML algorithms (e.g., Bayesian Optimization), and experimental planning modules[cite: 203, 204]. The 'Body' (hardware) consists of robotic platforms, synthesis modules, and characterization instruments[cite: 205]. The workflow forms a closed loop: the Brain plans experiments, the Body executes them, data is analyzed, and the loop repeats to optimize target properties[cite: 206, 207].",
    "A 1.4": "Figure 4: Data and AI agents in SDLs. This figure details the data flow and artificial intelligence components within an SDL. It highlights the role of Large Language Models (LLMs) and foundation models in parsing literature data to inform experimental priors[cite: 289, 290]. The diagram shows how active learning agents, specifically Bayesian Optimization, utilize acquisition functions to explore and exploit the chemical space[cite: 291, 292]. It also depicts the integration of simulation data (e.g., DFT calculations) to augment experimental datasets[cite: 293].",
    "A 1.5": "Figure 5: The current landscape of SDLs. A map categorizing existing self-driving laboratories based on their application domain and degree of autonomy. The domains include organic synthesis, inorganic materials, thin films, and nanomaterials[cite: 350, 351]. Specific examples of SDLs are plotted, such as 'Ada' for thin films and the 'A-Lab' for inorganic powders, showing their relative positions in terms of throughput and autonomous capability[cite: 352, 353]. The figure emphasizes the diversity of materials currently being accelerated by SDL technologies[cite: 354].",
    "A 1.6": "Figure 6: Challenges and future directions. This diagram summarizes the primary hurdles facing the widespread adoption of SDLs and the roadmap for the future. Key challenges identified include lack of standardization in hardware and software (interoperability), data scarcity and quality issues, and the high cost of infrastructure[cite: 412, 413]. Future directions point towards the development of 'Robotic Chemists' (humanoid robots), cloud-based laboratories, and the creation of universal foundation models for chemistry that can generalize across different experimental setups[cite: 414, 415].",
    "A 2.1": "Figure 2: Examples of types of automated hardware. The figure categorizes automated hardware for SDLs into three types (a-c) and supporting software (d-e)[cite: 121]. Examples include: (a) the OT-2 platform by OpenTrons (specialized hardware), (b) a robotic arm for chemical operations (general-purpose hardware), (c) the Sidekick liquid dispensing platform (open hardware), (d) a computer vision framework for glassware, and (e) solid weighing simulation software (digital twins)[cite: 121].",
    "A 2.2": "Figure 3: Summary of the software components of SDLs. A schematic diagram illustrating how the software components interface with the hardware and with each other[cite: 147]. The 'Orchestrator' sits at the center, communicating with 'Automated hardware', 'Public/Private databases', and the 'Exp. planner' (Experimental Planner) [cite: 140-147]. The workflow creates an autonomous closed-loop laboratory where experimental results are analyzed and fed back into the planner to generate new experimental recommendations[cite: 140, 147].",
    "A 3.1": "Figure 5: Autonomous optimization of chromatographic separation by varying the eluent composition. Top: Schematic visualization of the experimental setup used by Berridge, and structures of four analytes in a sample mixture. Bottom: Visualization of the simplex optimization performance upon variation of eluent composition and flow rate (left). Development of the chromatographic response function (CRF), the objective, throughout the course of the optimization campaign (middle); the optimized chromatogram (right)[cite: 224].",
    "A 3.2": "Figure 6: Purity optimization results in multi-step reaction-extraction process. Four parameters are varied: the temperature, solvent ratio (VR), the residence time, and the inlet pH. The size of the dots corresponds to the temperature, and the color represents the purity of N-benzyl-a-methylbenzylamine after the reaction. The star is the optimal purity obtained at experiment 53[cite: 245]."
    
  }
}