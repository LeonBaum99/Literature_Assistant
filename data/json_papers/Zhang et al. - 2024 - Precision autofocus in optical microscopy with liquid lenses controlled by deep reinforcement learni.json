{
  "filename": "Zhang et al. - 2024 - Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learni.pdf",
  "metadata": {
    "title": "Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning",
    "authors": [
      "Jing Zhang",
      "Yong-feng Fu",
      "Hao Shen",
      "Quan Liu",
      "Li-ning Sun",
      "Li-guo Chen"
    ],
    "arxiv_id": ""
  },
  "sections": {
    "Abstract": "Microscopic imaging is a critical tool in scientific research, biomedical studies, and engineering applications, with an urgent need for system miniaturization and rapid, precision autofocus techniques[cite: 1306]. However, traditional microscopes and autofocus methods face hardware limitations and slow software speeds in achieving this goal[cite: 1307]. In response, this paper proposes the implementation of an adaptive Liquid Lens Microscope System utilizing Deep Reinforcement Learning-based Autofocus (DRLAF)[cite: 1308]. The proposed study employs a custom-made liquid lens with a rapid zoom response, which is treated as an \"agent\"[cite: 1309]. Raw images are utilized as the \"state\", with voltage adjustments representing the \"actions\"[cite: 1310]. Deep reinforcement learning is employed to learn the focusing strategy directly from captured images, achieving end-to-end autofocus[cite: 1311]. In contrast to methodologies that rely exclusively on sharpness assessment as a model's labels or inputs, our approach involved the development of a targeted reward function, which has proven to markedly enhance the performance in microscope autofocus tasks[cite: 1312]. We explored various action group design methods and improved the microscope autofocus speed to an average of 3.15 time steps[cite: 1313]. Additionally, parallel \"state\" dataset lists with random sampling training are proposed which enhances the model's adaptability to unknown samples, thereby improving its generalization capability[cite: 1314]. The experimental results demonstrate that the proposed liquid lens microscope with DRLAF exhibits high robustness, achieving a 79% increase in speed compared to traditional search algorithms, a 97.2% success rate, and enhanced generalization compared to other deep learning methods[cite: 1315].",
    "Introduction": "Microscopic observation serves as a crucial instrument in scientific research, biomedical studies, and engineering applications, and has been extensively utilized across various domains, including medicine, materials science, and industry[cite: 1317]. However, traditional microscopes, characterized by intricate optical systems and focusing mechanical structures, are often cumbersome and susceptible to damage, thereby imposing significant constraints on scientific operations and application scenarios[cite: 1318]. Microscopic imaging necessitates the precise capture of samples with intricate details across an extensive range, rendering the continuous maintenance of optimal focus critically important[cite: 1319, 1323]. Consequently, the miniaturization of microscopic imaging systems and the development of rapid autofocusing techniques have been longstanding objectives in relevant fields, aimed at addressing the continually evolving demands of scientific and technological advancement[cite: 1323]. The construction of traditional microscopes typically incorporates a combination of multiple fixed-focus lenses and mechanical structures, which are employed to achieve imaging functions such as magnification and focusing[cite: 1324]. Additionally, they necessitate an adequate optical path length to enable the requisite mechanical movement for focus adjustment[cite: 1325]. Consequently, these designs are inevitably encumbered by drawbacks including bulky volumes, sluggish focusing speeds, and difficulties in enabling rapid autofocusing or operation within confined spaces[cite: 1326]. In contrast, owing to the absence of mechanical components and the ability to achieve focusing by adjusting electrical signals, liquid lenses offer advantages such as compact size, rapid response, and low manufacturing costs[cite: 1327, 1334]. Microscopes equipped with liquid lenses do not require additional mechanical parts for focusing, which effectively reduces the overall volume and enhances the efficiency of autofocusing[cite: 1335].  The field of microscope autofocus technology has witnessed considerable advancements over the past few decades[cite: 1336]. The advent of artificial intelligence and new optical components in recent years has led to the emergence of novel research trends in this field[cite: 1337]. Active autofocus microscopes employ the transmission and reception of specific signals to measure the distance to the object and achieve focus[cite: 1338]. While these methods are highly reliable and have rapid focusing speeds, they necessitate specialized hardware support, high system complexity, and high cost[cite: 1341, 1342]. Furthermore, passive microscope autofocus methodologies, which assess the extent of out-of-focus and identify the focusing position exclusively through images, have been extensively investigated due to their advantageous simplicity and cost-effectiveness[cite: 1343]. However, these methods usually require multiple image acquisitions and evaluations, and the focusing speed is slow[cite: 1346]. To enhance the speed and precision of focusing, some scholars have proposed the integration of deep learning into microscope autofocusing techniques[cite: 1347]. Nevertheless, these approaches are contingent upon the quality and quantity of the training data, which may limit its efficacy in scenarios that are not previously encountered[cite: 1353, 1354]. Furthermore, these methodologies fail to consider the sequence data embedded within the focusing process, which is challenging to fully utilize and optimize[cite: 1354]. Deep Reinforcement Learning is capable of learning the optimal decision-making strategy through continuous trial-and-error interaction with the environment[cite: 1355]. In addition, the system is capable of utilizing sequential information to derive a more objective strategy, which is particularly well-suited to the autofocus task[cite: 1356]. While these studies have contributed to the advancement of autofocusing techniques for microscopes, several challenges remain[cite: 1360]. (1) System complexity and cost: The autofocus of liquid lenses still necessitates the establishment of a focal-voltage model to adjust the electrical signal focus following the calculated out-of-focus distance, which consequently increases the pre-calibration workload[cite: 1361, 1363]. (2) Dependence on evaluation metrics: The annotation of some datasets still relies excessively on image quality evaluation values[cite: 1365]. Consequently, a reliance on image quality ratings alone may result in the model acquiring a restricted range of knowledge[cite: 1367]. (3) Insufficient generalization ability: Deep learning-based models are dependent on training data and require a substantial quantity of diverse data to enhance network performance[cite: 1368]. This results in a homogeneous nature of the training data, which in turn affects the model's generalization ability[cite: 1371]. To address the aforementioned issues, we proposed the implementation of an adaptive Liquid Lens Microscope System that utilizes Deep Reinforcement Learning-based Autofocus (DRLAF)[cite: 1372, 1394]. By leveraging the rapid response and electrical adjustment advantages of liquid lenses, this methodology employs sequential raw images as the \"state\" input for the deep reinforcement learning agent, to enable the model to discern objective focusing knowledge from them[cite: 1394]. Concurrently, different voltage adjustments are regarded as executable \"actions\" and deep reinforcement learning is employed to optimize the focusing policy[cite: 1395]. Additionally, the model is enhanced through the utilization of random sampling from parallel \"state\" datasets during the training phase, thereby facilitating its ability to generalize and adapt to unknown samples[cite: 1396].",
    "Result and discussion": "System review: Figure 1 illustrates the overall architecture of the proposed Liquid Lens Microscope System utilizing DRLAF[cite: 1407]. As shown in Fig. 1a, we constructed a microscope system equipped with a liquid lens, which operates on the principle of electrowetting for zoom control[cite: 1408]. The host computer runs the proposed autofocus model, based on deep reinforcement learning, which is trained using the captured image dataset[cite: 1411]. The model is capable of autonomously determining the requisite focus adjustments based on the characteristics of the current image, thereby facilitating expeditious end-to-end autofocus[cite: 1412]. Figure 1b shows the training images from different samples, including micro/nano-structured surfaces with high-contrast regular patterns, resolution test targets with medium-contrast similar patterns, and laser-processed metal substrates with medium-contrast irregular patterns [cite: 1413].\n\nPreparation and performance of the liquid lenses microscope system: The microscope was equipped with a constructed liquid lens module, which was positioned at the diaphragm location[cite: 1469]. The module was driven by a voltage source via flexible electrodes, thereby enabling the host computer to regulate the voltage[cite: 1470, 1471]. Through investigating the influence of different cavity structures and dielectric materials and thicknesses on the performance of liquid lenses, a high-performance liquid lens was successfully fabricated[cite: 1481]. The lens utilizes a 5 µm thick Parylene C layer as the dielectric layer and a 180 nm Teflon AF layer as the hydrophobic coating[cite: 1482]. Figure 2d and e illustrates the response time of the constructed liquid lens and the relationship between its focal length and driving voltage under different magnification levels of the microscope, respectively[cite: 1483]. As can be observed from Fig. 2e, the focal length of the liquid lens varies at different objective lens magnifications under the same driving voltage[cite: 1484]. Therefore, for a microscope system integrated with such a liquid lens, solely relying on constructing a focal length-voltage model and performing autofocus based on this model requires substantial prior calibration work, and accuracy is difficult to guarantee[cite: 1485]. It is necessary to apply intelligent focusing algorithms in the liquid lens microscope system to achieve fast and precise end-to-end autofocus [cite: 1486].\n\nEffect of actions on autofocus performance: The autofocus adjustment actions include forward adjustments, backward adjustments, and stop actions[cite: 1488]. To determine the optimal action space size for autofocus, the performance of the model with different action space sizes was studied[cite: 1491]. Figure 3a shows the distribution of the influence of action space size on the focusing deviation of the autofocusing results[cite: 1492]. It can be observed from the figure that the size of the action space significantly affects the focusing deviation of autofocusing[cite: 1497]. As the number of actions increases, the deviation distribution tends to converge to 0 V, leading to a significant reduction in focusing deviation[cite: 1498]. Figure 3b shows the impact of the action space size on the success rate and accuracy of autofocusing[cite: 1502]. As the action space size increases, both the accuracy and success rate of autofocusing significantly improve[cite: 1503]. Figure 3c illustrates the influence of the action space size on the number of autofocusing time steps and accuracy on the test set[cite: 1504]. As the action space size increases, the average number of autofocusing time steps decreases significantly and stabilizes, while the average accuracy increases significantly[cite: 1505]. Given that an action set with 7 actions achieves better focusing performance, Table 1 compares the accuracy and time steps for different configurations of these 7 actions[cite: 1511]. Overall, the action sets determined by the logarithmic method with smaller bases are more suitable [cite: 1515].\n\nEffect of state random sampling in training on autofocus performance: The proposed method introduces a deep reinforcement learning approach trained by random sampling from multiple state datasets[cite: 1519, 1520]. This approach is aimed at enhancing the model's autofocusing success rate and improving its generalization across diverse samples[cite: 1575]. Figure 4 illustrates the impact of state dataset list size on the model's autofocusing performance[cite: 1580]. It is observed that with an increase in the number of state datasets, the focusing deviation significantly decreases, trending converge to 0 V[cite: 1583]. As the number of state datasets increases, both the accuracy and success rate of autofocusing notably improve[cite: 1584]. Additionally, the model's performance on the test sets gradually surpasses that on the training sets[cite: 1585]. With 50 datasets, the model achieves a 97.2% success rate on the test set, along with a root mean square error (RMSE) of 2.85 x 10^-3 V for the predicted voltage, indicating an improved ability to generalize[cite: 1586, 1593]. This trend may be attributed to the model learning more comprehensive and diverse information as the number of state datasets increases [cite: 1594].\n\nGeneralization experiments: To further evaluate the model's generalization capability on unknown samples, we conducted specific autofocusing experiments[cite: 1664]. Three methods are tested on completely unfamiliar sample datasets[cite: 1665]. Compared to Method 2 (trained on single sample) and Method 1 (deep learning), Method 3 (DRLAF trained by random sampling from different samples) demonstrates significantly higher autofocusing success rates on unfamiliar datasets[cite: 1670]. This result further validates the effectiveness of the random sampling training strategy in enhancing model generalization [cite: 1671].\n\nAblation experiments on the reward function: To thoroughly analyze the impact of the proposed hybrid reward function design and the contribution of each reward component to the algorithm's performance, a series of ablation experiments were conducted[cite: 1682]. Reward 5 (the proposed reward): Sharpness + Time Step + Stop + Additional Reward (Table 3)[cite: 1684]. The figures illustrate that during training, Reward 5 exhibits the best convergence across all three samples[cite: 1687]. The results presented in the figures indicate that Reward 5 is the most effective reward function formulation investigated, as it exhibits consistent convergence across different samples while achieving a significant reduction in the number of time steps required [cite: 1904].\n\nAutofocus experiment: Figure 7 illustrates the imaging performance of the proposed autofocus algorithm under different samples and random initial defocus states[cite: 1906]. Following autofocusing by the proposed method, the resulting images (Fig. 7g-l) demonstrate rapid adjustment to achieve well-focused images regardless of the initial defocus severity, with clear details and significantly improved corresponding evaluation metric values[cite: 1908, 1909]. To assess algorithm performance, we compare the autofocus results with manually focused results (Fig. 7m-r)[cite: 1913]. Although distinguishing between the two is difficult when observed directly, the precision of the algorithmic output images is comparable to, and in some instances exceeds, the level of manual focusing, as indicated by evaluation metrics[cite: 1914]. To provide a more comprehensive assessment of the proposed autofocus method, two additional approaches, namely the golden section search-based and the Fibonacci search-based autofocus methods, are implemented in the system[cite: 1917]. Experimental results demonstrate that the proposed model achieves autofocus in an average of only 3.15 steps, reducing the time by 79% compared to the golden section search-based method and by 60.63% compared to Fibonacci search-based methods[cite: 1920].",
    "Conclusions": "This study proposes an innovative liquid lens microscope system that achieves rapid and precise autofocus by utilizing deep reinforcement learning[cite: 1921]. Firstly, a liquid lens driven by the electrical dielectric wetting principle was fabricated, offering the advantages of small volume and fast response speed, which can effectively improve the structural compactness and zoom speed of microscopes when integrated[cite: 1922]. Secondly, an end-to-end autofocus is achieved by training a deep reinforcement learning model, further enhancing the focusing speed[cite: 1923]. Concurrently, a reward function tailored for the autofocus task was designed, enabling the model to focus more rapidly and autonomously[cite: 1924]. Furthermore, several action group design methods were introduced, which effectively enhance the speed and accuracy of autofocus by adjusting key parameters[cite: 1925]. In the experiments, an average of 3.15 steps was required to achieve autofocus, representing a 79% and 60.63% improvement in speed compared to traditional search algorithms[cite: 1926]. Additionally, a novel method for random sampling from multiple \"state\" dataset lists was proposed to address the limitation of model sensitivity to only the trained data[cite: 1927]. By increasing the number of state datasets, the model's ability to extract common features was significantly enhanced, enabling reliable autofocus across different samples and fields of view[cite: 1928]. With the expansion of the state datasets to 50, the model achieved a 97.2% success rate, with a root mean square error (RMSE) of 2.85 x 10^-3 V for the predicted voltage on the test set[cite: 1929]. This result demonstrates that the trained agent developed a robust autofocus strategy that is not dependent on the training data, thereby improving its generalization capability[cite: 1930]. The proposed liquid lens microscope system utilizing DRLAF significantly simplifies the structural complexity, enhances system compactness, reduces operational difficulty, and increases focusing speed[cite: 1931].",
    "Materials and methods": "Liquid lens: We implemented adjustable focusing functionality using a custom-made liquid lens based on the electrowetting-on-dielectric (EWOD) effect[cite: 1933]. As depicted in Fig. 1a, the liquid lens consists of an upper glass cover plate, a lower ITO cover plate, a chamber, a Parylene C dielectric film coated on the chamber, and a hydrophobic Teflon AF film[cite: 1934]. The Parylene C film, with a thickness of 5 μm, was prepared using chemical vapor deposition, while the 180 nm-thick Teflon film was fabricated via spin-coating[cite: 1935]. It is noteworthy that we designed the cavity with a conical structure featuring rounded corners [cite: 1936].\n\nDataset processing: In this work, we utilize the image sequences collected during the autofocus process of the liquid lens as the state input for the reinforcement learning agent, aiming to provide it with sufficient visual information to guide the learning of the autofocus strategy[cite: 1940]. The experimental methodology employs a microscope with a fixed object distance, equipped with a voltage-driven liquid lens[cite: 1941]. The voltage is adjusted in increments of 0.1 V, allowing for changes in focal length to capture the complete focusing process (defocused-focused-defocused) of the sample[cite: 1942]. After processing the images, a batch suitable for input into the DRLAF is obtained, forming a \"state\" dataset denoted as S[cite: 1943]. S is a four-dimensional tensor with dimensions 100 x 1 x 224 x 224, representing 100 frames of single-channel grayscale images with a resolution of 224 x 224 pixels[cite: 1943]. The study proposes a random sampling training method, where multiple state datasets are combined into a list L for training [cite: 1947].\n\nReward function: The reward function plays a crucial role in the reinforcement learning framework by defining task objectives, evaluating actions, shaping policies, and providing feedback[cite: 1956]. Therefore, for the autofocus task, it is necessary to consider multiple factors such as image sharpness, focusing time, and stopping strategy to provide clear behavioral feedback and learning goals for the agent[cite: 1957]. As a result, we design the following reward function involving image sharpness, time step reward, stopping reward, and an additional reward component [cite: 1958, 1963, 1968, 1970].\n\nAction space: In the proposed reinforcement learning autofocus model, we design the agent's executable actions to adjust the voltage applied to the liquid lens with different step sizes[cite: 1973]. To ensure that the number of actions satisfies certain mathematical constraints, we leverage the properties of the logarithmic function, allowing mapping the variations of the number of actions and the step size scaling coefficient to a small range of variations in the base b of the logarithmic function[cite: 1978]."
  },
  "A 1.1": "Figure 1: Schematic of the liquid lens microscope system utilizing deep reinforcement learning-based autofocus. (a) Structure of the EWOD liquid lens module, powered by the voltage driving board through a flexible electrode, enabling seamless integration with existing microscopes (see §1.1)[cite: 1392]. (b) Training sample images with diverse surface features. The proposed random sampling method of these samples during training effectively enhances the model's generalization capability(see Materials and Methods and §4)[cite: 1393].",
  "A 1.2": "Figure 2: Variable-focus microscope system driven by the constructed liquid lens module and characterization of key performances. (a) Schematic of the microscope imaging system integrated with an inverted-truncated cone structured liquid lens module[cite: 1461]. (b) Scanning electron microscope image of the 5 µm thick Parylene C dielectric layer fabricated within the liquid lens[cite: 1462]. (c) Scanning electron microscope image of the 180 nm hydrophobic Teflon layer fabricated on top of the Parylene-C layer[cite: 1463]. (d) Response time curve of the liquid lens under a 40 V driving voltage[cite: 1464]. (e) Focal length-voltage variation curve of the microscope system integrated with the liquid lens module under different magnifications[cite: 1465].",
  "A 1.3": "Figure 3: Performance of DRLAF under different action space sizes. (a) Influence of action space size on the distribution of focusing deviation in autofocusing results[cite: 1570]. As the action space size increases, the focusing voltage deviation on both the training and test sets tends to converge to 0 V[cite: 1571]. (b) Impact of action space size on the success rate and accuracy of autofocusing[cite: 1571]. As the action space size increases, both the accuracy and success rate of autofocusing significantly increase[cite: 1572]. (c) Trends of the influence of action space size on the number of autofocusing steps and accuracy[cite: 1573]. As the action space size increases, the average number of autofocusing time steps decreases significantly and stabilizes, while the average accuracy increases significantly[cite: 1574].",
  "A 1.4": "Figure 4: Influence of state dataset list size on DRLAF trained by randomly sampling performance. (a) Impact of state dataset list size on the distribution of automatic focusing deviation[cite: 1651]. As the number of state datasets increases, the focusing deviation of the model significantly decreases[cite: 1652]. (b) Effect of state dataset list size on automatic focusing success rate[cite: 1653]. With an increase in the number of state datasets, both the accuracy and success rate of automatic focusing notably improve[cite: 1654]. (c) Influence of state dataset list size on automatic focusing time steps and accuracy[cite: 1655]. The trend shows that as the number of state datasets increases, the accuracy of automatic focusing gradually improves, but the average step exhibits a trend of initially decreasing and then increasing[cite: 1656]. Appropriate sizes of state dataset lists can be configured based on practical requirements[cite: 1657].",
  "A 1.5": "Figure 5: Mean and standard deviation of scaled returns for the DRLAF trained by random sampling using 5 different reward functions and 3 random seeds on the training and testing sets of 3 different samples[cite: 1780].",
  "A 1.6": "Figure 6: Mean and standard deviation of timesteps for the DRLAF trained by random sampling using 5 different reward functions and 3 random seeds during the training process on the training and testing sets of 3 different samples[cite: 1848].",
  "A 1.7": "Figure 7: Experimental results of DRLAF. (a-f) Initial images with random defocus amounts, exhibiting lower clarity evaluation values and noticeable differences among different samples[cite: 1900]. (g-l) Images obtained after autofocus demonstrate significantly improved clarity evaluation values[cite: 1901]. (m-r) Manually selected focused images, indicating that the results obtained from autofocus are comparable to those obtained manually, both in visual observation and clarity evaluation metric values[cite: 1902]."
}