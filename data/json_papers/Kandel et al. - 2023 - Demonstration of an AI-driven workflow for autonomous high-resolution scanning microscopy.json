{
  "filename": "Kandel et al. - 2023 - Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy.pdf",
  "metadata": {
    "title": "Demonstration of an Al-driven workflow for autonomous high-resolution scanning microscopy",
    "authors": [
      "Saugat Kandel",
      "Tao Zhou",
      "Anakha V. Babu",
      "Zichao Di",
      "Xinxin Li",
      "Xuedan Ma",
      "Martin Holt",
      "Antonino Miceli",
      "Charudatta Phatak",
      "Mathew J. Cherukara"
    ],
    "arxiv_id": ""
  },
  "sections": {
    "Abstract": "Modern scanning microscopes can image materials with up to sub-atomic spatial and sub-picosecond time resolutions, but these capabilities come with large volumes of data, which can be difficult to store and analyze[cite: 3185]. We report the Fast Autonomous Scanning Toolkit (FAST) that addresses this challenge by combining a neural network, route optimization, and efficient hardware controls to enable a self-driving experiment that actively identifies and measures a sparse but representative data subset in lieu of the full dataset[cite: 3186]. FAST requires no prior information about the sample, is computationally efficient, and uses generic hardware controls with minimal experiment-specific wrapping[cite: 3187]. We test FAST in simulations and a dark-field X-ray microscopy experiment of a WSe2 film[cite: 3188]. Our studies show that a FAST scan of <25% is sufficient to accurately image and analyze the sample[cite: 3189]. FAST is easy to adapt for any scanning microscope; its broad adoption will empower general multi-level studies of materials evolution with respect to time, temperature, or other parameters[cite: 3190, 3191].",
    "Introduction": "Scanning microscopes are versatile instruments that use photons, electrons, ions, neutrons, or mechanical probes to interrogate atomic-scale composition, topography, and functionality of materials, with up to sub-atomic spatial resolution and sub-picosecond time resolution[cite: 3192]. Notwithstanding the variation in the probe modalities, these instruments all rely on a scan of the sample to generate spatially resolved signals that are then collected to form an image of the sample[cite: 3193]. Fine-resolution large-field-of-view scanning experiments, however, come with some significant drawbacks: the volume of data generated and the probe-induced damage to the sample can be prohibitively large[cite: 3196]. Meanwhile, the information of interest in these experiments is often concentrated in sparse regions that contain interfaces, defects, or other specific structural elements[cite: 3198]. Directing the scan to only these locations could greatly reduce the scan time and data volume, but it is difficult to obtain this information a priori[cite: 3199]. Addressing this challenge with a human-in-the-loop protocol can be tedious and prohibitively time-consuming[cite: 3200]. Given these factors, the development of autonomous acquisition techniques that can continuously analyze acquired data and drive the sampling specifically toward regions of interest is imperative [cite: 3201].\n\nDeep learning (DL) based inversion methods are enabling real-time data analysis, which is, in turn, opening the door to self-driving techniques that make real-time acquisition decisions based on real-time data streams[cite: 3209]. Specific to microscopy, a popular Bayesian search approach is to use unsupervised Gaussian Processes (GPs), but their computational cost tends to scale cubically with the number of points acquired[cite: 3214, 3215]. Specifically for scanning microscopy applications, Godaliyadda et al. have proposed to achieve computationally efficient autonomous sampling with the Supervised Learning Approach for Dynamic Sampling (SLADS) technique[cite: 3228]. Zhang et al. have incorporated a neural network (NN) within the SLADS method (for the SLADS-Net method) and shown in numerical experiments that it is sufficient to train the method on only a generic image, eschewing any prior knowledge about the sample[cite: 3233]. In this work, we report the Fast Autonomous Scanning Toolkit (FAST) that combines the SLADS-Net method, a route optimization technique, and efficient and modular hardware controls to make on-the-fly sampling and scan path choices for synchrotron-based scanning microscopy[cite: 3235]. We validate the FAST scheme through real-time demonstration at the hard X-ray nanoprobe beamline at the APS using a few-layer exfoliated two-dimensional WSe2 thin film[cite: 3239, 3240].",
    "Results": "Self-driving scanning microscopy workflow\nFigure 2A broadly illustrates the FAST workflow for the experiments reported here[cite: 3277]. To initiate the workflow, a low-discrepancy quasi-random selection of sample position is measured corresponding to 1% of the total area of interest[cite: 3278]. The integrated intensities are transferred to an edge device, which uses Inverse Distance Weighted (IDW) interpolation to estimate the dark-field image, which serves as input for decision-making[cite: 3278]. This workflow adopts the SLADS-Net algorithm, which uses current measurements to identify unmeasured points that would have the greatest effect on reconstruction quality (Expected Reduction in Distortion or ERD)[cite: 3279, 3280, 3283]. We select a batch of 50 points with the highest ERD to minimize experimental dead-time[cite: 3292]. The coordinates of these 50 points are passed to a route optimization algorithm based on Google’s OR-Tools to generate the shortest path[cite: 3293]. For the 200 × 40 pixels object, the workflow required ≈ 0.15 s to compute new positions and ≈ 42 s to scan the set of 50 positions, representing an overhead of ≤ 2% [cite: 3342, 3343].\n\nNumerical demonstration for scanning dark-field microscopy\nWe validated the performance through a numerical experiment on pre-acquired dark-field microscopy data, comparing FAST with Raster grid (RG), Uniform random (UR), and Low-discrepancy random (LDR) sampling [cite: 3348-3356]. The test dataset is a dark field image of size 600 × 400 pixels representing WSe2 flakes[cite: 3357, 3358]. FAST sampling reproduces with high fidelity the flake boundaries and bubbles at 10% sampling, whereas LDR and raster schemes produce lower-quality reconstructions[cite: 3365, 3422]. The FAST reconstruction stabilizes at 27% coverage[cite: 3353]. FAST preferentially samples regions with significant heterogeneity over homogeneous regions [cite: 3428].\n\nExperimental demonstration\nWe demonstrated the application of the FAST workflow in a live experiment at a synchrotron beamline on a WSe2 sample with a field of view of 20 × 4 µm (200 × 40 points)[cite: 3431, 3435]. FAST identifies heterogeneity (bubble edges) within 5% coverage and is extensively sampled by 15% coverage[cite: 3437, 3438]. Reconstruction stabilizes between 15 and 20%[cite: 3440]. A partially scanned bubble appearing only in the 20% scan highlights the exploration-exploitation tradeoff in Bayesian search [cite: 3441-3444]. Furthermore, quantitative properties like the Center of Mass (CoM) calculations for film curvature were faithfully reproduced by FAST with just 20% scan coverage compared to the full 100% raster scan [cite: 3447-3456].",
    "Discussion": "In this work, we have showcased the FAST workflow that combines a sparse sampling algorithm with route planning to drive a scanning diffraction microscopy experiment[cite: 3480]. For our live demonstration, the FAST decision-making time was negligible, leading to an overall saving of ≈ 80 min (65%) of the experiment time[cite: 3482]. The generalizability of the FAST method comes from the fact that the key NN-based component is trained on just the standard cameraman image, not on close analogs of a sample of interest[cite: 3485]. The computational time complexity is O(2N log N + kM log N), which stands in contrast to O(N^3) for Gaussian Processes[cite: 3493, 3496]. For a 200 × 40 image, our workflow performs calculation within 1.5 s on a low-power CPU, compared to 6 s on a GPU for a smaller image using GPs[cite: 3497, 3498]. Challenges include the potential to miss isolated small features during initial sampling and the time required for motor movement[cite: 3504, 3510]. Future extensions include 3D imaging, fly scans, and ptychography[cite: 3518].",
    "Methods": "The SLADS-Net algorithm: The SLADS-Net algorithm is an adaptation of the SLADS algorithm, using a supervised procedure to estimate the reduction in distortion (RD)[cite: 3521, 3531]. It uses feature vectors capturing the local neighborhood of a pixel (spatial gradients, deviation from nearby values, measurement density)[cite: 3535, 3536]. These features are transformed using a radial basis function (RBF) kernel and processed by a nonlinear fully connected neural network (5 hidden layers, 50 nodes each) to predict the Expected Reduction in Distortion (ERD) [cite: 3538, 3560].\n\nTraining: We train the SLADS-Net neural network on only the standard cameraman image without using any a priori information about the sample[cite: 3540]. We generate training pairs for 10 different sample coverage percentages between 1% and 80% and train for 100 epochs using the Adam optimizer [cite: 3541-3543].\n\nExperimental measurements: At each measurement point, a tight region of interest (RoI) around the expected position of the thin film Bragg peak was extracted, and integrated intensities were used to guide the NN prediction [cite: 3552, 3553].\n\nStatistics: The imaged region was selected through visual inspection; no statistical method was used to predetermine sample size, and investigators were not blinded[cite: 3572, 3574, 3578].",
    "Data availability": "The numerical data used for this work are publicly available at https://github.com/saugatkandel/fast_smart_scanning[cite: 3580]. The raw experimental data is publicly available at https://doi.org/10.5281/zenodo.7939730[cite: 3581]. Source data are provided with this paper[cite: 3583].",
    "Code availability": "The FAST software and the code for the numerical simulations are publicly available at https://github.com/saugatkandel/fast_smart_scanning[cite: 3585]. The code used to analyze the experimental data is available at https://doi.org/10.5281/zenodo.7942774[cite: 3586].",
    "A 1.1": "Figure 1: Artist's representation of the autonomous dark-field scanning microscopy experiment at the Advanced Photon Source (APS). The APS synchrotron produces a coherent X-ray beam focused on a WSe2 film on a Si substrate, generating diffraction patterns collected by a 2D detector [cite: 3219-3221]. The beam position and detector acquisition are autonomously controlled by the FAST AI-based workflow[cite: 3223].",
    "A 1.2": "Figure 2: The FAST workflow. (A) The cycle includes quasi-random initial measurements transferred to an edge device, initial sample estimation, computing candidate points, optimizing the path, and performing new measurements until a completion criterion is met[cite: 3274, 3275]. (B) Candidate computation involves examining the local neighborhood of unmeasured points to generate feature vectors, transforming them via RBF kernel, and using a neural network to predict Expected Reduction in Distortion (ERD) [cite: 3276-3288].",
    "A 1.3": "Figure 3: Numerical comparison of sampling methods. (A) Ground truth image. (B-D) Reconstructions at 10% scan coverage for Raster Grid (RG), Low-discrepancy random (LDR), and FAST methods. (G-I) The actual scan points corresponding to these reconstructions. (E, F) Evolution of Normalized Root Mean Square Error (NRMSE) and Structural Similarity metric (SSIM) as a function of scan coverage, showing FAST stabilizing at 27%[cite: 3339, 3340, 3352, 3353].",
    "A 1.4": "Figure 4: Evolution of the FAST scan. (A, C, E) Reconstructions at 5%, 15%, and 20% coverage. (B, D, F) Corresponding actual measurement points. (G) Image obtained through a full-grid pointwise scan. (H) Points sampled specifically between 15% and 20% coverage[cite: 3419, 3420, 3439].",
    "A 1.5": "Figure 5: Comparison of the per measured point center of mass (COM) of the diffraction patterns. (A, B) Inpainted CoMx and CoMy for the full-grid raster scan. (C, D) Inpainted CoMx and CoMy for the FAST scan at 20% coverage, demonstrating faithful reproduction of film curvature information[cite: 3456, 3477, 3478]."
  }
}