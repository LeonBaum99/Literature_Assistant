{
  "filename": "Nousiainen et al. - 2024 - Laboratory experiments of model-based reinforcement learning for adaptive optics control.pdf",
  "metadata": {
    "title": "Laboratory experiments of model-based reinforcement learning for adaptive optics control",
    "authors": [
      "Jalo Nousiainen",
      "Byron Engler",
      "Markus Kasper",
      "Chang Rajani",
      "Tapio Helin",
      "CÃ©dric T. Heritier",
      "Sascha P. Quanz",
      "Adrian M. Glauser"
    ],
    "arxiv_id": ""
  },
  "sections": {
    "Abstract": "Direct imaging of Earth-like exoplanets is one of the most prominent scientific drivers of the next generation of ground-based telescopes. Typically, Earth-like exoplanets are located at small angular separations from their host stars, making their detection difficult. Consequently, the adaptive optics (AO) system's control algorithm must be carefully designed to distinguish the exoplanet from the residual light produced by the host star. A promising avenue of research to improve AO control builds on data-driven control methods, such as reinforcement learning (RL). RL is an active branch of the machine learning research field, where control of a system is learned through interaction with the environment. Thus, RL can be seen as an automated approach to AO control, where its usage is entirely a turnkey operation. In particular, model-based RL has been shown to cope with temporal and misregistration errors. Similarly, it has been demonstrated to adapt to nonlinear wavefront sensing while being efficient in training and execution. In this work, we implement and adapt an RL method called policy optimization for AO (PO4AO) to the GPU-based high-order adaptive optics testbench (GHOST) test bench at ESO headquarters, where we demonstrate a strong performance of the method in a laboratory environment. Our implementation allows the training to be performed parallel to inference, which is crucial for on-sky operation. In particular, we study the predictive and self-calibrating aspects of the method. The new implementation on GHOST running PyTorch introduces only around 700 \u00b5s of in addition to hardware, pipeline, and Python interface latency. We open-source well-documented code for the implementation and specify the requirements for the RTC pipeline. We also discuss the important hyperparameters of the method and how they affect the method. Further, the paper discusses the source of the latency and the possible paths for a lower latency implementation.",
    "Introduction": "High contrast imaging (HCI) utilizes a combination of extreme adaptive optics (XAO) and coronagraphy to generate images of faint sources near bright point sources, such as exoplanets near their host stars. Direct imaging of exoplanets has been largely limited to only a few dozen very young and luminous giant exoplanets using existing HCI instruments. However, a greater number of planets could be directly imaged by enhancing the sensitivity in the vicinity of the host star, with the performance of the XAO system being the primary limiting factor in achieving such sensitivity. In HCI, when imaging in close proximity to the star, the main performance limitations of a well-tuned adaptive optics (AO) system controlled with the common integrator controller are photon noise and temporal error. The temporal delay error of AO systems controlled by standard methods arises from the integration of wavefront sensor detector data, detector readout, computation of the correction signal, and its application to the deformable mirror (DM). This delay amounts to at least two AO system operating cycles at the maximum camera framerate, where readout takes one entire frame, during which atmospheric turbulence has evolved and no longer matches the DM correction precisely. There are two ways to mitigate the adverse effect of the temporal delay error for HCI: by increasing the operating frequency of the AO system or by implementing predictive control. The acceleration of the AO system can be accomplished, for example, by adding a second stage downstream from a classical first-stage AO system. This second-stage system solely observes the residual from the first-stage AO system and can operate independently from the first-stage, employing DMs that can handle fast AO loops. One such example is the upgrade of SPHERE, which is referred to as SPHERE+, which is expected to provide a considerable enhancement in raw point-spread function (PSF) contrast close to the star. The other (not mutually exclusive) approach is to use a predictive control algorithm. A big part of the turbulence is presumably in frozen flow considering the millisecond timescale of AO control, and hence, a significant fraction of wavefront disturbances can be predicted. Moreover, if the predictive control algorithm is fast enough, both strategies can be combined by operating the faster second stage with predictive control. Besides the performance limitations induced by photon noise and temporal error, AO can suffer from dynamic modeling errors, such as misregistration, optical gain effect for the Pyramid wavefront sensor (WFS). Coping with these limitations usually requires external tuning and recalibration of a possible predictive control algorithm. This paper presents a laboratory demonstration of a data-driven predictive control algorithm called the policy optimizations for AO (PO4AO) implemented on a second stage AO system following a first stage running a classical integrator control. One of the main advantages of implementing fully data-driven control, such as PO4AO, is that it continuously learns a system model from the data rather than using a static calibration or synthetic model. Consequently, it is less affected by pseudo-open-loop reconstruction errors, such as misregistration or the optical gain effect. Our contributions are two-fold: first, we thoroughly test the performance and robustness of PO4AO in a laboratory setup under different conditions. Second, we open-source Python-based implementation of the method that can be implemented in any AO system that runs Python-based controllers and has GPUs. We also discuss how the method can be tuned and further developed for different AO systems.",
    "Related Work": "PO4AO addresses the predictive control and reconstruction in the XAO control loop as a single reinforcement learning (RL) problem; hence, PO4AO is related to many aspects of XAO control, such as predictive control, optimal gain compensation, misregistration identification, reconstruction algorithms, and vibration canceling. Remarkable progress has been achieved with various approaches to tackle the XAO control problem. These methods include the Kalman filter-based linear controllers, sometimes combined with machine learning for system identification. These methods rely on linear models for wavefront sensing and temporal evolution to obtain a state estimation of the system. Other methods focus on correcting temporal error and vary from spatio-temporal linear filters to filters operating on single modes, such as Fourier or Zernike modes. The predictive filters are obtained either from modeling or utilizing data analysis/machine learning. Some methods have also been tested on-sky. Moreover, machine learning methods utilizing neural networks (NNs) for predictive control have been studied, where NNs show a lot potential, especially for AO systems with high number of degrees of freedom (DoF), and in noisy conditions. Lately, NNs, and the more modern deep NNs, have also been used for the wavefront reconstruction step. The results indicate that NN reconstruction is less sensitive to non-linearity and increases the operational range of the pyramid WFS. Finally, different NN-based RL approaches have been studied during the last years. PO4AO differs from other RL methods in AO literature by using so-called model-based RL instead of model-free RL (for a discussion on the difference between these methods, see Ref. 11). For interested readers, Fowler and Landman provide a more thorough review of machine learning methods for wavefront control and phase prediction.",
    "Classical Adaptive Optics Control and Baseline Controller": "An AO system is commonly controlled with a linear integrator controller, referred to as the integrator. We consider it our reference method against the PO4AO as it is still widely used in AO. Integrator control in AO usually relies on the so-called interaction matrix mapping DM commands to WFS measurements. Once the interaction matrix is estimated, the inverse problem, i.e., reconstruction v given Delta w, needs to be considered. As D is generally not invertible, some regularization approach is needed. Here, we restrict ourselves to linear methods described by a reconstruction matrix C mapping WFS measurements to DM commands. As our regularization method, we project D to a smaller dimensional subspace spanned by the Karhunen-Lo\u00e8ve (KL) modal basis. Each KL mode in the basis has a representation in terms of actuator voltages. This relation is fully determined by a transformation matrix mapping DM actuator voltages to m first modal coefficients. The regularized reconstruction matrix is now defined by the Moore-Penrose pseudo-inverse. The number of modes m defines stability at the cost of resolution; smaller m results in lower noise amplification while producing a reconstruction with fewer modal basis functions (less detailed reconstruction). An optimal m balances the error produced by these two effects. We use the leaky integrator as a baseline AO controller to which PO4AO is compared. At a given time step, the WFS measures the residual wavefront. The leaky integrator then obtains the new control voltages. The DM saturation can cause a build-up of modes outside the control radius. Hence, introducing a leakage typically chosen near one, e.g., 0.99, in the DM commands commonly used to remove those unseen modes and increase robustness.",
    "Adaptive Optics as a Markov Decision Process": "In this paper, we model AO control loop as a Markov decision process (MDP), which is the de facto mathematical framework for sequential decision problems in RL. In AO control, instead of the full state of the system (exact DM shape, the full atmosphere profile, etc.), we only observe WFS data, that represents only a partial information of the whole system. These kinds of processes are referred to as partially observed MDPs in the RL literature, and, in theory, optimal decisions (control) should consider all the past measurements observed (from the beginning of operation). However, we expand the state space to include a history of WFS measurement and DM control voltages to guarantee approximately Markovian statistics and treat the process as an ordinary MDP, where optimal decisions can be taken directly from the previous state formulation (in our case a concatenation of past measurement and actions). We identify an action as the applied differential voltage. We now define the state s_t of the MDP as a sequence of observations and actions. We assume that there exist Markovian transition dynamics where the only new element of the next state is the next observation. This transition model contains information on the time delay, misregistration and non-linearity errors, and atmospheric turbulence. Further, in the following formulation of the control algorithm, the initial reconstruction matrix serves as preprocessing to observation and does not connect measurement to action directly. As the reward function of the MDP, we consider the negative Euclidian norm of the residual wavefront observed through the WFS with a regularization term that favors small actions. The addition of the regularization term effectively regularizes the control algorithm, making it less prone to saturation and oscillation, especially early in the training procedure.",
    "Model-Based Policy Optimization": "The key idea of PO4AO is to learn a non-linear control law that maps past telemetry to new DM commands from data collected from the AO loop and maximizes the reward. In RL terminology, this control law is referred to as the policy and will be formulated as a mapping from the current state to the next action. Hence, the policy combines the reconstruction and control steps in AO. In this work, the policy is constructed as a NN, and its parameters are derived indirectly via model-based policy optimization. More precisely, the method collects data to learn a dynamics model that is also represented by an NN and can be used to predict the subsequent state given the current state and an action. The dynamics model is then used to optimize the policy. Both NNs are fully convolutional NN (CNN) with three layers. The method first runs the so-called warm-up phase, where an initial data set is collected by injecting random control signals into the control system, followed by training the involved NN models. The warm-up phase aims to ensure rough estimates of policy and dynamic NN and, consequently, stabilize the training procedure in the beginning. After the warm-up phase, the method iterates three phases: (1) Running the policy, (2) Improving the dynamics model, and (3) Improving the policy. Phases 2 and 3 are run in parallel to phase 1. The dynamics model is trained via a supervised learning objective, minimizing the squared difference between true next states and predictions. The policy is optimized to maximize the expected reward within a planning horizon H, utilizing the dynamics model to simulate the future.",
    "PO4AO Implementation and Hyperparameters": "PO4AO has a lot of free adjustable parameters (see Table 1). We arrange hyperparameters under four different subcategories: Reinforcement Learning Parameters, Training Parameters, Markov Decision Process Parameters, and Replay Buffers. Reinforcement Learning Parameters set the frequency on which the policy NN is updated. The episode length is the number of frames in an episode; a single training procedure is run during the episode. The warm-up length determines how many episodes are run in the warm-up phase. The initial and minimum warm-up noise set the range for the noise added during warm-up, which is reduced linearly. The loss function penalty parameter alpha defines the amount of regularization in the reward function. Training Parameters set the number of gradient steps in dynamics and policy optimization. After the warm-up phase, the loop is suspended, and the first training procedure is run. After the first training iteration, the loop is closed with policy, and the parallel training procedure is started. The mini batch size is the number of data points used to calculate a single gradient step. MDP Parameters specify the MDP formulation. The number of history frames decides the number of past measurements in the MDP formulation. The planning horizon sets the future time window considered by the PO4AO. Replay Buffers include the warm-up and replay buffer. The warm-up buffer saves the data recorded during the warm-up phase. The newest data is added to the replay buffer, which keeps the latest replay buffer size episodes in memory. The mini-batch sampled during the training is sampled from the warm-up buffer with the probability set by the train warm-up percentage and otherwise from the replay buffer.",
    "Experiments": "Here, we present the results of several experiments performed with PO4AO on GHOST to explore its performance for high-level conditions relevant to operational on-sky AO. Specifically, we explore the impact of temporal delay, robustness for low S/N, ability to cope with misregistration, and the effect of history length on performance. In all experiments, the PO4AO is compared against an integrator whose gain is adjusted to minimize WFS residuals. The GHOST laboratory AO system has been built to evaluate new AO control techniques. It utilizes a simple single-source on-axis setup equipped with a pyramid WFS and a Boston micromachines deformable mirror (DM-492). A programmable spatial light modulator (SLM) introduces turbulence with high spatial resolution. To simulate a faster second-stage system using GHOST, we generated residual phase screens numerically for the lab setup. This exact set of residual turbulence phase-screens is then replayed by the SLM for all our GHOST experiments. In the time delay experiment, we ran PO4AO on three different temporal delays (0, 1, and 2 additional frames). PO4AO outperforms the integrator after the warm-up of 20 episodes, providing around a factor of three improvement in reconstructed wavefront variance for all delays. In the low flux experiment, we set the light source such that the flux was around 6000 camera counts/frame (S/N ~ 2). PO4AO considerably reduces the photon flux inside the control radius compared to the integrator. In the vibration reduction experiment, we studied the effect of the number of past telemetry frames. The PO4AO correction performance improves with the number of history frames considered. We observed a vibration spike at 16 Hz; PO4AO with 128 history frames dampens the spike. In the misregistration experiment, we introduced various degrees of misregistration by shifting the DM off-axis. PO4AO is able to obtain stability and performance with dynamic misregistration while the integrator gets unstable. Finally, we discuss the latency introduced by the method. The total delay budget accounts for pipeline latency and the Python implementation delay, including CNN forward pass, data handling, and state updates.",
    "Conclusion and Discussion": "To conclude, this paper demonstrates that PO4AO is a robust controller for a second-stage AO system in a lab simulation. RL is shown to mitigate several critical error terms in XAO control, such as misregistration, photon noise, and temporal error. Moreover, running PO4AO is a turnkey operation as the hyperparameters are tuned only when the method is implemented, and the method adapts automatically to changing conditions like noise level, misregistration, and wind profile. Extensive experiments on GHOST confirm that PO4AO can adapt to and mitigate these error terms on real hardware. In addition, we showed that PO4AO could also mitigate vibrations if it considers enough past telemetry frames. However, like most deep RL methods, PO4AO is somewhat sensitive to the choice of hyperparameters. Tuning the parameters can take time, but the method performs robustly under all conditions once a good combination of hyperparameters is found. The method did not improve the system's stability to more degrees of freedom. We observed that the integrator was stable to ~350 KL modes, and PO4AO did not enable us to control more KL modes robustly. Additionally, we open-sourced the Python implementation of PO4AO, which is well-commented and can be easily adapted to numerical simulations or real hardware. The paper also discusses the hyperparameters of PO4AO and how they affect the method. The open-source implementation introduces an additional ~700 \u00b5s to the pipeline latency, making it suitable for systems running lower than 1000 Hz. Various avenues exist to optimize the method further, such as transitioning to lower-level programming languages, using NVIDIA TensorRT, optimizing memory handling, and streamlining the pipeline.",
    "Appendix A: Implementation Details": "This section discusses open-source Python implementation. The code consists of three Python files: po4ao.py, po4ao_models.py, po4ao_util.py, and po4ao_config.py. The po4ao.py script includes the main function that starts the warm-up phase and training procedure and closes the loop. It includes the step-function that interfaces the Python code to the RTC pipeline. The file po4ao_models.py contains the NN models: the policy and the dynamics. The file po4ao_util.py includes utility functions, such as the implementation for replay buffers and shared memory optimizers. The configuration file po4ao_config.py includes all adjustable hyperparameters of the method. The parameters include integrator settings, NN model architecture parameters, and control delay parameters.",
    "Data availability": "The codes used in this paper are available on GitHub repository [https://github.com/jnousi/PO4AO.git]. The code is documented and annotated to help readers understand the methodology and reproduce the results. We encourage readers to use the data and codes for their own research and to cite this paper as the source of the data.",
    "A 1.1": "Fig. 1: NN architectures. Both NN, the dynamics, and policy take input tensor concatenations of past actions and observations. They also share the same fully convolutional structure in the first three layers. At the output layer, the policy model includes the KL-filtering scheme (upper right corner), and the dynamics model output is multiplied with the WFS mask (lower right corner). For the GHOST, the input and output images are 24x24 pixels (set by the DM).",
    "A 1.2": "Fig. 2: GHOST coronagraphic PSFs. (a) The PSF without any turbulence, and the DM set to be flat. (b) The PSF with simulated 1-stage systems residual phase screens played on SLM, and a flat DM. The speckle at around 1 o'clock is a ghost in the system.",
    "A 1.3": "Fig. 3: PO4AO interface for RTC pipeline. COSMIC pipeline preprocesses the raw WFS data, projects it to DM-space with command matrix (using the modal basis matrices: S2M and V2M), then writes the \"delta volts\" to the shared memory buffer, and suspends the loop. Python interface (the green box) reads the shared memory buffer and passes the data to the PO4AO implementation. The PO4AO calculates the next command and saves the data (orange boxes), and the Python interface writes the command to shared memory, where COSMIC registers the command and passes it to the saturation management algorithm (SMA)/clipping stage.",
    "A 1.4": "Fig. 4: Learning curves for time delay experiments. The red lines correspond to the performance of PO4AO during each episode, and the blue lines are for the integrator. A single episode is 500 frames. The gray dashed line marks the end of the integrator warm-up for PO4AO. In all cases, the PO4AO outperforms the integrator all ready after the warm-up period. The training is done parallel to control, so the 10 episodes correspond to ~14 s in the figure.",
    "A 1.5": "Fig. 5: PSFs on different additional control delays. The top row is for the integrator control, and the bottom is for PO4AO. The PO4AO and its hyper-parameters are exactly the same for all time delays the time delay is learned from the interaction.",
    "A 1.6": "Fig. 6: The contrast with different time delays. The blue lines correspond to the azimuthal average of integrator images, and the red lines are the same for PO4AO. The line style indicates the length of the time delay. The solid black line is for flat DM, and the dashed black line is for flat DM (open loop) with first-stage residuals played on SLM. Note that around 11 lambda/D is the correction area of the DM-492, and around 18 lambda/D is the correction area of the numerically simulated first-stage DM. PO4AO provides better contrast inside the second-stage control radius for all time delays.",
    "A 1.7": "Fig. 7: Learning curve for the low flux experiment. The blue line is the cumulative reward after each episode for the integrator, and the red line is for PO4AO. The dashed green line is the reward after each episode when the turbulence was not played and the loop opened.",
    "A 1.8": "Fig. 8: PSFs during the low flux experiment. The left image is for the integrator, and the right is for the PO4AO.",
    "A 1.9": "Fig. 9: Learning curves for different history lengths. We plot the cumulative reward after each episode for all history lengths after the warm-up phase. All history lengths have the same warm-up phase (not included in the plot). The longer the time length is, the better the performance.",
    "A 1.10": "Fig. 10: Temporal PSD of mode #1 (tip) on different history lengths.",
    "A 1.11": "Fig. 11: The misregistration experiment. Here, we plot the cumulative loss over an episode during the misregistration experiment. The blue line is the well-tuned integrator calibrated with centered DM, and the green line is the well-tuned integrator calibrated with DM 120 microns off-axis. The red line is the PO4AO calibrated and pre-trained with centered DM. The dashed black lines indicate the moment when DM was manually shifted. Since the shifting was done manually, the gray areas around the line indicate the uncertainty of the exact moment."
  }
}