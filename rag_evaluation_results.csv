Tier,Question,Target_Tag,Hit,Answer,Sources,Latency
1,What physical quantity is the controller changing (the actuator variable) in the liquid-lens autofocus setup?,liquid lenses,True,The controller is changing the voltage applied to the liquid lens [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Effect of actions on autofocus performance].,precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | autofocus: ai-driven alignment of nanofocusing x-r...,22.07
1,Which classic search methods are used as baselines in the DRL autofocus paper?,autofocus,True,The classic search methods used as baselines in the DRL autofocus paper are golden section search-based [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Autofocus experiment] and Fibonacci search-based methods [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Autofocus experiment].,precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | autofocus: ai-driven alignment of nanofocusing x-r...,20.96
1,What is the main objective of 'adaptive scanning' compared to a fixed raster grid in ptychography?,ptychography,True,"The main objective of 'adaptive scanning' compared to a fixed raster grid in ptychography is to lower the dose required for an electron ptychographic reconstruction by adaptively scanning the specimen, thereby providing the required spatial information redundancy in the regions of highest importance [Deep reinforcement learning for data-driven adaptive scanning in ptychography | Conclusion].",deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti...,21.84
1,What does the metric QSSIM represent in the ptychography evaluation?,ptychography,True,The metric QSSIM represents the quality of the ptychographic reconstruction based on Structural Similarity Index Measure (SSIM) [Deep reinforcement learning for data-driven adaptive scanning in ptychography | Methods].,deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al...,12.66
1,What are the discrete actions available to the agent (action space) in the DQN beam alignment paper?,alignment,True,"Based on the provided context, here are the discrete actions available to the agent in the DQN beam alignment paper:

- ""The agent can choose any of the four actions during the interaction with the model."" [Deep reinforcement learning for self-tuning laser source of dissipative solitons | Deep Q-learning algorithm]",a general bayesian algorithm for the autonomous al... | autofocus: ai-driven alignment of nanofocusing x-r... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | deep reinforcement learning for self-tuning laser ...,17.67
1,What are the two main limitations of a well-tuned integrator controller that motivate RL for adaptive optics?,optics,True,"The two main limitations of a well-tuned integrator controller that motivate RL for adaptive optics are:

1. **Limited ability to handle dynamic modeling errors**: Integrator controllers rely on static calibration or synthetic models, which can be affected by pseudo-open-loop reconstruction errors like misregistration or optical gain effect [Laboratory experiments of model-based reinforcement learning for adaptive optics control | 1 Introduction].
2. **Performance limitations induced by photon noise and temporal error**: These factors limit the performance of integrator controllers, even with optimal tuning [Laboratory experiments of model-based reinforcement learning for adaptive optics control | 1 Introduction].",laboratory experiments of model-based reinforcemen... | laboratory experiments of model-based reinforcemen... | laboratory experiments of model-based reinforcemen... | deep reinforcement learning for self-tuning laser ... | laboratory experiments of model-based reinforcemen...,34.61
2,"What batch size (number of points) is chosen for acquisition in FAST, and what practical reason is given?",FAST,True,The batch size chosen for acquisition in FAST is 50 points [Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy | Discussion]. The practical reason given for this choice is to ensure that communication time stays negligible and there's no noticeable loss in the quality of points acquired compared to a pointwise candidate selection scheme [Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy | Discussion].,demonstration of an ai-driven workflow for autonom... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | demonstration of an ai-driven workflow for autonom...,24.14
2,At roughly what scan coverage does reconstruction quality 'stabilize' in FAST (based on SSIM/NRMSE trends)?,FAST,True,"Based on the provided context, I do not know based on the provided context because the retrieved sections do not mention this. Would you like me to find related papers online?",deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al... | demonstration of an ai-driven workflow for autonom... | a general bayesian algorithm for the autonomous al... | demonstration of an ai-driven workflow for autonom...,10.86
2,"If the full grid is 200 x 40, how many points is that total, and how many points correspond to 20% coverage?",FAST,False,"Based on the provided context:

- The total number of points in a full grid of 200 x 40 is **8000** [A general Bayesian algorithm for the autonomous alignment of beamlines | 2].
- For 20% coverage, that corresponds to **1600 points** (20% of 8000) [A general Bayesian algorithm for the autonomous alignment of beamlines | 2].",a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | the rise of data‐driven microscopy powered by mach... | self-driving laboratories for chemistry and materi...,23.65
2,Which specific workflow design choice is described as the primary contributor to the large time saving in FAST?,FAST,True,The primary contributor to the large time saving in FAST is the choice to acquire a batch of 50 measurements between the selection of prospective measurement points [Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy | Discussion].,demonstration of an ai-driven workflow for autonom... | demonstration of an ai-driven workflow for autonom... | autofocus: ai-driven alignment of nanofocusing x-r... | performance metrics to unleash the power of self-d... | autofocus: ai-driven alignment of nanofocusing x-r...,15.03
2,"What value of k is used for k-nearest neighbors, and what time complexity (Big-O) is stated for the method?",FAST,False,I do not know based on the provided context because the retrieved sections do not mention this. Would you like me to find related papers online?,a general bayesian algorithm for the autonomous al... | self-driving laboratories for chemistry and materi... | performance metrics to unleash the power of self-d... | self-driving laboratories for chemistry and materi... | self-driving laboratories for chemistry and materi...,9.72
2,"Why are Gaussian Processes included as a comparator in FAST, and what scaling drawback is highlighted?",FAST,False,"Based on the provided context:

Gaussian Processes are included as a comparator in FAST because they serve as a baseline for comparison with other models used in the algorithm, such as the Bessel kernel [A general Bayesian algorithm for the autonomous alignment of beamlines | 3.3. Noisy models].

The scaling drawback highlighted is that Gaussian Processes can have high computational complexity due to the need for matrix inversion or Cholesky decomposition, which can be ill-conditioned without a small noise level ('jitter') added to the diagonal of the kernel [A general Bayesian algorithm for the autonomous alignment of beamlines | 3.3. Noisy models].",a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | inverse design of chiral functional films by a rob... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al...,34.06
2,"List the reward hyperparameters (e.g., alpha, beta, mu, delta) for DRL autofocus and what each incentivizes.",autofocus,True,"The reward hyperparameters for DRL autofocus in the provided context are:

- Alpha (α): 100 [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Reward function]
- Beta (β): 30 [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Reward function]
- Mu (μ): 200 [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Reward function]
- Delta (δ): 100 [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Reward function]

These hyperparameters incentivize the following aspects of the autofocus task:

- Alpha (α) encourages achieving clear imaging.
- Beta (β) rewards reducing the time to focus.
- Mu (μ) incentivizes stopping automatically once focused.
- Delta (δ) enhances the discriminative ability of the reward function by setting relatively large positive and negative rewards for the clearest and least clear images, respectively.",precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | autofocus: ai-driven alignment of nanofocusing x-r...,63.6
2,What are the two action-set designs in the DRL autofocus paper and how do they differ conceptually?,autofocus,True,"Based on the provided context, there are two action-set designs in the DRL autofocus paper:

1. **Action Set with 5 Actions**: This set includes forward adjustments, backward adjustments, stop actions, and is constructed based on a factor of 5 [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Effect of actions on autofocus performance].

2. **Action Set with 7 Actions**: This set also includes forward adjustments, backward adjustments, stop actions, but it's configured differently to achieve better focusing performance. It's determined by the logarithmic method with smaller bases [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Effect of actions on autofocus performance].

Conceptually, they differ in the number of actions and their configuration:
- The 5-action set has fewer options for adjustments, which might affect speed and accuracy.
- The 7-action set offers more choices, allowing for potentially more precise adjustments but could also introduce complexity.",precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | autofocus: ai-driven alignment of nanofocusing x-r...,59.36
2,"What is the reported speed improvement versus a named baseline in DRL autofocus, and what is the speed proxy?",autofocus,True,"The reported speed improvement versus a named baseline in DRL autofocus is a reduction of time by 79% compared to the golden section search-based method [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Autofocus experiment]. The speed proxy used is the average autofocusing time step, which was achieved in an average of only 3.15 steps [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Autofocus experiment].",precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | autofocus: ai-driven alignment of nanofocusing x-r... | autofocus: ai-driven alignment of nanofocusing x-r...,30.72
2,"What are the ROP reconstruction settings (batch size, step size) and the approximate reconstruction time reported for ptychography?",ptychography,True,I do not know based on the provided context because the retrieved sections do not mention ROP reconstruction settings or approximate reconstruction time for ptychography. Would you like me to find related papers online?,deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al...,13.47
2,Summarize the encoder/feature-extractor architecture used to represent partial scans in the adaptive ptychography paper.,ptychography,True,The encoder/feature-extractor architecture used to represent partial scans in the adaptive ptychography paper is not explicitly described in the provided context. The retrieved sections do not mention this. Would you like me to find related papers online?,deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al... | deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al... | deep reinforcement learning for data-driven adapti...,15.34
2,"From the main hyperparameter table in the AO RL paper: what are episode length, warm-up episodes, replay buffer size, and planning horizon?",optics,True,"Based on the provided context:

- **Episode Length**: The number of frames in an episode is set to 500 frames for GHOST running at 350 Hz, which is approximately 1.4 seconds [Laboratory experiments of model-based reinforcement learning for adaptive optics control | 6.1 Reinforcement Learning Parameters].
- **Warm-up Episodes**: The warm-up length determines how many episodes are run in the warm-up phase. For example, if the warm-up length is set to 20, the first 20 episodes are run with the noisy integrator [Laboratory experiments of model-based reinforcement learning for adaptive optics control | 6.1 Reinforcement Learning Parameters].
- **Replay Buffer Size**: The replay buffer keeps the latest J episodes in memory, where J is usually set to 20 episodes (10,000 frames) in this case [Laboratory experiments of model-based reinforcement learning for adaptive optics control | 6.4 Replay Buffers].
- **Planning Horizon**: I do not know based on the provided context because the retrieved sections do not mention this. Would you like me to find related papers online?",laboratory experiments of model-based reinforcemen... | laboratory experiments of model-based reinforcemen... | laboratory experiments of model-based reinforcemen... | laboratory experiments of model-based reinforcemen... | laboratory experiments of model-based reinforcemen...,68.21
3,Compare the utility/decision signal FAST uses vs. the reward metrics in adaptive ptychography. Why does each make sense in its domain?,,,"Based on the provided context:

**FAST (Fast Adaptive Sampling Technique) used in adaptive ptychography:**
- FAST uses a utility function that balances exploration and exploitation based on the posterior variance [Adaptive Ptychography | 3.2. Utility Function].
- This makes sense in its domain because it helps to efficiently sample new points while also exploiting known good solutions, leading to faster convergence.

**Reward metrics in adaptive ptychography (using the example of autofocus in optical microscopy):**
- The reward function used combines multiple terms: clear imaging (α), reducing focusing steps (β), and stopping automatically (μ, δ) [Precision Autofocus | Reward Function].
- This makes sense because it encourages the agent to optimize for all these objectives simultaneously, preventing bias towards a single term.

**Comparison:**
- FAST's utility function is designed to balance exploration and exploitation in the context of adaptive sampling, making it suitable for efficient optimization.
- The reward function used in autofocus balances multiple objectives relevant to that specific task (clear imaging, reducing focusing steps, stopping automatically), ensuring the agent optimizes for all these aspects simultaneously.

Sources:
[Adaptive Ptychography | 3.2. Utility Function]
[Precision Autofocus | Reward Function]",a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | precision autofocus in optical microscopy with liq... | inverse design of chiral functional films by a rob... | a general bayesian algorithm for the autonomous al...,68.31
3,"How does FAST define 'scanning efficiency,' and in what way is this fundamentally different from raster-grid scanning?",FAST,True,"FAST defines 'scanning efficiency' as ""the ratio of the number of measurements taken to the total time required for those measurements"" [Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy | Discussion]. This is fundamentally different from raster-grid scanning because FAST selects points based on their estimated relevance (using Expected Reward Density), while raster-grid scanning systematically visits every point in a predefined grid, regardless of its potential information content.",demonstration of an ai-driven workflow for autonom... | a general bayesian algorithm for the autonomous al... | demonstration of an ai-driven workflow for autonom... | a general bayesian algorithm for the autonomous al... | precision autofocus in optical microscopy with liq...,24.86
3,Compare how at least two different papers address the trade-off between image quality and sample damage/dose. What is each paper's 'knob' for reducing damage?,,,"Based on the provided context, I cannot directly compare how different papers address the trade-off between image quality and sample damage/dose because the retrieved sections do not mention this. Would you like me to find related papers online?",the rise of data‐driven microscopy powered by mach... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al...,13.49
3,Why might discretizing the action space improve stability or convergence compared to continuous control in the DRL autofocus setting?,autofocus,True,"Discretizing the action space improves stability or convergence compared to continuous control in the DRL autofocus setting because [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Effect of actions on autofocus performance] it enhances the action selectivity of the model in the vicinity of the focal point position, enabling more precise actions and leading to a significant reduction in focusing deviation as the number of actions increases. Additionally, [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Effect of actions on autofocus performance] shows that increasing the action space size improves both the accuracy and success rate of autofocusing significantly.",precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | precision autofocus in optical microscopy with liq... | deep reinforcement learning for self-tuning laser ... | deep reinforcement learning for self-tuning laser ...,40.5
3,Contrast the observation/state and reward shaping between DQN alignment and DRL autofocus; explain why the reward needs are different.,,,"Based on the provided context:

**Observation/State:**
- In DQN Alignment (Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning), the observation or state is the raw image [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Abstract].
- In DRL Autofocus (AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems), the state includes the current position and orientation of the mirrors, as well as their corresponding parameters like focal spot size and shape [AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems | 5. The AI-driven controller in operating conditions].

**Reward Shaping:**
- In DQN Alignment, a targeted reward function is used to enhance performance in microscope autofocus tasks. This function includes terms for achieving clear imaging (α), reducing focusing steps (β), and stopping automatically (μ) [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Reward function].
- In DRL Autofocus, the reward is based on the quality of the focused beam, which is a combination of factors like focal spot size, shape, and intensity. The specific reward function is not explicitly mentioned in the provided context.

**Reward Needs:**
- The rewards need to be carefully designed to balance different objectives:
  - In DQN Alignment, all terms (α, β, μ) should have maximum absolute values on the same order of magnitude to prevent bias towards a single term [Precision autofocus in optical microscopy with liquid lenses controlled by deep reinforcement learning | Reward function].
- The reward needs differ because:
  - DQN Alignment focuses on optimizing image quality and focusing speed for microscopic imaging, while also considering automatic stopping.
  - DRL Autofocus aims to optimize the focused beam's quality (size, shape, intensity) for nanofocusing X-ray mirror systems.

I do not know based on the provided context because the retrieved sections do not mention this. Would you like me to find related papers online?",precision autofocus in optical microscopy with liq... | a general bayesian algorithm for the autonomous al... | deep reinforcement learning for self-tuning laser ... | precision autofocus in optical microscopy with liq... | autofocus: ai-driven alignment of nanofocusing x-r...,114.05
3,"The ptychography paper uses a particular discount factor setting (e.g., gamma = 0). Why is that choice made and what does it imply?",ptychography,True,"The provided context does not mention the discount factor setting in the ptychography paper or why that choice is made. Therefore, I do not know based on the provided context because the retrieved sections do not mention this. Would you like me to find related papers online?",a general bayesian algorithm for the autonomous al... | deep reinforcement learning for data-driven adapti... | deep reinforcement learning for data-driven adapti... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al...,15.74
3,"Where does the exploration–exploitation trade-off appear in FAST results/figures, and what mitigation strategy is proposed?",FAST,True,"The exploration–exploitation trade-off appears in the context of FAST results/figures as a challenge where ""the time required for the movement exceeds the time required for a single measurement"" [Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy | Discussion]. The mitigation strategy proposed is to ""explicitly include a measurement-density-based term or a movement-time-based term in the candidate selection procedure"" [Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy | Discussion].",demonstration of an ai-driven workflow for autonom... | a general bayesian algorithm for the autonomous al... | performance metrics to unleash the power of self-d... | a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al...,28.33
3,List the main latency bottlenecks mentioned across at least two papers and the optimizations suggested.,,,"Based on the provided context, here are the main latency bottlenecks mentioned across at least two papers and the optimizations suggested:

1. **CPU Bottleneck**: Both ""Laboratory experiments of model-based reinforcement learning for adaptive optics control"" [8 Conclusion and Discussion] and ""AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems"" [1 Introduction] mention CPU as a potential latency bottleneck.
   - Optimization suggested in the first paper: Optimize code to reduce computational load.

2. **Data Processing Efficiency**: Both papers also discuss data processing efficiency as a potential bottleneck.
   - Optimization suggestions:
     - In ""Laboratory experiments of model-based reinforcement learning..."", streamlining the pipeline to handle control voltage images instead of vectors is suggested.
     - In ""AutoFocus: AI-driven alignment of nanofocusing X-ray mirror systems"", creating a precise digital twin allows for more efficient data processing.

3. **Memory Handling**: The first paper suggests optimizing memory handling through techniques like circular buffers to enhance data storage and retrieval efficiency, reducing computational load.",laboratory experiments of model-based reinforcemen... | performance metrics to unleash the power of self-d... | a general bayesian algorithm for the autonomous al... | autofocus: ai-driven alignment of nanofocusing x-r... | self-driving laboratories for chemistry and materi...,57.64
3,"Identify two domain-shift risks (new sample types, new optics, etc.) and how each paper attempts to handle them.",,,"Based on the provided context, here are two domain-shift risks and how each paper attempts to handle them:

1. **Domain Shift Risk:** New sample types in drug discovery pipeline.
   - **Handling Method:** The paper ""Self-Driving Laboratories for Chemistry and Materials Science"" acknowledges this risk by highlighting the need for better guidelines and reporting standards to facilitate machine learning applications in the field [Self-Driving Laboratories for Chemistry and Materials Science | 5.1. Drug Discovery Pipeline].

2. **Domain Shift Risk:** New optics in adaptive optics control.
   - **Handling Method:** The paper ""Laboratory experiments of model-based reinforcement learning for adaptive optics control"" addresses this risk by focusing on high contrast imaging and adaptive optics, suggesting that the method can adapt to new optical conditions [Laboratory experiments of model-based reinforcement learning for adaptive optics control | ABSTRACT.].",a general bayesian algorithm for the autonomous al... | a general bayesian algorithm for the autonomous al... | self-driving laboratories for chemistry and materi... | laboratory experiments of model-based reinforcemen... | inverse design of chiral functional films by a rob...,43.37
